{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-612/blob/main/Copy_of_Project_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Required Libraries"
      ],
      "metadata": {
        "id": "_A_qwFq-455m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This section imports all necessary libraries for data processing, similarity computation, evaluation, and visualization.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import normalize\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "H3xCaUKr4y-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate and Store Similarity Matrices for a Movie Recommendation System\n",
        "\n",
        "This script generates the core similarity matrices required for building a hybrid movie recommendation system, using both user-based and content-based approaches.\n",
        "\n",
        "1. **User-Based Similarity**\n",
        "   Identifies similarities between users based on their movie rating behaviors to support collaborative filtering.\n",
        "\n",
        "* *Cosine Similarity*: Measures the angle between users' rating vectors\n",
        "* *Jaccard Similarity*: Measures the proportion of commonly rated movies between users\n",
        "\n",
        "2. **Content-Based Similarity**\n",
        "   Determines the similarity between movies based on their genre attributes to enable recommendations based on movie features.\n",
        "\n",
        "* *Cosine Similarity*: Measures the angle between genre vectors\n",
        "* *Jaccard Similarity*: Measures the overlap in genre tags between movies\n",
        "\n",
        "All similarity matrices are saved locally, uploaded to GitHub and Google Drive, and will be used in subsequent scripts for generating personalized movie recommendations based on user and content similarities.\n"
      ],
      "metadata": {
        "id": "SREBYMrDj3FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Step 1: Load datasets ---\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_subset.csv\")\n",
        "movies = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/movies_subset.csv\")\n",
        "print(f\"Loaded {ratings['userId'].nunique()} users, {ratings['movieId'].nunique()} movies.\")\n",
        "\n",
        "# --- Step 2: Sample 10,000 users and save ratings ---\n",
        "ratings_sampled_file = \"ratings_sampled.csv\"\n",
        "if not os.path.exists(ratings_sampled_file):\n",
        "    sampled_user_ids = ratings['userId'].drop_duplicates().sample(n=10000, random_state=42)\n",
        "    ratings_sampled = ratings[ratings['userId'].isin(sampled_user_ids)]\n",
        "    ratings_sampled.to_csv(ratings_sampled_file, index=False)\n",
        "    print(\"Sampled ratings saved.\")\n",
        "else:\n",
        "    ratings_sampled = pd.read_csv(ratings_sampled_file)\n",
        "    print(\"Sampled ratings loaded from file.\")\n",
        "\n",
        "# --- Step 3: Create user-movie matrix ---\n",
        "user_movie_matrix = ratings_sampled.pivot(index='userId', columns='movieId', values='rating')\n",
        "user_ids = user_movie_matrix.index.tolist()\n",
        "user_means = user_movie_matrix.mean(axis=1)\n",
        "\n",
        "# --- Step 4a: Cosine Similarity (User-Based) ---\n",
        "cosine_user_file = \"cosine_user_similarity_sampled.csv\"\n",
        "if not os.path.exists(cosine_user_file):\n",
        "    user_movie_centered = user_movie_matrix.sub(user_means, axis=0).fillna(0)\n",
        "    cosine_sim_matrix = cosine_similarity(user_movie_centered.values)\n",
        "    cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=user_ids, columns=user_ids)\n",
        "    cosine_sim_df.to_csv(cosine_user_file)\n",
        "    print(\"Saved cosine user-user similarity matrix.\")\n",
        "else:\n",
        "    print(\"Cosine user-user similarity matrix already exists.\")\n",
        "\n",
        "# --- Step 4b: Jaccard Similarity (User-Based) ---\n",
        "jaccard_user_file = \"jaccard_user_similarity_sampled.csv\"\n",
        "if not os.path.exists(jaccard_user_file):\n",
        "    print(\"Computing Jaccard user-user similarity matrix...\")\n",
        "    user_movie_binary = user_movie_matrix.notna().astype(int)\n",
        "    binary_array = user_movie_binary.values.astype(bool)\n",
        "    intersection = np.dot(binary_array, binary_array.T)\n",
        "    row_sums = binary_array.sum(axis=1, keepdims=True)\n",
        "    union = row_sums + row_sums.T - intersection\n",
        "    jaccard_sim_matrix = intersection / np.maximum(union, 1)\n",
        "    jaccard_sim_df = pd.DataFrame(jaccard_sim_matrix, index=user_ids, columns=user_ids)\n",
        "    jaccard_sim_df.to_csv(jaccard_user_file)\n",
        "    print(\"Saved Jaccard user-user similarity matrix.\")\n",
        "else:\n",
        "    print(\"Jaccard user-user similarity matrix already exists.\")\n",
        "\n",
        "# --- Step 4.5: One-hot encode the genre column ---\n",
        "movies['genres'] = movies['genres'].fillna('')\n",
        "genre_dummies = movies['genres'].str.get_dummies(sep='|')\n",
        "unique_movies = pd.concat([movies[['movieId', 'title']], genre_dummies], axis=1)\n",
        "\n",
        "# --- Step 5: Content-Based Similarity using Genre ---\n",
        "genre_cols = genre_dummies.columns.tolist()\n",
        "movie_ids = unique_movies['movieId'].tolist()\n",
        "genre_matrix = unique_movies[genre_cols].values\n",
        "genre_matrix_normalized = normalize(genre_matrix, norm='l2')\n",
        "\n",
        "# --- Step 5a: Cosine Similarity (Content-Based) ---\n",
        "cosine_content_file = \"cosine_content_similarity.csv\"\n",
        "if not os.path.exists(cosine_content_file):\n",
        "    cosine_content_sim = cosine_similarity(genre_matrix_normalized)\n",
        "    cosine_content_df = pd.DataFrame(cosine_content_sim, index=movie_ids, columns=movie_ids)\n",
        "    cosine_content_df.to_csv(cosine_content_file)\n",
        "    print(\"Saved cosine content-based similarity matrix.\")\n",
        "else:\n",
        "    print(\"Cosine content-based similarity matrix already exists.\")\n",
        "\n",
        "# --- Step 5b: Jaccard Similarity (Content-Based) ---\n",
        "jaccard_content_file = \"jaccard_content_similarity.csv\"\n",
        "if not os.path.exists(jaccard_content_file):\n",
        "    print(\"Computing Jaccard content-based similarity matrix...\")\n",
        "    binary_array = genre_matrix.astype(bool)\n",
        "    intersection = np.dot(binary_array, binary_array.T)\n",
        "    row_sums = binary_array.sum(axis=1, keepdims=True)\n",
        "    union = row_sums + row_sums.T - intersection\n",
        "    jaccard_content_matrix = intersection / np.maximum(union, 1)\n",
        "    jaccard_content_df = pd.DataFrame(jaccard_content_matrix, index=movie_ids, columns=movie_ids)\n",
        "    jaccard_content_df.to_csv(jaccard_content_file)\n",
        "    print(\"Saved Jaccard content-based similarity matrix.\")\n",
        "else:\n",
        "    print(\"Jaccard content-based similarity matrix already exists.\")\n",
        "\n",
        "# --- Transpose to get movie-user matrix ---\n",
        "movie_user_matrix = user_movie_matrix.T\n",
        "\n",
        "# --- Fill missing ratings with 0 for cosine ---\n",
        "movie_user_filled = movie_user_matrix.fillna(0)\n",
        "\n",
        "# --- Step 6a: Cosine Similarity (Item-Item) ---\n",
        "cosine_item_file = \"cosine_item_similarity.csv\"\n",
        "if not os.path.exists(cosine_item_file):\n",
        "    print(\"Computing cosine item-item similarity matrix...\")\n",
        "    cosine_item_matrix = cosine_similarity(movie_user_filled.values)\n",
        "    cosine_item_df = pd.DataFrame(cosine_item_matrix, index=movie_user_matrix.index, columns=movie_user_matrix.index)\n",
        "    cosine_item_df.to_csv(cosine_item_file)\n",
        "    print(\"Saved cosine item-item similarity matrix.\")\n",
        "else:\n",
        "    print(\"Cosine item-item similarity matrix already exists.\")\n",
        "\n",
        "# --- Step 6b: Jaccard Similarity (Item-Item) ---\n",
        "jaccard_item_file = \"jaccard_item_similarity.csv\"\n",
        "if not os.path.exists(jaccard_item_file):\n",
        "    print(\"Computing Jaccard item-item similarity matrix...\")\n",
        "    movie_user_binary = movie_user_matrix.notna().astype(int)\n",
        "    binary_array = movie_user_binary.values.astype(bool)\n",
        "    intersection = np.dot(binary_array, binary_array.T)\n",
        "    row_sums = binary_array.sum(axis=1, keepdims=True)\n",
        "    union = row_sums + row_sums.T - intersection\n",
        "    jaccard_item_matrix = intersection / np.maximum(union, 1)\n",
        "    jaccard_item_df = pd.DataFrame(jaccard_item_matrix, index=movie_user_matrix.index, columns=movie_user_matrix.index)\n",
        "    jaccard_item_df.to_csv(jaccard_item_file)\n",
        "    print(\"Saved Jaccard item-item similarity matrix.\")\n",
        "else:\n",
        "    print(\"Jaccard item-item similarity matrix already exists.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSvefQtiLXV_",
        "outputId": "b396b07a-2d67-437c-b7f7-f315ebdd3cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 59029 users, 11190 movies.\n",
            "Sampled ratings loaded from file.\n",
            "Cosine user-user similarity matrix already exists.\n",
            "Jaccard user-user similarity matrix already exists.\n",
            "Cosine content-based similarity matrix already exists.\n",
            "Jaccard content-based similarity matrix already exists.\n",
            "Cosine item-item similarity matrix already exists.\n",
            "Computing Jaccard item-item similarity matrix...\n",
            "Saved Jaccard item-item similarity matrix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content-Based Recommender Systems"
      ],
      "metadata": {
        "id": "PH8llaU4PyDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF-Based Content-Based Recommender System\n",
        "\n",
        "This recommender system leverages **movie genres** as content features and constructs **user preference profiles** using TF-IDF weighting. The steps are as follows:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   Movie metadata and user ratings are loaded. Missing genre information is handled by filling empty fields.\n",
        "\n",
        "2. **Genre Feature Engineering using TF-IDF**:\n",
        "   Instead of dummy-encoding genres, the `TfidfVectorizer` is applied to the pipe-separated genre strings to produce a weighted genre matrix. This gives more importance to **rare or unique genres** while reducing the influence of common ones.\n",
        "\n",
        "3. **Similarity Matrix Construction**:\n",
        "   Cosine similarity is computed between TF-IDF genre vectors for all movies. If the matrix exists locally, it is loaded; otherwise, it is computed and saved.\n",
        "\n",
        "4. **Train-Test Split**:\n",
        "   The ratings dataset is split into training and test sets using an 80/20 split.\n",
        "\n",
        "5. **User Profile Construction**:\n",
        "   For each user, a profile vector is built by taking a **weighted average** of the TF-IDF vectors of movies they rated. The weights are their rating values. Only movies present in both the ratings and TF-IDF matrix are used.\n",
        "\n",
        "6. **Prediction Logic with Fallbacks**:\n",
        "   To predict a rating:\n",
        "\n",
        "   * If user or movie data is missing, fallback to global or movie average.\n",
        "   * Otherwise, compute cosine similarity between the user profile and the target movie's TF-IDF vector.\n",
        "   * Scale similarity linearly to a 0.5–5.0 rating range.\n",
        "\n",
        "7. **Evaluation**:\n",
        "   Only predictions with valid user profiles and TF-IDF vectors are evaluated. Metrics computed include:\n",
        "\n",
        "   * RMSE (Root Mean Squared Error)\n",
        "   * MAE (Mean Absolute Error)\n",
        "   * R² (Coefficient of Determination)\n",
        "\n",
        "8. **Fallback Tracking**:\n",
        "   Fallback occurrences are counted for analysis, distinguishing between missing profiles and movie vectors.\n"
      ],
      "metadata": {
        "id": "bapxPlTmpLzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Step 1: Load datasets\n",
        "movies = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/movies_subset.csv\")\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_sampled.csv\")\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# Step 2: Apply TF-IDF to genres\n",
        "movies['genres'] = movies['genres'].fillna('')\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split('|'))\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(movies['genres'])\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=movies['movieId'], columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "movie_ids = movies['movieId'].tolist()\n",
        "\n",
        "# Step 3: Load or compute cosine similarity matrix\n",
        "cosine_content_file_local = \"cosine_content_similarity_tfidf.csv\"\n",
        "cosine_content_file_drive_id = \"1bBdBjdJEx3YadMfnFoqRihHSS6l_Q0La\"\n",
        "cosine_drive_url = f\"https://drive.google.com/uc?id={cosine_content_file_drive_id}\"\n",
        "\n",
        "try:\n",
        "    import gdown\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"gdown\"])\n",
        "    import gdown\n",
        "\n",
        "if not os.path.exists(cosine_content_file_local):\n",
        "    print(\"Cosine content similarity file not found locally. Attempting download...\")\n",
        "    try:\n",
        "        gdown.download(cosine_drive_url, cosine_content_file_local, quiet=False)\n",
        "    except Exception as e:\n",
        "        print(\"Download failed. Computing cosine similarity matrix...\")\n",
        "        cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "        cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=movies['movieId'], columns=movies['movieId'])\n",
        "        cosine_sim_df.to_csv(cosine_content_file_local)\n",
        "        print(\"Cosine content similarity matrix computed and saved locally.\")\n",
        "else:\n",
        "    print(\"Cosine content similarity matrix found locally.\")\n",
        "    cosine_sim_df = pd.read_csv(cosine_content_file_local, index_col=0)\n",
        "    cosine_sim_df.columns = cosine_sim_df.columns.astype(int)\n",
        "    cosine_sim_df.index = cosine_sim_df.index.astype(int)\n",
        "\n",
        "# Step 4: Split ratings into training and test sets\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Create user-movie matrix and metadata from training set\n",
        "user_movie_matrix = train_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "user_ids = user_movie_matrix.index.tolist()\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "movie_means = train_ratings.groupby('movieId')['rating'].mean()\n",
        "\n",
        "# Step 6: Build user profile using TF-IDF genre weights\n",
        "def build_user_profile(user_id):\n",
        "    if user_id not in user_movie_matrix.index:\n",
        "        return None\n",
        "    rated_movies = user_movie_matrix.loc[user_id].dropna()\n",
        "    if rated_movies.empty:\n",
        "        return None\n",
        "    tfidf_subset = tfidf_df.loc[rated_movies.index.intersection(tfidf_df.index)]\n",
        "    if tfidf_subset.empty:\n",
        "        return None\n",
        "    ratings_vector = rated_movies.loc[tfidf_subset.index].values.reshape(-1, 1)\n",
        "    weighted_profile = np.dot(ratings_vector.T, tfidf_subset.values) / ratings_vector.sum()\n",
        "    return weighted_profile.flatten()\n",
        "\n",
        "# A. Track Fallbacks\n",
        "fallback_counter = {'no_profile': 0, 'no_movie_vector': 0, 'success': 0}\n",
        "\n",
        "# Step 7: Prediction function with fallback and tracking\n",
        "def predict_content_based(user_id, movie_id):\n",
        "    if movie_id not in cosine_sim_df.columns or user_id not in user_movie_matrix.index:\n",
        "        fallback_counter['no_movie_vector'] += 1\n",
        "        return global_mean\n",
        "    user_profile = build_user_profile(user_id)\n",
        "    if user_profile is None:\n",
        "        fallback_counter['no_profile'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "    if movie_id not in tfidf_df.index:\n",
        "        fallback_counter['no_movie_vector'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "    movie_vector = tfidf_df.loc[movie_id].values\n",
        "    sim = cosine_similarity([user_profile], [movie_vector])[0][0]\n",
        "    fallback_counter['success'] += 1\n",
        "    predicted_rating = sim * 5.0\n",
        "    return max(0.5, min(predicted_rating, 5.0))\n",
        "\n",
        "# B. Filter Test Set (Optional)\n",
        "def evaluate_model(test_df, filter_strict=True):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_id = int(row['userId'])\n",
        "        movie_id = int(row['movieId'])\n",
        "\n",
        "        if filter_strict:\n",
        "            if user_id not in user_movie_matrix.index or movie_id not in cosine_sim_df.columns:\n",
        "                continue\n",
        "            user_profile = build_user_profile(user_id)\n",
        "            if user_profile is None:\n",
        "                continue\n",
        "\n",
        "        true_rating = row['rating']\n",
        "        pred_rating = predict_content_based(user_id, movie_id)\n",
        "        y_true.append(true_rating)\n",
        "        y_pred.append(pred_rating)\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"No valid test samples for evaluation.\")\n",
        "        return\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(\"\\nFiltered Content-Based Recommender Performance\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE : {mae:.4f}\")\n",
        "    print(f\"R²  : {r2:.4f}\")\n",
        "\n",
        "# Step 8: Run evaluation\n",
        "evaluate_model(test_ratings)\n",
        "\n",
        "# Display fallback usage\n",
        "print(\"\\nPrediction Source Breakdown\")\n",
        "print(f\"Predictions with user profile: {fallback_counter['success']}\")\n",
        "print(f\"Fallback (no profile): {fallback_counter['no_profile']}\")\n",
        "print(f\"Fallback (no movie vector): {fallback_counter['no_movie_vector']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtb1GUMc93nQ",
        "outputId": "46b71cde-525a-480f-8237-c5e3faac0afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Cosine content similarity matrix found locally.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filtered Content-Based Recommender Performance\n",
            "RMSE: 2.4144\n",
            "MAE : 2.0859\n",
            "R²  : -4.3994\n",
            "\n",
            "Prediction Source Breakdown\n",
            "Predictions with user profile: 1854\n",
            "Fallback (no profile): 0\n",
            "Fallback (no movie vector): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy Genre-Based Content Recommender with Centered User Profiles and Baseline Adjustment\n",
        "\n",
        "This recommender system uses movie genre information (one-hot encoded) to construct user profiles and generate predictions through content similarity. Key steps in the methodology are as follows:\n",
        "\n",
        "**1. Data Loading and Preprocessing**\n",
        "Movie metadata and user ratings are loaded from external CSV sources. Missing genre entries are filled with empty strings. Genres are then converted into binary indicators using `str.get_dummies()` to form a genre feature matrix for all movies.\n",
        "\n",
        "**2. Cosine Similarity Matrix Computation**\n",
        "Using the genre matrix, cosine similarity is computed between movies. This matrix helps determine how similar one movie is to another based on genre overlap. If a precomputed similarity matrix exists, it is loaded from local storage. Otherwise, it is computed and saved for future use.\n",
        "\n",
        "**3. Train-Test Split**\n",
        "The ratings dataset is randomly split into 80% training and 20% testing using `train_test_split`. The training set is used to build user profiles, and the test set is used for performance evaluation.\n",
        "\n",
        "**4. User Profile Construction (Centered Ratings)**\n",
        "For each user, the profile is built as a weighted average of genre vectors of the movies they’ve rated. The weights are centered ratings—each rating minus the user’s mean rating. This captures a user’s unique preference pattern, rather than their general generosity or harshness.\n",
        "\n",
        "**5. Prediction Logic with Fallback Handling**\n",
        "To predict a user’s rating for a movie:\n",
        "\n",
        "* If either the user profile or the movie vector is missing, a fallback value is returned (either the global mean or the movie mean).\n",
        "* If both are available, cosine similarity between the user profile and the movie vector is computed.\n",
        "* The similarity score is scaled and added to the movie’s average rating to generate the prediction:\n",
        "  `predicted_rating = movie_mean + (similarity_score * 1.0)`\n",
        "* The predicted value is constrained within the allowable rating range of 0.5 to 5.0.\n",
        "\n",
        "**6. Evaluation**\n",
        "The model is evaluated on test samples where both user profiles and movie vectors exist. It reports the following performance metrics:\n",
        "\n",
        "* RMSE (Root Mean Squared Error)\n",
        "* MAE (Mean Absolute Error)\n",
        "* R² (Coefficient of Determination)\n",
        "\n",
        "**7. Fallback Tracking**\n",
        "During prediction, fallback events are tracked and categorized as:\n",
        "\n",
        "* Successful predictions using user profiles and genre vectors,\n",
        "* Fallback due to missing user profile,\n",
        "* Fallback due to missing movie vector.\n"
      ],
      "metadata": {
        "id": "swnFdCUYCb0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Step 1: Load datasets\n",
        "movies = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/movies_subset.csv\")\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_sampled.csv\")\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# Step 2: Genre matrix preparation\n",
        "movies['genres'] = movies['genres'].fillna('')\n",
        "genre_dummies = movies['genres'].str.get_dummies(sep='|')\n",
        "movie_features = pd.concat([movies[['movieId', 'title']], genre_dummies], axis=1)\n",
        "genre_matrix = genre_dummies.values\n",
        "movie_ids = movie_features['movieId'].tolist()\n",
        "\n",
        "# Step 3: Load or compute cosine similarity matrix\n",
        "cosine_content_file_local = \"cosine_content_similarity.csv\"\n",
        "cosine_content_file_drive_id = \"1bBdBjdJEx3YadMfnFoqRihHSS6l_Q0La\"\n",
        "cosine_drive_url = f\"https://drive.google.com/uc?id={cosine_content_file_drive_id}\"\n",
        "\n",
        "try:\n",
        "    import gdown\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"gdown\"])\n",
        "    import gdown\n",
        "\n",
        "if not os.path.exists(cosine_content_file_local):\n",
        "    print(\"Cosine content similarity file not found locally. Attempting download...\")\n",
        "    try:\n",
        "        gdown.download(cosine_drive_url, cosine_content_file_local, quiet=False)\n",
        "    except Exception as e:\n",
        "        print(\"Download failed. Computing cosine similarity matrix...\")\n",
        "        genre_matrix_norm = genre_matrix / np.linalg.norm(genre_matrix, axis=1, keepdims=True)\n",
        "        cosine_sim_matrix = cosine_similarity(genre_matrix_norm)\n",
        "        cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=movie_ids, columns=movie_ids)\n",
        "        cosine_sim_df.to_csv(cosine_content_file_local)\n",
        "        print(\"Cosine content similarity matrix computed and saved locally.\")\n",
        "else:\n",
        "    print(\"Cosine content similarity matrix found locally.\")\n",
        "    cosine_sim_df = pd.read_csv(cosine_content_file_local, index_col=0)\n",
        "    cosine_sim_df.columns = cosine_sim_df.columns.astype(int)\n",
        "    cosine_sim_df.index = cosine_sim_df.index.astype(int)\n",
        "\n",
        "# Step 4: Split ratings into training and test sets\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Create user-movie matrix and metadata from training set\n",
        "user_movie_matrix = train_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "user_ids = user_movie_matrix.index.tolist()\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "movie_means = train_ratings.groupby('movieId')['rating'].mean()\n",
        "\n",
        "# Step 6: Build centered user profile\n",
        "def build_user_profile(user_id):\n",
        "    if user_id not in user_movie_matrix.index:\n",
        "        return None\n",
        "    rated_movies = user_movie_matrix.loc[user_id].dropna()\n",
        "    if rated_movies.empty:\n",
        "        return None\n",
        "    rated_movie_ids = rated_movies.index\n",
        "    rated_genres = genre_dummies.loc[movies['movieId'].isin(rated_movie_ids)]\n",
        "    user_mean = rated_movies.mean()\n",
        "    centered_ratings = (rated_movies - user_mean).values.reshape(-1, 1)\n",
        "    profile = np.dot(centered_ratings.T, rated_genres.values) / len(centered_ratings)\n",
        "    return profile.flatten()\n",
        "\n",
        "# A. Track Fallbacks\n",
        "fallback_counter = {'no_profile': 0, 'no_movie_vector': 0, 'success': 0}\n",
        "\n",
        "# Step 7: Prediction function with improved logic\n",
        "def predict_content_based(user_id, movie_id):\n",
        "    if movie_id not in cosine_sim_df.columns or user_id not in user_movie_matrix.index:\n",
        "        fallback_counter['no_movie_vector'] += 1\n",
        "        return global_mean\n",
        "\n",
        "    user_profile = build_user_profile(user_id)\n",
        "    if user_profile is None:\n",
        "        fallback_counter['no_profile'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    movie_idx = movies[movies['movieId'] == movie_id].index\n",
        "    if movie_idx.empty:\n",
        "        fallback_counter['no_movie_vector'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    fallback_counter['success'] += 1\n",
        "    movie_vector = genre_dummies.loc[movie_idx[0]].values\n",
        "    sim = cosine_similarity([user_profile], [movie_vector])[0][0]\n",
        "    movie_baseline = movie_means.get(movie_id, global_mean)\n",
        "    predicted_rating = movie_baseline + sim * 1.0\n",
        "    return max(0.5, min(predicted_rating, 5.0))\n",
        "\n",
        "# B. Filter Test Set (Optional)\n",
        "def evaluate_model(test_df, filter_strict=True):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_id = int(row['userId'])\n",
        "        movie_id = int(row['movieId'])\n",
        "\n",
        "        if filter_strict:\n",
        "            if user_id not in user_movie_matrix.index or movie_id not in cosine_sim_df.columns:\n",
        "                continue\n",
        "            user_profile = build_user_profile(user_id)\n",
        "            if user_profile is None:\n",
        "                continue\n",
        "\n",
        "        true_rating = row['rating']\n",
        "        pred_rating = predict_content_based(user_id, movie_id)\n",
        "        y_true.append(true_rating)\n",
        "        y_pred.append(pred_rating)\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"No valid test samples for evaluation.\")\n",
        "        return\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(\"\\nFiltered Content-Based Recommender Performance\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE : {mae:.4f}\")\n",
        "    print(f\"R²  : {r2:.4f}\")\n",
        "\n",
        "# Step 8: Run evaluation\n",
        "evaluate_model(test_ratings)\n",
        "\n",
        "# Display fallback usage\n",
        "print(\"\\nPrediction Source Breakdown\")\n",
        "print(f\"Predictions with user profile: {fallback_counter['success']}\")\n",
        "print(f\"Fallback (no profile): {fallback_counter['no_profile']}\")\n",
        "print(f\"Fallback (no movie vector): {fallback_counter['no_movie_vector']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDOYHSYYymDO",
        "outputId": "11654dc4-0d0f-4e0c-8f84-4a0ea7d9ab5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Cosine content similarity matrix found locally.\n",
            "\n",
            "Filtered Content-Based Recommender Performance\n",
            "RMSE: 1.1041\n",
            "MAE : 0.8485\n",
            "R²  : -0.1291\n",
            "\n",
            "Prediction Source Breakdown\n",
            "Predictions with user profile: 1854\n",
            "Fallback (no profile): 0\n",
            "Fallback (no movie vector): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jaccard-Based Content Recommender System\n",
        "\n",
        "### Overview\n",
        "\n",
        "This model is a content-based recommender system that uses **Jaccard similarity** to measure the overlap between user genre preferences and movie genres. Unlike cosine similarity, which considers vector angles and magnitude, Jaccard measures the proportion of shared genres to the total number of unique genres across two sets. This makes it especially effective when working with binary data such as multi-hot encoded genre vectors.\n",
        "\n",
        "### Workflow Description\n",
        "\n",
        "**Step 1: Data Loading**\n",
        "The system loads a subset of the MovieLens dataset. Movie genres are parsed into binary (multi-hot) vectors where each column represents a genre and a 1 indicates the presence of that genre in a movie.\n",
        "\n",
        "**Step 2: Jaccard Similarity Matrix**\n",
        "The system attempts to download a precomputed Jaccard similarity matrix from Google Drive. If the file is unavailable or cannot be downloaded, the matrix is computed locally using pairwise Jaccard similarity between movie genre vectors. This similarity reflects how many genres two movies share divided by the total number of genres they have combined.\n",
        "\n",
        "**Step 3: Train-Test Split**\n",
        "The ratings dataset is split into training and testing subsets. A user-movie matrix is constructed from the training data to track which users have rated which movies and what those ratings were.\n",
        "\n",
        "**Step 4: User Profile Construction**\n",
        "Each user’s profile is built using the genres of the movies they have rated. The genres are weighted by centered ratings (i.e., ratings adjusted around the user’s mean) to reflect preferences more accurately. The result is a single vector representing the user’s overall taste in genres.\n",
        "\n",
        "**Step 5: Rating Prediction**\n",
        "To predict a rating, the model compares the user’s profile with a candidate movie’s genre vector using Jaccard similarity. This similarity score is scaled and added to the movie’s average rating (baseline) to produce a predicted rating. If either the user profile or movie vector is unavailable, the model falls back to the movie mean or global mean rating.\n",
        "\n",
        "**Step 6: Model Evaluation**\n",
        "The model’s performance is evaluated using RMSE, MAE, and R² metrics on the test set. Fallbacks are also tracked, providing insight into how often predictions rely on default values due to missing profiles or vectors.\n",
        "\n",
        "### Merits of This Approach\n",
        "\n",
        "This approach is easy to interpret and computationally straightforward. It is particularly well-suited for binary genre data. Since it builds user preferences based on content rather than relying on other users, it works well even when rating overlap between users is low. It is also more robust in cases where cold-start items are present, as long as those items are tagged with genre data. Once the Jaccard matrix is computed, predictions are fast and efficient.\n"
      ],
      "metadata": {
        "id": "TIM9fwhyEcmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from sklearn.metrics import jaccard_score, mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Load datasets\n",
        "movies = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/movies_subset.csv\")\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_sampled.csv\")\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# Step 2: Prepare binary genre matrix\n",
        "movies['genres'] = movies['genres'].fillna('')\n",
        "genre_dummies = movies['genres'].str.get_dummies(sep='|')\n",
        "movie_ids = movies['movieId'].tolist()\n",
        "genre_matrix = genre_dummies.values\n",
        "\n",
        "# Step 3: Load or compute Jaccard similarity matrix\n",
        "jaccard_file_local = \"jaccard_content_similarity.csv\"\n",
        "jaccard_file_drive_id = \"1K8Jg_EOELLEqcEpwtFaG8RfA_xTQzVO9\"\n",
        "jaccard_drive_url = f\"https://drive.google.com/uc?id={jaccard_file_drive_id}\"\n",
        "\n",
        "try:\n",
        "    import gdown\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"gdown\"])\n",
        "    import gdown\n",
        "\n",
        "if not os.path.exists(jaccard_file_local):\n",
        "    print(\"Jaccard similarity file not found locally. Attempting download...\")\n",
        "    try:\n",
        "        gdown.download(jaccard_drive_url, jaccard_file_local, quiet=False)\n",
        "        print(\"Download successful.\")\n",
        "        jaccard_sim_df = pd.read_csv(jaccard_file_local, index_col=0)\n",
        "        jaccard_sim_df.columns = jaccard_sim_df.columns.astype(int)\n",
        "        jaccard_sim_df.index = jaccard_sim_df.index.astype(int)\n",
        "    except Exception as e:\n",
        "        print(\"Download failed. Computing Jaccard similarity matrix...\")\n",
        "        n = genre_matrix.shape[0]\n",
        "        jaccard_matrix = np.zeros((n, n))\n",
        "        for i in tqdm(range(n)):\n",
        "            for j in range(i, n):\n",
        "                sim = jaccard_score(genre_matrix[i], genre_matrix[j])\n",
        "                jaccard_matrix[i, j] = sim\n",
        "                jaccard_matrix[j, i] = sim\n",
        "        jaccard_sim_df = pd.DataFrame(jaccard_matrix, index=movie_ids, columns=movie_ids)\n",
        "        jaccard_sim_df.to_csv(jaccard_file_local)\n",
        "        print(\"Jaccard similarity matrix computed and saved.\")\n",
        "else:\n",
        "    print(\"Jaccard similarity file found locally. Loading...\")\n",
        "    jaccard_sim_df = pd.read_csv(jaccard_file_local, index_col=0)\n",
        "    jaccard_sim_df.columns = jaccard_sim_df.columns.astype(int)\n",
        "    jaccard_sim_df.index = jaccard_sim_df.index.astype(int)\n",
        "\n",
        "# Step 4: Split data\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Create user-movie matrix and metadata\n",
        "user_movie_matrix = train_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "user_ids = user_movie_matrix.index.tolist()\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "movie_means = train_ratings.groupby('movieId')['rating'].mean()\n",
        "\n",
        "# Step 6: Build centered user profile\n",
        "def build_user_profile(user_id):\n",
        "    if user_id not in user_movie_matrix.index:\n",
        "        return None\n",
        "    rated_movies = user_movie_matrix.loc[user_id].dropna()\n",
        "    if rated_movies.empty:\n",
        "        return None\n",
        "    rated_movie_ids = rated_movies.index\n",
        "    rated_genres = genre_dummies.loc[movies['movieId'].isin(rated_movie_ids)]\n",
        "    user_mean = rated_movies.mean()\n",
        "    centered_ratings = (rated_movies - user_mean).values.reshape(-1, 1)\n",
        "    profile = np.dot(centered_ratings.T, rated_genres.values) / len(centered_ratings)\n",
        "    return profile.flatten()\n",
        "\n",
        "# A. Track Fallbacks\n",
        "fallback_counter = {'no_profile': 0, 'no_movie_vector': 0, 'success': 0}\n",
        "\n",
        "# Step 7: Prediction function using Jaccard\n",
        "def predict_content_based(user_id, movie_id):\n",
        "    if movie_id not in jaccard_sim_df.columns or user_id not in user_movie_matrix.index:\n",
        "        fallback_counter['no_movie_vector'] += 1\n",
        "        return global_mean\n",
        "\n",
        "    user_profile = build_user_profile(user_id)\n",
        "    if user_profile is None:\n",
        "        fallback_counter['no_profile'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    movie_idx = movies[movies['movieId'] == movie_id].index\n",
        "    if movie_idx.empty:\n",
        "        fallback_counter['no_movie_vector'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    fallback_counter['success'] += 1\n",
        "    movie_vector = genre_dummies.loc[movie_idx[0]].values\n",
        "    sim = jaccard_score(user_profile > 0, movie_vector > 0)\n",
        "    movie_baseline = movie_means.get(movie_id, global_mean)\n",
        "    predicted_rating = movie_baseline + sim * 1.0\n",
        "    return max(0.5, min(predicted_rating, 5.0))\n",
        "\n",
        "# B. Evaluate Model\n",
        "def evaluate_model(test_df, filter_strict=True):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_id = int(row['userId'])\n",
        "        movie_id = int(row['movieId'])\n",
        "\n",
        "        if filter_strict:\n",
        "            if user_id not in user_movie_matrix.index or movie_id not in jaccard_sim_df.columns:\n",
        "                continue\n",
        "            user_profile = build_user_profile(user_id)\n",
        "            if user_profile is None:\n",
        "                continue\n",
        "\n",
        "        true_rating = row['rating']\n",
        "        pred_rating = predict_content_based(user_id, movie_id)\n",
        "        y_true.append(true_rating)\n",
        "        y_pred.append(pred_rating)\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"No valid test samples for evaluation.\")\n",
        "        return\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(\"\\nFiltered Content-Based Recommender Performance (Jaccard)\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE : {mae:.4f}\")\n",
        "    print(f\"R²  : {r2:.4f}\")\n",
        "\n",
        "# Step 8: Run evaluation\n",
        "evaluate_model(test_ratings)\n",
        "\n",
        "# Display fallback usage\n",
        "print(\"\\nPrediction Source Breakdown\")\n",
        "print(f\"Predictions with user profile: {fallback_counter['success']}\")\n",
        "print(f\"Fallback (no profile): {fallback_counter['no_profile']}\")\n",
        "print(f\"Fallback (no movie vector): {fallback_counter['no_movie_vector']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsrkbJHZEdgR",
        "outputId": "26cf469a-210d-43ae-95e7-30de1966b88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Jaccard similarity file not found locally. Attempting download...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1K8Jg_EOELLEqcEpwtFaG8RfA_xTQzVO9\n",
            "From (redirected): https://drive.google.com/uc?id=1K8Jg_EOELLEqcEpwtFaG8RfA_xTQzVO9&confirm=t&uuid=ebf9aa7e-aad6-4c6d-88e5-328d280fa89d\n",
            "To: /content/jaccard_content_similarity.csv\n",
            "100%|██████████| 822M/822M [00:19<00:00, 42.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download successful.\n",
            "\n",
            "Filtered Content-Based Recommender Performance (Jaccard)\n",
            "RMSE: 1.1105\n",
            "MAE : 0.8486\n",
            "R²  : -0.1422\n",
            "\n",
            "Prediction Source Breakdown\n",
            "Predictions with user profile: 1854\n",
            "Fallback (no profile): 0\n",
            "Fallback (no movie vector): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User-Based Recommender Systems"
      ],
      "metadata": {
        "id": "aOe8JhcTQFCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User-Based Collaborative Filtering (No Bias Adjustment)\n",
        "\n",
        "### Overview\n",
        "\n",
        "This recommender system is built on a **user-user collaborative filtering approach** where predictions are made by leveraging the ratings of users who have exhibited similar behavior in the past. In contrast to the bias-aware model, this version makes predictions purely based on **weighted averages of neighbor ratings**, without adjusting for user or item tendencies. Cosine similarity is used to determine the degree of similarity between users.\n",
        "\n",
        "### Workflow Description\n",
        "\n",
        "**Step 1: Data Loading**\n",
        "The system loads a sample from the MovieLens dataset, which includes ratings by users for various movies.\n",
        "\n",
        "**Step 2: Train-Test Split and Matrix Preparation**\n",
        "The dataset is split into training and testing sets. The training data is converted into a user-movie matrix, with users as rows and movies as columns, where each cell contains a user’s rating or remains blank if the user hasn’t rated that movie.\n",
        "\n",
        "**Step 3: User Similarity Matrix**\n",
        "The system checks for the existence of a precomputed cosine similarity matrix between users. If not available, it calculates the cosine similarity based on the filled-in user-movie matrix, where missing values are replaced with zero. This matrix represents how closely aligned users are in their rating behavior.\n",
        "\n",
        "**Step 4: Fallback Tracking**\n",
        "Fallback counters are initialized to track cases where predictions cannot be made using neighbor data — either because no similar users exist or because those users haven’t rated the movie in question.\n",
        "\n",
        "**Step 5: Prediction Without Bias Adjustment**\n",
        "To make a prediction for a (user, movie) pair:\n",
        "\n",
        "* The model identifies similar users based on cosine similarity.\n",
        "* It checks whether any of those users rated the target movie.\n",
        "* If they have, the model calculates a **weighted average** of their ratings, using the similarity scores as weights.\n",
        "* The predicted rating is this weighted average, clipped between 0.5 and 5.0 to remain within valid bounds.\n",
        "* If no valid neighbors are found, the model falls back to the movie’s average rating or the global average.\n",
        "\n",
        "**Step 6: Evaluation**\n",
        "The model is evaluated using RMSE, MAE, and R² on the test set. Predictions are made only where possible; fallback logic ensures coverage for all test points, but the evaluation can be restricted to valid predictions if desired.\n",
        "\n",
        "**Step 7: Reporting**\n",
        "The system prints out the number of predictions made using similar users and tracks the number of fallback cases where no similarity or rating data were available.\n",
        "\n",
        "### Merits of This Approach\n",
        "\n",
        "This model is **simple, fast, and intuitive**, making it well-suited for baseline implementations and cases where interpretability and scalability are priorities. It performs well when there is **sufficient rating overlap** between users. Since it avoids bias correction, it is **computationally lighter** and may be preferred in scenarios where rating biases are not a significant concern or where real-time prediction speed is critical. However, in highly sparse datasets, its accuracy may suffer due to lack of sufficient overlap between users."
      ],
      "metadata": {
        "id": "39kDfK3bObYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gdown\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Step 1: Load ratings data\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_sampled.csv\")\n",
        "print(\"Ratings data loaded successfully.\")\n",
        "\n",
        "# Step 2: Split ratings and build matrix\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "user_movie_matrix = train_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "movie_means = train_ratings.groupby('movieId')['rating'].mean()\n",
        "\n",
        "# Step 3: Load or compute user similarity matrix\n",
        "user_sim_file_local = \"user_similarity_matrix.csv\"\n",
        "user_sim_drive_id = \"1W9Byb7cdbUPJId8GzDt55yLQ0SaLLHc5\"\n",
        "user_sim_drive_url = f\"https://drive.google.com/uc?id={user_sim_drive_id}\"\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(user_sim_file_local):\n",
        "        print(\"User similarity matrix not found locally. Attempting download...\")\n",
        "        gdown.download(user_sim_drive_url, user_sim_file_local, quiet=False)\n",
        "\n",
        "    user_sim_df = pd.read_csv(user_sim_file_local, index_col=0)\n",
        "    user_sim_df.columns = user_sim_df.columns.astype(int)\n",
        "    user_sim_df.index = user_sim_df.index.astype(int)\n",
        "    print(\"User similarity matrix loaded.\")\n",
        "except Exception as e:\n",
        "    print(\"Download failed or file not found. Computing user similarity matrix...\")\n",
        "    user_matrix_filled = user_movie_matrix.fillna(0)\n",
        "    similarity_matrix = cosine_similarity(user_matrix_filled)\n",
        "    user_sim_df = pd.DataFrame(similarity_matrix, index=user_matrix_filled.index, columns=user_matrix_filled.index)\n",
        "    user_sim_df.to_csv(user_sim_file_local)\n",
        "    print(\"User similarity matrix computed and saved.\")\n",
        "\n",
        "# Step 4: Fallback tracker\n",
        "fallback_counter = {'no_sim_users': 0, 'no_rating': 0, 'success': 0}\n",
        "\n",
        "# Step 5: Prediction function (no bias adjustment)\n",
        "def predict_user_based(user_id, movie_id, top_k=20):\n",
        "    if user_id not in user_sim_df.index or movie_id not in user_movie_matrix.columns:\n",
        "        fallback_counter['no_rating'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    sim_users = user_sim_df.loc[user_id].sort_values(ascending=False).drop(user_id, errors='ignore')\n",
        "    sim_users = sim_users[sim_users > 0]\n",
        "\n",
        "    if sim_users.empty:\n",
        "        fallback_counter['no_sim_users'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    neighbors = sim_users.index\n",
        "    neighbor_ratings = user_movie_matrix.loc[user_movie_matrix.index.intersection(neighbors), movie_id].dropna()\n",
        "\n",
        "    if neighbor_ratings.empty:\n",
        "        fallback_counter['no_rating'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    top_users = neighbor_ratings.index\n",
        "    sims = sim_users.loc[top_users].values\n",
        "    ratings = neighbor_ratings.loc[top_users].values\n",
        "\n",
        "    weighted_sum = np.dot(sims, ratings)\n",
        "    norm_factor = np.sum(np.abs(sims))\n",
        "\n",
        "    if norm_factor == 0:\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    prediction = weighted_sum / norm_factor\n",
        "    fallback_counter['success'] += 1\n",
        "    return max(0.5, min(prediction, 5.0))\n",
        "\n",
        "# Step 6: Evaluation\n",
        "def evaluate_user_model(test_df, filter_strict=True):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_id = int(row['userId'])\n",
        "        movie_id = int(row['movieId'])\n",
        "\n",
        "        if filter_strict:\n",
        "            if user_id not in user_sim_df.index or movie_id not in user_movie_matrix.columns:\n",
        "                continue\n",
        "\n",
        "        true_rating = row['rating']\n",
        "        pred_rating = predict_user_based(user_id, movie_id)\n",
        "        y_true.append(true_rating)\n",
        "        y_pred.append(pred_rating)\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"No valid test samples for evaluation.\")\n",
        "        return\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(\"\\nUser-Based Collaborative Filtering (No Bias Adjustment)\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE : {mae:.4f}\")\n",
        "    print(f\"R²  : {r2:.4f}\")\n",
        "\n",
        "# Step 7: Run evaluation\n",
        "evaluate_user_model(test_ratings)\n",
        "\n",
        "# Step 8: Print fallback summary\n",
        "print(\"\\nPrediction Source Breakdown\")\n",
        "print(f\"Predictions using neighbors: {fallback_counter['success']}\")\n",
        "print(f\"Fallback (no similar users): {fallback_counter['no_sim_users']}\")\n",
        "print(f\"Fallback (no ratings from neighbors): {fallback_counter['no_rating']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk-bgg_9JPnO",
        "outputId": "ab21b4bf-6b5c-46dd-91c3-ce4f4dd9b5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratings data loaded successfully.\n",
            "User similarity matrix loaded.\n",
            "\n",
            "User-Based Collaborative Filtering (No Bias Adjustment)\n",
            "RMSE: 1.0937\n",
            "MAE : 0.8372\n",
            "R²  : -0.1002\n",
            "\n",
            "Prediction Source Breakdown\n",
            "Predictions using neighbors: 1006\n",
            "Fallback (no similar users): 1398\n",
            "Fallback (no ratings from neighbors): 366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## User-Based Collaborative Filtering with Bias Adjustment\n",
        "\n",
        "### Overview\n",
        "\n",
        "This recommender system is based on **user-user collaborative filtering**, where predictions are made by identifying users with similar tastes and leveraging their ratings. It enhances the basic user-based model by incorporating **bias correction**, accounting for differences in how users rate movies (some rate higher or lower than average) and how some movies tend to receive consistently high or low ratings. Cosine similarity is used to determine how similar users are based on their rating patterns.\n",
        "\n",
        "### Workflow Description\n",
        "\n",
        "**Step 1: Data Loading**\n",
        "The system loads a sample of the MovieLens ratings dataset, which includes user IDs, movie IDs, and ratings.\n",
        "\n",
        "**Step 2: Train-Test Split and Rating Matrix Construction**\n",
        "The dataset is divided into training and test sets. From the training set, a user-movie matrix is created, where each row represents a user and each column a movie, filled with ratings when available. This matrix forms the basis for computing similarities.\n",
        "\n",
        "**Step 3: User Similarity Matrix**\n",
        "The system attempts to load a precomputed cosine similarity matrix between users. If the file is missing or corrupted, it computes the similarity directly from the user-movie matrix by filling missing values with zero and applying cosine similarity. The result is a square matrix where each value represents how similar two users are based on their historical ratings.\n",
        "\n",
        "**Step 4: Fallback Tracking**\n",
        "The model includes fallback counters to record how often it fails to make predictions due to missing users, missing movie ratings, or lack of overlap in ratings.\n",
        "\n",
        "**Step 5: Prediction Function with Bias Adjustment**\n",
        "To predict a user’s rating for a given movie:\n",
        "\n",
        "* The model retrieves the top similar users based on cosine similarity.\n",
        "* It checks whether those similar users have rated the target movie.\n",
        "* It computes the **neighbor-centered ratings** by subtracting each neighbor’s mean rating from their rating of the movie.\n",
        "* These centered ratings are then weighted by their similarity to the target user.\n",
        "* Finally, the prediction is computed by combining the global average rating with the target user’s bias (their mean rating minus global mean), the movie’s bias (its average rating minus global mean), and the weighted contribution from neighbors.\n",
        "\n",
        "**Step 6: Evaluation**\n",
        "The model is evaluated using standard metrics: RMSE, MAE, and R². Only predictions that can be made using the trained similarity matrix and user profiles are included in the evaluation. This provides insight into both the accuracy and reliability of the model.\n",
        "\n",
        "**Step 7: Reporting**\n",
        "At the end of execution, the system prints out the number of predictions successfully made using neighbors, as well as the number of fallback cases where either no similar users or no neighbor ratings were found.\n",
        "\n",
        "### Merits of This Approach\n",
        "\n",
        "This model offers a more realistic and fair method of prediction by **accounting for systematic bias** in the data. Users who tend to rate generously or harshly are adjusted relative to the average, and movies that are universally liked or disliked are also normalized. This makes the predictions more reliable, especially in cases where raw ratings alone may misrepresent user preferences. Additionally, cosine similarity provides a robust and scalable way to compare users, especially in sparse matrices common in recommendation systems."
      ],
      "metadata": {
        "id": "lmW5YnlwN-FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gdown\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Step 1: Load ratings data\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_sampled.csv\")\n",
        "print(\"Ratings data loaded successfully.\")\n",
        "\n",
        "# Step 2: Split ratings\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "user_movie_matrix = train_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "movie_means = train_ratings.groupby('movieId')['rating'].mean()\n",
        "\n",
        "# Step 3: Load or compute user similarity matrix\n",
        "user_sim_file_local = \"user_similarity_matrix.csv\"\n",
        "user_sim_drive_id = \"1W9Byb7cdbUPJId8GzDt55yLQ0SaLLHc5\"\n",
        "user_sim_drive_url = f\"https://drive.google.com/uc?id={user_sim_drive_id}\"\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(user_sim_file_local):\n",
        "        print(\"User similarity matrix not found locally. Attempting download...\")\n",
        "        gdown.download(user_sim_drive_url, user_sim_file_local, quiet=False)\n",
        "\n",
        "    user_sim_df = pd.read_csv(user_sim_file_local, index_col=0)\n",
        "    user_sim_df.columns = user_sim_df.columns.astype(int)\n",
        "    user_sim_df.index = user_sim_df.index.astype(int)\n",
        "    print(\"User similarity matrix loaded.\")\n",
        "except Exception as e:\n",
        "    print(\"Download failed or file corrupt. Computing user similarity matrix...\")\n",
        "    user_matrix_filled = user_movie_matrix.fillna(0)\n",
        "    similarity_matrix = cosine_similarity(user_matrix_filled)\n",
        "    user_sim_df = pd.DataFrame(similarity_matrix, index=user_matrix_filled.index, columns=user_matrix_filled.index)\n",
        "    user_sim_df.to_csv(user_sim_file_local)\n",
        "    print(\"User similarity matrix computed and saved.\")\n",
        "\n",
        "# Step 4: Track fallbacks\n",
        "fallback_counter = {'no_sim_users': 0, 'no_rating': 0, 'success': 0}\n",
        "\n",
        "# Step 5: Bias-aware prediction function\n",
        "def predict_user_based(user_id, movie_id, top_k=20):\n",
        "    if user_id not in user_sim_df.index or movie_id not in user_movie_matrix.columns:\n",
        "        fallback_counter['no_rating'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    sim_users = user_sim_df.loc[user_id].sort_values(ascending=False).drop(user_id, errors='ignore')\n",
        "    sim_users = sim_users[sim_users > 0]\n",
        "\n",
        "    if sim_users.empty:\n",
        "        fallback_counter['no_sim_users'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    neighbors = sim_users.index\n",
        "    neighbor_ratings = user_movie_matrix.loc[user_movie_matrix.index.intersection(neighbors), movie_id].dropna()\n",
        "\n",
        "    if neighbor_ratings.empty:\n",
        "        fallback_counter['no_rating'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    top_users = neighbor_ratings.index\n",
        "    sims = sim_users.loc[top_users].values\n",
        "    ratings = neighbor_ratings.loc[top_users].values\n",
        "\n",
        "    # Bias correction using neighbor means\n",
        "    neighbor_means = user_movie_matrix.loc[top_users].mean(axis=1).values\n",
        "    adjusted_ratings = ratings - neighbor_means\n",
        "\n",
        "    weighted_sum = np.dot(sims, adjusted_ratings)\n",
        "    norm_factor = np.sum(np.abs(sims))\n",
        "\n",
        "    if norm_factor == 0:\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    # Add user and item bias\n",
        "    bu = user_movie_matrix.loc[user_id].mean() - global_mean if user_id in user_movie_matrix.index else 0\n",
        "    bi = movie_means.get(movie_id, global_mean) - global_mean\n",
        "    prediction = global_mean + bu + bi + (weighted_sum / norm_factor)\n",
        "\n",
        "    fallback_counter['success'] += 1\n",
        "    return max(0.5, min(prediction, 5.0))\n",
        "\n",
        "# Step 6: Evaluation function\n",
        "def evaluate_user_model(test_df, filter_strict=True):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_id = int(row['userId'])\n",
        "        movie_id = int(row['movieId'])\n",
        "\n",
        "        if filter_strict:\n",
        "            if user_id not in user_sim_df.index or movie_id not in user_movie_matrix.columns:\n",
        "                continue\n",
        "\n",
        "        true_rating = row['rating']\n",
        "        pred_rating = predict_user_based(user_id, movie_id)\n",
        "        y_true.append(true_rating)\n",
        "        y_pred.append(pred_rating)\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"No valid test samples for evaluation.\")\n",
        "        return\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(\"\\nUser-Based Collaborative Filtering with Bias Adjustment\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE : {mae:.4f}\")\n",
        "    print(f\"R²  : {r2:.4f}\")\n",
        "\n",
        "# Step 7: Run evaluation\n",
        "evaluate_user_model(test_ratings)\n",
        "\n",
        "# Step 8: Print fallback summary\n",
        "print(\"\\nPrediction Source Breakdown\")\n",
        "print(f\"Predictions using neighbors: {fallback_counter['success']}\")\n",
        "print(f\"Fallback (no similar users): {fallback_counter['no_sim_users']}\")\n",
        "print(f\"Fallback (no ratings from neighbors): {fallback_counter['no_rating']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC0v7OwjHMI3",
        "outputId": "6afba038-3864-414b-da5e-fa7e96f77730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratings data loaded successfully.\n",
            "User similarity matrix loaded.\n",
            "\n",
            "User-Based Collaborative Filtering with Bias Adjustment\n",
            "RMSE: 1.1083\n",
            "MAE : 0.8396\n",
            "R²  : -0.1299\n",
            "\n",
            "Prediction Source Breakdown\n",
            "Predictions using neighbors: 1006\n",
            "Fallback (no similar users): 1398\n",
            "Fallback (no ratings from neighbors): 366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF-Based User-User Collaborative Filtering\n",
        "\n",
        "### Overview\n",
        "\n",
        "This recommender system enhances traditional collaborative filtering by **encoding user genre preferences using TF-IDF** (Term Frequency-Inverse Document Frequency) derived from movie genres. Instead of comparing raw ratings, this model computes similarity between users based on the **weighted semantic meaning of genres they've rated highly**. The approach integrates both content and collaborative elements, making it a hybrid recommender system with a user-user architecture.\n",
        "\n",
        "### Workflow Description\n",
        "\n",
        "**Step 1: Data Loading**\n",
        "The system loads movie metadata and user ratings from the MovieLens dataset.\n",
        "\n",
        "**Step 2: Genre TF-IDF Encoding**\n",
        "Movie genres are treated as tokenized text inputs and transformed into a TF-IDF matrix. This encodes each movie's genre profile in a way that emphasizes **unique or rare genres** over common ones, improving the system’s ability to differentiate between user tastes.\n",
        "\n",
        "**Step 3: Matrix Construction**\n",
        "The user-movie matrix is built from training data. The global average rating and per-movie average ratings are also computed for fallback use during prediction.\n",
        "\n",
        "**Step 4: User Profile Construction**\n",
        "Each user's profile is generated by taking a **weighted average of the TF-IDF vectors** of the movies they have rated. Ratings serve as weights to reflect the degree of interest in each movie's genre composition. This results in a compact vector that summarizes a user's genre preferences in a nuanced way.\n",
        "\n",
        "**Step 5: User Similarity Matrix**\n",
        "Cosine similarity is applied to the user TF-IDF profile matrix, producing a **user-user similarity matrix** based on genre affinity instead of rating overlap.\n",
        "\n",
        "**Step 6: Fallback Tracker**\n",
        "Fallback counters are initialized to keep track of scenarios where predictions must rely on global or item-level averages due to missing data.\n",
        "\n",
        "**Step 7: Prediction Function**\n",
        "To predict a rating for a given (user, movie) pair:\n",
        "\n",
        "* Similar users are identified via the TF-IDF cosine similarity matrix.\n",
        "* Their ratings for the target movie are extracted.\n",
        "* A weighted average is calculated using similarity scores.\n",
        "* If no similar users or ratings are available, the system falls back to the movie mean or global mean.\n",
        "\n",
        "**Step 8: Evaluation**\n",
        "The model is evaluated using standard metrics: RMSE, MAE, and R². These metrics provide insight into how well the system predicts user ratings compared to actual outcomes.\n",
        "\n",
        "**Step 9–10: Reporting**\n",
        "Performance results are printed, along with fallback usage statistics indicating how often the model relied on global/item-level means versus collaborative logic.\n",
        "\n",
        "### Merits of This Approach\n",
        "\n",
        "* **Genre-Aware Similarity**: Unlike standard collaborative filtering, this model captures **semantic similarity in genre preferences**, even when users have no movies in common.\n",
        "* **Cold-Start Support**: Since it uses TF-IDF of genres, it is more resilient in cold-start scenarios with new users or movies.\n",
        "* **Hybrid Nature**: This method fuses content-based profiling (TF-IDF) with user-based collaborative filtering, allowing it to generalize better than either approach alone.\n",
        "* **Improved Diversity**: TF-IDF reduces the dominance of frequent genres (e.g., \"Drama\"), leading to more **diverse and tailored recommendations**."
      ],
      "metadata": {
        "id": "kR71ACDrJMi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gdown\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Step 1: Load datasets\n",
        "movies = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/movies_subset.csv\")\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_sampled.csv\")\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# Step 2: Prepare genre TF-IDF matrix\n",
        "movies['genres'] = movies['genres'].fillna('')\n",
        "tfidf = TfidfVectorizer(tokenizer=lambda x: x.split('|'))\n",
        "tfidf_matrix = tfidf.fit_transform(movies['genres'])\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=movies['movieId'], columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Step 3: Train-test split and user-movie matrix\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "user_movie_matrix = train_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "movie_means = train_ratings.groupby('movieId')['rating'].mean()\n",
        "\n",
        "# Step 4: Create user TF-IDF profiles\n",
        "def build_user_tfidf_profile(user_id):\n",
        "    if user_id not in user_movie_matrix.index:\n",
        "        return None\n",
        "    rated_movies = user_movie_matrix.loc[user_id].dropna()\n",
        "    if rated_movies.empty:\n",
        "        return None\n",
        "    tfidf_subset = tfidf_df.loc[rated_movies.index.intersection(tfidf_df.index)]\n",
        "    if tfidf_subset.empty:\n",
        "        return None\n",
        "    ratings_vector = rated_movies.loc[tfidf_subset.index].values.reshape(-1, 1)\n",
        "    weighted_profile = np.dot(ratings_vector.T, tfidf_subset.values) / ratings_vector.sum()\n",
        "    return weighted_profile.flatten()\n",
        "\n",
        "print(\"Building user TF-IDF profiles...\")\n",
        "user_profiles = {}\n",
        "for user_id in user_movie_matrix.index:\n",
        "    profile = build_user_tfidf_profile(user_id)\n",
        "    if profile is not None:\n",
        "        user_profiles[user_id] = profile\n",
        "user_profile_df = pd.DataFrame(user_profiles).T\n",
        "\n",
        "# Step 5: Compute cosine similarity between users\n",
        "print(\"Computing user-user cosine similarity...\")\n",
        "user_sim_matrix = cosine_similarity(user_profile_df)\n",
        "user_sim_df = pd.DataFrame(user_sim_matrix, index=user_profile_df.index, columns=user_profile_df.index)\n",
        "\n",
        "# Step 6: Fallback tracker\n",
        "fallback_counter = {'no_sim_users': 0, 'no_rating': 0, 'success': 0}\n",
        "\n",
        "# Step 7: Prediction function\n",
        "def predict_user_based_tfidf(user_id, movie_id, top_k=20):\n",
        "    if user_id not in user_sim_df.index or movie_id not in user_movie_matrix.columns:\n",
        "        fallback_counter['no_rating'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    sim_users = user_sim_df.loc[user_id].sort_values(ascending=False).drop(user_id, errors='ignore')\n",
        "    sim_users = sim_users[sim_users > 0]\n",
        "\n",
        "    if sim_users.empty:\n",
        "        fallback_counter['no_sim_users'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    neighbors = sim_users.index\n",
        "    neighbor_ratings = user_movie_matrix.loc[user_movie_matrix.index.intersection(neighbors), movie_id].dropna()\n",
        "\n",
        "    if neighbor_ratings.empty:\n",
        "        fallback_counter['no_rating'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    top_users = neighbor_ratings.index\n",
        "    sims = sim_users.loc[top_users].values\n",
        "    ratings = neighbor_ratings.loc[top_users].values\n",
        "\n",
        "    weighted_sum = np.dot(sims, ratings)\n",
        "    norm_factor = np.sum(np.abs(sims))\n",
        "\n",
        "    if norm_factor == 0:\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    prediction = weighted_sum / norm_factor\n",
        "    fallback_counter['success'] += 1\n",
        "    return max(0.5, min(prediction, 5.0))\n",
        "\n",
        "# Step 8: Evaluation\n",
        "def evaluate_tfidf_user_model(test_df, filter_strict=True):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_id = int(row['userId'])\n",
        "        movie_id = int(row['movieId'])\n",
        "\n",
        "        if filter_strict:\n",
        "            if user_id not in user_sim_df.index or movie_id not in user_movie_matrix.columns:\n",
        "                continue\n",
        "\n",
        "        true_rating = row['rating']\n",
        "        pred_rating = predict_user_based_tfidf(user_id, movie_id)\n",
        "        y_true.append(true_rating)\n",
        "        y_pred.append(pred_rating)\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"No valid test samples for evaluation.\")\n",
        "        return\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(\"\\nTF-IDF-Based User-User Collaborative Filtering Performance\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE : {mae:.4f}\")\n",
        "    print(f\"R²  : {r2:.4f}\")\n",
        "\n",
        "# Step 9: Run evaluation\n",
        "evaluate_tfidf_user_model(test_ratings)\n",
        "\n",
        "# Step 10: Fallback report\n",
        "print(\"\\nPrediction Source Breakdown\")\n",
        "print(f\"Predictions using neighbors: {fallback_counter['success']}\")\n",
        "print(f\"Fallback (no similar users): {fallback_counter['no_sim_users']}\")\n",
        "print(f\"Fallback (no ratings from neighbors): {fallback_counter['no_rating']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYcrH1vDL21f",
        "outputId": "937efd1a-c1dc-4eea-c70a-4258a06b6ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building user TF-IDF profiles...\n",
            "Computing user-user cosine similarity...\n",
            "\n",
            "TF-IDF-Based User-User Collaborative Filtering Performance\n",
            "RMSE: 1.1297\n",
            "MAE : 0.8664\n",
            "R²  : -0.2199\n",
            "\n",
            "Prediction Source Breakdown\n",
            "Predictions using neighbors: 1384\n",
            "Fallback (no similar users): 0\n",
            "Fallback (no ratings from neighbors): 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jaccard-Based User-User Collaborative Filtering\n",
        "\n",
        "### Overview\n",
        "\n",
        "This recommender system builds a user-user collaborative filtering model by measuring **Jaccard similarity** between users based on the items they’ve rated. Unlike cosine similarity, which factors in rating values, **Jaccard similarity only considers binary co-occurrence** (i.e., whether two users have rated the same items). The model integrates a **bias-aware prediction function** to improve rating accuracy by accounting for global, user, and item-level biases.\n",
        "\n",
        "### Workflow Description\n",
        "\n",
        "**Step 1: Data Loading**\n",
        "The system loads the user-movie ratings data from the MovieLens dataset. It calculates the global mean rating and individual movie mean ratings for use in fallback scenarios.\n",
        "\n",
        "**Step 2: Matrix Construction**\n",
        "The user-movie matrix is constructed via a pivot table. Each row represents a user, each column a movie, and each cell the corresponding rating.\n",
        "\n",
        "**Step 3: Jaccard Similarity Matrix**\n",
        "The Jaccard similarity between every user pair is computed based on binary presence of ratings (i.e., did both users rate the same movies?).\n",
        "If a precomputed similarity matrix is provided via a Google Drive link, the system downloads and loads it to avoid recomputation.\n",
        "\n",
        "**Step 4: Fallback Tracker**\n",
        "A fallback counter is initialized to track how often predictions rely on global or item-level means due to lack of neighbor information.\n",
        "\n",
        "**Step 5: Bias-Aware Prediction Function**\n",
        "To predict a rating for a given user-movie pair:\n",
        "\n",
        "* The model identifies the top-k most similar users who have rated the target movie.\n",
        "* Each neighbor’s mean rating is subtracted from their rating for the movie to center it.\n",
        "* A weighted average of these adjusted ratings is calculated using Jaccard similarity scores.\n",
        "* The final prediction incorporates:\n",
        "\n",
        "  * **Global mean rating**\n",
        "  * **User bias**: how the user's average rating deviates from the global mean\n",
        "  * **Item bias**: how the item's average rating deviates from the global mean\n",
        "  * **Neighborhood adjustment** from similar users\n",
        "\n",
        "**Step 6: Evaluation**\n",
        "The model is evaluated against a test set using standard metrics:\n",
        "\n",
        "* RMSE (Root Mean Squared Error)\n",
        "* MAE (Mean Absolute Error)\n",
        "* R² (Coefficient of Determination)\n",
        "\n",
        "**Step 7–8: Reporting**\n",
        "Evaluation results are printed, followed by fallback breakdown:\n",
        "\n",
        "* How many predictions were made using collaborative logic\n",
        "* How many relied on fallback mechanisms due to missing ratings or similar users\n",
        "\n",
        "### Merits of This Approach\n",
        "\n",
        "* **Simplicity & Interpretability**: Jaccard similarity is easy to interpret, capturing pure overlap without requiring numerical ratings.\n",
        "* **Bias Handling**: By explicitly incorporating user and item bias, the model avoids common pitfalls of over- or under-predicting due to personal or item-based tendencies.\n",
        "* **Cold-Start Ready**: If no similar users or ratings are found, the model gracefully falls back to average values, maintaining robustness.\n",
        "* **No Rating Values Needed for Similarity**: The model can function effectively even with just rating presence data (e.g., implicit feedback scenarios).\n",
        "* **Complementary with Other Models**: This approach can be used alongside cosine-based or TF-IDF models in an ensemble setup for improved performance.\n"
      ],
      "metadata": {
        "id": "ytowGZ60dRPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gdown\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Step 1: Load ratings data\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_sampled.csv\")\n",
        "print(\"Ratings data loaded successfully.\")\n",
        "\n",
        "# Step 2: Split ratings\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "user_movie_matrix = train_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "movie_means = train_ratings.groupby('movieId')['rating'].mean()\n",
        "\n",
        "# Step 3: Load Jaccard user similarity matrix from Google Drive\n",
        "jaccard_sim_local = \"jaccard_user_similarity_sampled.csv\"\n",
        "jaccard_drive_id = \"1zNbErjNpko2MoPtEbqi52cAbMJxyzrP2\"\n",
        "jaccard_drive_url = f\"https://drive.google.com/uc?id={jaccard_drive_id}\"\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(jaccard_sim_local):\n",
        "        print(\"Downloading Jaccard similarity matrix from Google Drive...\")\n",
        "        gdown.download(jaccard_drive_url, jaccard_sim_local, quiet=False)\n",
        "\n",
        "    user_sim_df = pd.read_csv(jaccard_sim_local, index_col=0)\n",
        "    user_sim_df.columns = user_sim_df.columns.astype(int)\n",
        "    user_sim_df.index = user_sim_df.index.astype(int)\n",
        "    print(\"Jaccard similarity matrix loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Download failed or file corrupt. Please verify the link or download manually.\")\n",
        "    raise e\n",
        "\n",
        "# Step 4: Fallback counter\n",
        "fallback_counter = {'no_sim_users': 0, 'no_rating': 0, 'success': 0}\n",
        "\n",
        "# Step 5: Prediction function (Bias-Aware)\n",
        "def predict_user_based(user_id, movie_id, top_k=20):\n",
        "    if user_id not in user_sim_df.index or movie_id not in user_movie_matrix.columns:\n",
        "        fallback_counter['no_rating'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    sim_users = user_sim_df.loc[user_id].sort_values(ascending=False).drop(user_id, errors='ignore')\n",
        "    sim_users = sim_users[sim_users > 0]\n",
        "\n",
        "    if sim_users.empty:\n",
        "        fallback_counter['no_sim_users'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    neighbors = sim_users.index\n",
        "    neighbor_ratings = user_movie_matrix.loc[user_movie_matrix.index.intersection(neighbors), movie_id].dropna()\n",
        "\n",
        "    if neighbor_ratings.empty:\n",
        "        fallback_counter['no_rating'] += 1\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    top_users = neighbor_ratings.index\n",
        "    sims = sim_users.loc[top_users].values\n",
        "    ratings = neighbor_ratings.loc[top_users].values\n",
        "\n",
        "    neighbor_means = user_movie_matrix.loc[top_users].mean(axis=1).values\n",
        "    adjusted_ratings = ratings - neighbor_means\n",
        "\n",
        "    weighted_sum = np.dot(sims, adjusted_ratings)\n",
        "    norm_factor = np.sum(np.abs(sims))\n",
        "\n",
        "    if norm_factor == 0:\n",
        "        return movie_means.get(movie_id, global_mean)\n",
        "\n",
        "    bu = user_movie_matrix.loc[user_id].mean() - global_mean if user_id in user_movie_matrix.index else 0\n",
        "    bi = movie_means.get(movie_id, global_mean) - global_mean\n",
        "    prediction = global_mean + bu + bi + (weighted_sum / norm_factor)\n",
        "\n",
        "    fallback_counter['success'] += 1\n",
        "    return max(0.5, min(prediction, 5.0))\n",
        "\n",
        "# Step 6: Evaluation\n",
        "def evaluate_user_model(test_df, filter_strict=True):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_id = int(row['userId'])\n",
        "        movie_id = int(row['movieId'])\n",
        "\n",
        "        if filter_strict:\n",
        "            if user_id not in user_sim_df.index or movie_id not in user_movie_matrix.columns:\n",
        "                continue\n",
        "\n",
        "        true_rating = row['rating']\n",
        "        pred_rating = predict_user_based(user_id, movie_id)\n",
        "        y_true.append(true_rating)\n",
        "        y_pred.append(pred_rating)\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"No valid test samples for evaluation.\")\n",
        "        return\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(\"\\nJaccard-Based User-Based Collaborative Filtering with Bias Adjustment\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE : {mae:.4f}\")\n",
        "    print(f\"R²  : {r2:.4f}\")\n",
        "\n",
        "# Step 7: Run evaluation\n",
        "evaluate_user_model(test_ratings)\n",
        "\n",
        "# Step 8: Print fallback summary\n",
        "print(\"\\nPrediction Source Breakdown\")\n",
        "print(f\"Predictions using neighbors: {fallback_counter['success']}\")\n",
        "print(f\"Fallback (no similar users): {fallback_counter['no_sim_users']}\")\n",
        "print(f\"Fallback (no ratings from neighbors): {fallback_counter['no_rating']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wguEoipJdSNa",
        "outputId": "f47b53a9-649a-4b0c-a099-9511624ed186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratings data loaded successfully.\n",
            "Jaccard similarity matrix loaded successfully.\n",
            "\n",
            "Jaccard-Based User-Based Collaborative Filtering with Bias Adjustment\n",
            "RMSE: 1.2267\n",
            "MAE : 0.9389\n",
            "R²  : -0.3840\n",
            "\n",
            "Prediction Source Breakdown\n",
            "Predictions using neighbors: 2770\n",
            "Fallback (no similar users): 0\n",
            "Fallback (no ratings from neighbors): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Item-Item Collaborative Filtering"
      ],
      "metadata": {
        "id": "QUAWg77rlBc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine-Based Item-Item Collaborative Filtering\n",
        "\n",
        "### Overview\n",
        "\n",
        "This recommender system builds an **item-item collaborative filtering model** by measuring **cosine similarity between items** based on how users rate them. It predicts how much a user will like a particular movie by analyzing **ratings from the same user on similar items**, weighted by similarity scores. The model also incorporates a **bias-aware prediction function** to improve accuracy by adjusting for global, user, and item-specific rating tendencies.\n",
        "\n",
        "### Workflow Description\n",
        "\n",
        "**Step 1: Data Loading**\n",
        "The system loads the MovieLens user-movie ratings dataset and calculates:\n",
        "\n",
        "* The **global average rating**\n",
        "* **Per-movie average ratings** (item bias)\n",
        "* **Per-user average ratings** (user bias)\n",
        "\n",
        "These statistics serve as fallbacks when neighborhood information is insufficient.\n",
        "\n",
        "**Step 2: Matrix Construction**\n",
        "A user-movie matrix is created using a pivot table, where:\n",
        "\n",
        "* Rows represent users\n",
        "* Columns represent movies\n",
        "* Cell values are the ratings provided\n",
        "\n",
        "**Step 3: Cosine Similarity Matrix (Item-Based)**\n",
        "A cosine similarity matrix is computed between movies (items) based on user ratings.\n",
        "\n",
        "* Each movie is represented by a vector of user ratings.\n",
        "* Cosine similarity measures how similarly users rate each pair of movies.\n",
        "* If a precomputed matrix is provided via a Google Drive link, it is downloaded to save time.\n",
        "\n",
        "**Step 4: Fallback Tracker**\n",
        "A dictionary is initialized to keep track of fallback cases:\n",
        "\n",
        "* No similar items found\n",
        "* User has no prior ratings\n",
        "* Successful predictions made using neighbors\n",
        "\n",
        "**Step 5: Bias-Aware Prediction Function**\n",
        "To predict a rating for a (user, movie) pair:\n",
        "\n",
        "* The model identifies items that the user has already rated.\n",
        "* It then retrieves cosine similarities between those items and the target movie.\n",
        "* Using the **top-k most similar items** that the user rated:\n",
        "\n",
        "  * A **weighted average of their adjusted ratings** is computed.\n",
        "  * Adjustments are made by removing each item’s bias (how it deviates from global mean).\n",
        "* The final prediction is the sum of:\n",
        "\n",
        "  * Global average\n",
        "  * User bias\n",
        "  * Item bias\n",
        "  * Weighted contribution from the neighborhood\n",
        "\n",
        "**Step 6: Evaluation**\n",
        "The model is evaluated using a held-out test set with standard metrics:\n",
        "\n",
        "* RMSE (Root Mean Squared Error)\n",
        "* MAE (Mean Absolute Error)\n",
        "* R² (Coefficient of Determination)\n",
        "\n",
        "**Step 7–8: Reporting**\n",
        "The script prints:\n",
        "\n",
        "* The performance scores (RMSE, MAE, R²)\n",
        "* A summary of how often fallback logic was triggered\n",
        "\n",
        "### Merits of This Approach\n",
        "\n",
        "* **Interpretability**: Cosine similarity captures how similarly users rate pairs of items, making the model transparent and explainable.\n",
        "* **Personalized to User Behavior**: Recommendations are grounded in the user’s individual rating history.\n",
        "* **Bias Correction**: Incorporates global, user, and item-level biases for more realistic predictions.\n",
        "* **Resilient to Sparsity**: Even with a sparse rating matrix, predictions are possible using fallback to movie or global averages.\n",
        "* **Scalable**: Precomputing the item-item similarity matrix makes prediction fast and scalable at runtime.\n",
        "* **Cold-Start Friendly (Partially)**: Works better for new users than new items, especially when they’ve rated just a few movies.\n"
      ],
      "metadata": {
        "id": "orwR5MUwlKDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gdown\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Step 1: Load ratings data\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_sampled.csv\")\n",
        "print(\"Ratings data loaded successfully.\")\n",
        "\n",
        "# Step 2: Train-test split\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "user_movie_matrix = train_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "movie_means = train_ratings.groupby('movieId')['rating'].mean()\n",
        "user_means = user_movie_matrix.mean(axis=1)\n",
        "\n",
        "# Step 3: Load item-item similarity matrix from Google Drive\n",
        "item_sim_file_local = \"item_similarity_matrix.csv\"\n",
        "item_sim_drive_id = \"1zNbErjNpko2MoPtEbqi52cAbMJxyzrP2\"\n",
        "item_sim_drive_url = f\"https://drive.google.com/uc?id={item_sim_drive_id}\"\n",
        "\n",
        "if not os.path.exists(item_sim_file_local):\n",
        "    print(\"Downloading item similarity matrix...\")\n",
        "    gdown.download(item_sim_drive_url, item_sim_file_local, quiet=False)\n",
        "\n",
        "item_sim_df = pd.read_csv(item_sim_file_local, index_col=0)\n",
        "item_sim_df.columns = item_sim_df.columns.astype(str)\n",
        "item_sim_df.index = item_sim_df.index.astype(str)\n",
        "print(\"Item similarity matrix loaded.\")\n",
        "\n",
        "# Step 4: Fallback counter\n",
        "fallback_counter = {'no_sim_items': 0, 'no_rating': 0, 'success': 0}\n",
        "\n",
        "# Step 5: Bias-aware prediction function\n",
        "def predict_item_based(user_id, movie_id, top_k=20):\n",
        "    movie_id = str(movie_id)\n",
        "    if user_id not in user_movie_matrix.index or movie_id not in item_sim_df.columns:\n",
        "        fallback_counter['no_rating'] += 1\n",
        "        return movie_means.get(int(movie_id), global_mean)\n",
        "\n",
        "    user_ratings = user_movie_matrix.loc[user_id].dropna()\n",
        "    rated_movie_ids = user_ratings.index.astype(str)\n",
        "    similarities = item_sim_df.loc[movie_id, rated_movie_ids].dropna()\n",
        "    similarities = similarities[similarities > 0]\n",
        "\n",
        "    if similarities.empty:\n",
        "        fallback_counter['no_sim_items'] += 1\n",
        "        return movie_means.get(int(movie_id), global_mean)\n",
        "\n",
        "    top_items = similarities.sort_values(ascending=False).head(top_k)\n",
        "    sims = top_items.values\n",
        "    ratings = user_ratings.loc[top_items.index.astype(int)].values\n",
        "    item_bias = movie_means.loc[top_items.index.astype(int)].values - global_mean\n",
        "    adjusted_ratings = ratings - item_bias\n",
        "\n",
        "    weighted_sum = np.dot(sims, adjusted_ratings)\n",
        "    norm_factor = np.sum(np.abs(sims))\n",
        "\n",
        "    if norm_factor == 0:\n",
        "        return movie_means.get(int(movie_id), global_mean)\n",
        "\n",
        "    bu = user_means.get(user_id, global_mean) - global_mean\n",
        "    bi = movie_means.get(int(movie_id), global_mean) - global_mean\n",
        "    prediction = global_mean + bu + bi + (weighted_sum / norm_factor)\n",
        "\n",
        "    fallback_counter['success'] += 1\n",
        "    return max(0.5, min(prediction, 5.0))\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "def evaluate_item_model(test_df, filter_strict=True):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_id = int(row['userId'])\n",
        "        movie_id = int(row['movieId'])\n",
        "\n",
        "        if filter_strict and (str(movie_id) not in item_sim_df.columns or user_id not in user_movie_matrix.index):\n",
        "            continue\n",
        "\n",
        "        true_rating = row['rating']\n",
        "        pred_rating = predict_item_based(user_id, movie_id)\n",
        "        y_true.append(true_rating)\n",
        "        y_pred.append(pred_rating)\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"No valid test samples.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nItem-Based Collaborative Filtering with Bias Adjustment\")\n",
        "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.4f}\")\n",
        "    print(f\"MAE : {mean_absolute_error(y_true, y_pred):.4f}\")\n",
        "    print(f\"R²  : {r2_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "# Step 7: Run evaluation\n",
        "evaluate_item_model(test_ratings)\n",
        "\n",
        "# Step 8: Print fallback summary\n",
        "print(\"\\nPrediction Source Breakdown\")\n",
        "print(f\"Predictions using neighbors: {fallback_counter['success']}\")\n",
        "print(f\"Fallback (no similar items): {fallback_counter['no_sim_items']}\")\n",
        "print(f\"Fallback (no ratings from user): {fallback_counter['no_rating']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "cqABrjIWlXba",
        "outputId": "8b7ce935-f591-4faa-8b10-c4ba28695394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratings data loaded successfully.\n",
            "Downloading item similarity matrix...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zNbErjNpko2MoPtEbqi52cAbMJxyzrP2\n",
            "From (redirected): https://drive.google.com/uc?id=1zNbErjNpko2MoPtEbqi52cAbMJxyzrP2&confirm=t&uuid=69d2924a-46e5-45bc-91fc-f0395e27ac24\n",
            "To: /content/item_similarity_matrix.csv\n",
            "100%|██████████| 401M/401M [00:13<00:00, 29.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item similarity matrix loaded.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index(['1', '605'], dtype='object', name='movieId')] are in the [index]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-4275630435>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Step 7: Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mevaluate_item_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# Step 8: Print fallback summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-4275630435>\u001b[0m in \u001b[0;36mevaluate_item_model\u001b[0;34m(test_df, filter_strict)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtrue_rating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mpred_rating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_item_based\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_rating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_rating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-4275630435>\u001b[0m in \u001b[0;36mpredict_item_based\u001b[0;34m(user_id, movie_id, top_k)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0muser_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_movie_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mrated_movie_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_sim_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmovie_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrated_movie_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msimilarities\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_ellipsis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;31m# no multi-index, so validate all of the indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1087\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m                 \u001b[0;31m# This is an elided recursive call to iloc/loc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"not applicable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_deprecated_callable_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1418\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['1', '605'], dtype='object', name='movieId')] are in the [index]\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Content-Based Filtering Using Genre Vectors and Cosine Similarity\n",
        "\n",
        "This code implements a **content-based recommender system** using movie genres. Each movie is represented as a binary (multi-hot) vector based on its associated genres (e.g., Action, Comedy, Drama). The steps include:\n",
        "\n",
        "* Normalizing the genre vectors using **L2 norm** so that each vector has unit length.\n",
        "* Calculating **cosine similarity** between movie vectors to measure how similar their genre compositions are.\n",
        "* Creating a function that, given a movie title, returns the top-N most similar movies (excluding itself) based purely on genre similarity.\n",
        "\n",
        "This technique does not rely on user ratings — instead, it recommends items that are similar in content (genre) to a given movie.\n",
        "\n",
        "This code implements a **non-personalized content-based recommender system** using only movie genres. It does **not use user ratings or preferences**. Instead, it recommends movies that are **similar in genre** to a specified movie.\n",
        "\n",
        "#### How It Works:\n",
        "\n",
        "* Each movie is represented as a binary (multi-hot encoded) vector across genres (e.g., Action, Comedy, Drama).\n",
        "* These vectors are **L2-normalized** so that all movies lie on a unit hypersphere — making **cosine similarity** an effective way to measure closeness.\n",
        "* Given a movie title, the model:\n",
        "\n",
        "  * Finds its genre vector.\n",
        "  * Computes cosine similarity to all other movies.\n",
        "  * Returns the top-N most similar movies (excluding itself).\n",
        "\n",
        "#### What It Does Not Do:\n",
        "\n",
        "* It does **not use any user data** (no `userId`, no ratings).\n",
        "* There is **no personalization**. All users will get the same recommendations for a given movie.\n",
        "\n",
        "#### Best Use Case:\n",
        "\n",
        "This type of model is ideal when:\n",
        "\n",
        "* You have **no user data** (cold start).\n",
        "* You want to recommend movies **based on content alone** (e.g., genre-based similarity).\n",
        "* You’re building a basic recommender system that can later be enhanced with collaborative filtering or hybrid techniques.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KD0olH695T2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "# --- Step 1: Use unique movies for similarity computation ---\n",
        "unique_movies = movies.copy().reset_index(drop=True)\n",
        "\n",
        "# --- Step 2: Normalize genre matrix ---\n",
        "genre_cols = all_genres  # Assumes 'all_genres' is your list of genre columns\n",
        "genre_matrix = unique_movies[genre_cols].values\n",
        "genre_matrix_normalized = normalize(genre_matrix, norm='l2')\n",
        "\n",
        "# --- Step 3: Create title-to-index mapping ---\n",
        "movie_idx = pd.Series(unique_movies.index, index=unique_movies['title']).drop_duplicates()\n",
        "\n",
        "# --- Step 4: Recommendation Function Based on Genre Similarity ---\n",
        "def get_recommendations(title, topN=20):\n",
        "    if title not in movie_idx:\n",
        "        return f\"Movie '{title}' not found in dataset.\"\n",
        "\n",
        "    idx = movie_idx[title]\n",
        "    query_vector = genre_matrix_normalized[idx].reshape(1, -1)\n",
        "    sim_scores = cosine_similarity(query_vector, genre_matrix_normalized)[0]\n",
        "\n",
        "    # Rank and filter out the movie itself\n",
        "    sim_scores = list(enumerate(sim_scores))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:topN+1]\n",
        "\n",
        "    # Output as list of (title, similarity score)\n",
        "    recommendations = [(unique_movies['title'][i], score) for i, score in sim_scores]\n",
        "    return recommendations\n",
        "\n",
        "# --- Step 5: Use Fixed Target Movie ---\n",
        "target_title = 'O.J.: Made in America (2016)'\n",
        "\n",
        "# Print explanation\n",
        "print(\"\\nContent-based Recommendations using GENRE similarity (cosine distance):\")\n",
        "\n",
        "# Run recommendation\n",
        "if target_title in movie_idx:\n",
        "    print(f\"\\nTop 20 Movies Most Similar in Genre to '{target_title}':\")\n",
        "    for title, sim in get_recommendations(target_title):\n",
        "        print(f\"{title:<45} Similarity: {sim:.4f}\")\n",
        "else:\n",
        "    print(f\"Movie '{target_title}' not found in the dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24cDrAHnBkhH",
        "outputId": "b3bdece5-648f-4d00-d138-c882b13a9834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Content-based Recommendations using GENRE similarity (cosine distance):\n",
            "\n",
            "Top 20 Movies Most Similar in Genre to 'O.J.: Made in America (2016)':\n",
            "Catwalk (1996)                                Similarity: 1.0000\n",
            "Anne Frank Remembered (1995)                  Similarity: 1.0000\n",
            "Man of the Year (1995)                        Similarity: 1.0000\n",
            "Crumb (1994)                                  Similarity: 1.0000\n",
            "Unzipped (1995)                               Similarity: 1.0000\n",
            "Hoop Dreams (1994)                            Similarity: 1.0000\n",
            "Wonderful, Horrible Life of Leni Riefenstahl, The (Macht der Bilder: Leni Riefenstahl, Die) (1993) Similarity: 1.0000\n",
            "War Room, The (1993)                          Similarity: 1.0000\n",
            "Celluloid Closet, The (1995)                  Similarity: 1.0000\n",
            "Haunted World of Edward D. Wood Jr., The (1996) Similarity: 1.0000\n",
            "Maya Lin: A Strong Clear Vision (1994)        Similarity: 1.0000\n",
            "Synthetic Pleasures (1995)                    Similarity: 1.0000\n",
            "Microcosmos (Microcosmos: Le peuple de l'herbe) (1996) Similarity: 1.0000\n",
            "Line King: The Al Hirschfeld Story, The (1996) Similarity: 1.0000\n",
            "Snowriders (1996)                             Similarity: 1.0000\n",
            "When We Were Kings (1996)                     Similarity: 1.0000\n",
            "Thin Blue Line, The (1988)                    Similarity: 1.0000\n",
            "Paris Is Burning (1990)                       Similarity: 1.0000\n",
            "Koyaanisqatsi (a.k.a. Koyaanisqatsi: Life Out of Balance) (1983) Similarity: 1.0000\n",
            "Paradise Lost: The Child Murders at Robin Hood Hills (1996) Similarity: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports tools for normalizing feature vectors and computing similarity between them.\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "# Use only unique movie rows for similarity matrix\n",
        "# Copies the movies DataFrame and resets the index to ensure each movie is uniquely indexed.\n",
        "unique_movies = movies.copy().reset_index(drop=True)\n",
        "\n",
        "# Normalize genre matrix\n",
        "# Extracts the genre vectors for each movie (multi-hot encoded).\n",
        "# Applies L2 normalization so that all genre vectors have a length of 1 (helps with cosine similarity).\n",
        "genre_cols = all_genres\n",
        "genre_matrix = unique_movies[genre_cols].values\n",
        "genre_matrix_normalized = normalize(genre_matrix, norm='l2')\n",
        "\n",
        "# Create title-to-index map for unique movies\n",
        "# Creates a dictionary-like mapping from movie titles to their corresponding row index — used to look up vector positions.\n",
        "movie_idx = pd.Series(unique_movies.index, index=unique_movies['title']).drop_duplicates()\n",
        "\n",
        "# Define function to get recommendations\n",
        "# Defines a function that takes a movie title and returns the top N most similar movies.\n",
        "def get_recommendations(title, topN=20):\n",
        "    if title not in movie_idx:\n",
        "        return f\"Movie '{title}' not found in dataset.\"\n",
        "\n",
        "    idx = movie_idx[title]\n",
        "    query_vector = genre_matrix_normalized[idx].reshape(1, -1)\n",
        "    sim_scores = cosine_similarity(query_vector, genre_matrix_normalized)[0]\n",
        "\n",
        "    # Enumerate and sort scores, excluding the movie itself\n",
        "    sim_scores = list(enumerate(sim_scores))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:topN+1]\n",
        "\n",
        "    # Format output: list of (title, similarity) tuples\n",
        "    recommendations = [(unique_movies['title'][i], score) for i, score in sim_scores]\n",
        "    return recommendations\n",
        "\n",
        "\n",
        "# Sample 20 titles\n",
        "print(\"Available sample titles:\")\n",
        "print(unique_movies['title'].sample(20, random_state=41).to_list())\n",
        "\n",
        "# Randomly select a movie title from the available titles\n",
        "random_title = random.choice(unique_movies['title'].to_list())\n",
        "\n",
        "print(f\"\\n Randomly selected movie for recommendation: '{random_title}'\")\n",
        "\n",
        "# Explanation\n",
        "print(\"\\n Content-based Recommendations are based on GENRE similarity using cosine similarity between genre vectors.\")\n",
        "\n",
        "# Get recommendations\n",
        "print(f\"\\nTop 20 Movies Most Similar in Genre to '{random_title}':\")\n",
        "for title, sim in get_recommendations(random_title):\n",
        "    print(f\"{title:<45} Similarity: {sim:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-H_FaMjJ5WPM",
        "outputId": "0fee41f0-ed2d-48a7-80eb-1b251708f890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available sample titles:\n",
            "['Atomica (2017)', 'Adventures in Babysitting (1987)', 'Big Picture, The (1989)', 'Annabelle (2014)', 'Beau Is Afraid (2023)', 'First Blood (Rambo: First Blood) (1982)', 'The Meg (2018)', 'Little Nemo: Adventures in Slumberland (1992)', 'Amen. (2002)', 'Danger: Diabolik (Diabolik) (1968)', 'Bugsy Malone (1976)', 'The Good Dinosaur (2015)', 'Goofy Movie, A (1995)', 'Man Called Horse, A (1970)', 'Terms and Conditions May Apply (2013)', 'StageFright: Aquarius (1987)', 'Shanghai Dreams (Qing hong) (2005)', 'I, Daniel Blake (2016)', \"Amores Perros (Love's a Bitch) (2000)\", \"Cookie's Fortune (1999)\"]\n",
            "\n",
            " Randomly selected movie for recommendation: 'Brave One, The (2007)'\n",
            "\n",
            " Content-based Recommendations are based on GENRE similarity using cosine similarity between genre vectors.\n",
            "\n",
            "Top 20 Movies Most Similar in Genre to 'Brave One, The (2007)':\n",
            "Amateur (1994)                                Similarity: 1.0000\n",
            "Kiss of Death (1995)                          Similarity: 1.0000\n",
            "Fresh (1994)                                  Similarity: 1.0000\n",
            "Guilty as Sin (1993)                          Similarity: 1.0000\n",
            "Killing Zoe (1994)                            Similarity: 1.0000\n",
            "Perfect World, A (1993)                       Similarity: 1.0000\n",
            "Purple Noon (Plein soleil) (1960)             Similarity: 1.0000\n",
            "Mulholland Falls (1996)                       Similarity: 1.0000\n",
            "Cape Fear (1962)                              Similarity: 1.0000\n",
            "Blood and Wine (Blood & Wine) (1996)          Similarity: 1.0000\n",
            "Desperate Measures (1998)                     Similarity: 1.0000\n",
            "Playing God (1997)                            Similarity: 1.0000\n",
            "Jackie Brown (1997)                           Similarity: 1.0000\n",
            "Twilight (1998)                               Similarity: 1.0000\n",
            "Rope (1948)                                   Similarity: 1.0000\n",
            "Shadow of a Doubt (1943)                      Similarity: 1.0000\n",
            "Lodger: A Story of the London Fog, The (1927) Similarity: 1.0000\n",
            "Few Good Men, A (1992)                        Similarity: 1.0000\n",
            "Simple Plan, A (1998)                         Similarity: 1.0000\n",
            "Limey, The (1999)                             Similarity: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Content-Based Rating Prediction Using Genre Similarity, User Behavior, and Fallback Handling\n",
        "\n",
        "This code demonstrates a *hybrid recommendation system* that combines **content-based filtering using genre similarity** with **collaborative filtering using user-specific ratings**. The objective is to predict how much a user will like a movie they've never seen, based on the genres of that movie and their past rating behavior.\n",
        "\n",
        "The prediction process incorporates a **fallback mechanism** and **debug printouts** to gracefully handle edge cases where standard hybrid predictions aren’t possible. These cases include users with no rating history, movies not present in the similarity matrix, or when no meaningful similarity is found.\n",
        "\n",
        "#### How it Works:\n",
        "\n",
        "1. **Genre Vector Normalization**:\n",
        "\n",
        "   * The genre columns are multi-hot encoded (e.g., Action, Comedy, etc.).\n",
        "   * Each movie’s genre vector is normalized using L2 norm so that cosine similarity is well-defined and scale-invariant.\n",
        "\n",
        "2. **Genre-Based Similarity Matrix**:\n",
        "\n",
        "   * Cosine similarity is computed between all pairs of movies based on genre vectors.\n",
        "\n",
        "3. **Mapping Setup**:\n",
        "\n",
        "   * The code builds lookup maps between `movieId` and its corresponding row index in the genre matrix to allow fast access.\n",
        "\n",
        "4. **Hybrid Prediction Function**:\n",
        "\n",
        "   * For a given `user_id` and `movie_id`, the function:\n",
        "\n",
        "     * Retrieves all movies rated by the user.\n",
        "     * Finds the top-K rated movies that are most genre-similar to the target movie.\n",
        "     * Computes a **weighted average of the ratings**, where the weights are the genre similarity scores.\n",
        "     * **If no such ratings or similarities are available**, the function **falls back to the global average rating of the movie**.\n",
        "     * Each fallback trigger is logged with a `[Debug]` message.\n",
        "\n",
        "5. **Application**:\n",
        "\n",
        "   * The model is tested on a sample user and generates predicted ratings for movies that are most similar in genre to a reference movie (e.g., *Heat (1995)*).\n",
        "\n",
        "This hybrid approach offers:\n",
        "\n",
        "* Personalization from collaborative filtering.\n",
        "* Interpretability from content-based features (genres).\n",
        "* Robustness from fallback logic to handle cold-starts or sparse data situations.\n"
      ],
      "metadata": {
        "id": "p6o2lnQkoljT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --- Step 1: Keep All Ratings (No User Filtering) ---\n",
        "ratings_filtered = ratings.copy()\n",
        "\n",
        "# Build user-movie matrix\n",
        "user_movie_matrix = ratings_filtered.pivot(index='userId', columns='movieId', values='rating')\n",
        "\n",
        "# --- Step 2: Prepare Genre Matrix ---\n",
        "# Convert genre strings to lists\n",
        "movies['genres'] = movies['genres'].apply(lambda x: x.split('|') if isinstance(x, str) else [])\n",
        "\n",
        "# Filter only movies present in ratings\n",
        "valid_movie_ids = user_movie_matrix.columns\n",
        "movies_filtered = movies[movies['movieId'].isin(valid_movie_ids)].copy()\n",
        "\n",
        "# One-hot encode genres\n",
        "mlb = MultiLabelBinarizer()\n",
        "genre_matrix = mlb.fit_transform(movies_filtered['genres'])\n",
        "genre_matrix_normalized = normalize(genre_matrix, norm='l2')\n",
        "\n",
        "# Create mappings\n",
        "unique_movies = movies_filtered.reset_index(drop=True)\n",
        "movieId_to_index = dict(zip(unique_movies['movieId'], unique_movies.index))\n",
        "index_to_movieId = dict(zip(unique_movies.index, unique_movies['movieId']))\n",
        "movie_idx = pd.Series(unique_movies.index, index=unique_movies['title']).drop_duplicates()\n",
        "\n",
        "# --- Step 3: Compute Cosine Similarity Between Genre Vectors ---\n",
        "genre_sim_matrix = cosine_similarity(genre_matrix_normalized)\n",
        "\n",
        "# --- Step 4: Hybrid Prediction Function with Fallbacks and Recommendation Message ---\n",
        "def predict_rating_genre_weighted(user_id, target_movie_id, k=10):\n",
        "    if user_id not in user_movie_matrix.index or target_movie_id not in movieId_to_index:\n",
        "        print(f\"[Debug] Invalid user_id {user_id} or movie_id {target_movie_id}. Returning NaN.\")\n",
        "        return np.nan\n",
        "\n",
        "    user_ratings = user_movie_matrix.loc[user_id].dropna()\n",
        "    if user_ratings.empty:\n",
        "        print(f\"[Fallback] User {user_id} has no ratings. Using global average for movieId {target_movie_id}.\")\n",
        "        pred = ratings_filtered[ratings_filtered['movieId'] == target_movie_id]['rating'].mean()\n",
        "        print(f\"[Recommendation] Predicted Rating: {pred:.2f} → {'Recommend' if pred >= 3.5 else 'Not Recommended'}\")\n",
        "        return pred\n",
        "\n",
        "    target_idx = movieId_to_index[target_movie_id]\n",
        "    rated_movie_indices = [movieId_to_index[mid] for mid in user_ratings.index if mid in movieId_to_index]\n",
        "\n",
        "    if not rated_movie_indices:\n",
        "        print(f\"[Fallback] Rated movies not found for user {user_id}. Using global average.\")\n",
        "        pred = ratings_filtered[ratings_filtered['movieId'] == target_movie_id]['rating'].mean()\n",
        "        print(f\"[Recommendation] Predicted Rating: {pred:.2f} → {'Recommend' if pred >= 3.5 else 'Not Recommended'}\")\n",
        "        return pred\n",
        "\n",
        "    sims = genre_sim_matrix[target_idx, rated_movie_indices]\n",
        "    sims_series = pd.Series(sims, index=[index_to_movieId[i] for i in rated_movie_indices])\n",
        "\n",
        "    top_similar = sims_series.sort_values(ascending=False).head(k)\n",
        "    top_ratings = user_ratings[top_similar.index]\n",
        "\n",
        "    # Debug logs for inspection\n",
        "    print(f\"\\n[Debug] Similarity Weights for User {user_id} on Target Movie {target_movie_id}:\")\n",
        "    print(top_similar)\n",
        "    print(\"[Debug] Corresponding Ratings:\")\n",
        "    print(top_ratings)\n",
        "\n",
        "    weighted_sum = np.dot(top_similar.values, top_ratings.values)\n",
        "    normalization = np.sum(top_similar.values)\n",
        "\n",
        "    if normalization > 0:\n",
        "        pred = weighted_sum / normalization\n",
        "        print(f\"[Prediction] Personalized prediction used for user {user_id} on movieId {target_movie_id}.\")\n",
        "        print(f\"[Recommendation] Predicted Rating: {pred:.2f} → {'Recommend' if pred >= 3.5 else 'Not Recommended'}\")\n",
        "        return pred\n",
        "    else:\n",
        "        print(f\"[Fallback] No similarity weights found. Using global average for movieId {target_movie_id}.\")\n",
        "        pred = ratings_filtered[ratings_filtered['movieId'] == target_movie_id]['rating'].mean()\n",
        "        print(f\"[Recommendation] Predicted Rating: {pred:.2f} → {'Recommend' if pred >= 3.5 else 'Not Recommended'}\")\n",
        "        return pred\n",
        "\n",
        "# --- Step 5: Run with Fixed User and Movie ---\n",
        "\n",
        "# Set static user and movie\n",
        "user_id = 174949\n",
        "target_movie = 'O.J.: Made in America (2016)'\n",
        "\n",
        "print(f\"Using user {user_id} for prediction.\")\n",
        "print(f\"Target movie exists: '{target_movie}' →\", target_movie in movie_idx)\n",
        "\n",
        "if target_movie in movie_idx:\n",
        "    idx = movie_idx[target_movie]\n",
        "    sim_scores = cosine_similarity(genre_matrix_normalized[idx].reshape(1, -1), genre_matrix_normalized)[0]\n",
        "    sim_indices = np.argsort(sim_scores)[::-1][1:11]  # Exclude the movie itself\n",
        "\n",
        "    top_similar_movie_ids = unique_movies.loc[sim_indices, 'movieId']\n",
        "    top_similar_titles = unique_movies.loc[sim_indices, 'title']\n",
        "\n",
        "    print(f\"\\nTop 10 Genre-Similar Movies to '{target_movie}':\")\n",
        "    print(top_similar_titles)\n",
        "\n",
        "    print(f\"\\nPredicted Ratings for User {user_id} Using Hybrid Genre-Based Model:\\n\")\n",
        "    for movie_id, title in zip(top_similar_movie_ids, top_similar_titles):\n",
        "        pred = predict_rating_genre_weighted(user_id=user_id, target_movie_id=movie_id, k=100)\n",
        "        print(f\"{title:<45} Predicted Rating: {pred:.2f}\")\n",
        "else:\n",
        "    print(\"Target movie not found in index.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX0bnB3JAwCt",
        "outputId": "d24f4e40-7167-4755-9617-7ffe4dbbe49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using user 174949 for prediction.\n",
            "Target movie exists: 'O.J.: Made in America (2016)' → True\n",
            "\n",
            "Top 10 Genre-Similar Movies to 'O.J.: Made in America (2016)':\n",
            "11172    Indiana Jones: The Search for the Lost Golden ...\n",
            "98                                          Catwalk (1996)\n",
            "104                           Anne Frank Remembered (1995)\n",
            "118                                 Man of the Year (1995)\n",
            "139                                           Crumb (1994)\n",
            "177                                        Unzipped (1995)\n",
            "213                                     Hoop Dreams (1994)\n",
            "315      Wonderful, Horrible Life of Leni Riefenstahl, ...\n",
            "480                                   War Room, The (1993)\n",
            "496                           Celluloid Closet, The (1995)\n",
            "Name: title, dtype: object\n",
            "\n",
            "Predicted Ratings for User 174949 Using Hybrid Genre-Based Model:\n",
            "\n",
            "\n",
            "[Debug] Similarity Weights for User 174949 on Target Movie 287443:\n",
            "1207    0.0\n",
            "2671    0.0\n",
            "dtype: float64\n",
            "[Debug] Corresponding Ratings:\n",
            "1207    5.0\n",
            "2671    3.0\n",
            "Name: 174949, dtype: float64\n",
            "[Fallback] No similarity weights found. Using global average for movieId 287443.\n",
            "[Recommendation] Predicted Rating: 3.50 → Recommend\n",
            "Indiana Jones: The Search for the Lost Golden Age (2021) Predicted Rating: 3.50\n",
            "\n",
            "[Debug] Similarity Weights for User 174949 on Target Movie 108:\n",
            "1207    0.0\n",
            "2671    0.0\n",
            "dtype: float64\n",
            "[Debug] Corresponding Ratings:\n",
            "1207    5.0\n",
            "2671    3.0\n",
            "Name: 174949, dtype: float64\n",
            "[Fallback] No similarity weights found. Using global average for movieId 108.\n",
            "[Recommendation] Predicted Rating: 4.00 → Recommend\n",
            "Catwalk (1996)                                Predicted Rating: 4.00\n",
            "\n",
            "[Debug] Similarity Weights for User 174949 on Target Movie 116:\n",
            "1207    0.0\n",
            "2671    0.0\n",
            "dtype: float64\n",
            "[Debug] Corresponding Ratings:\n",
            "1207    5.0\n",
            "2671    3.0\n",
            "Name: 174949, dtype: float64\n",
            "[Fallback] No similarity weights found. Using global average for movieId 116.\n",
            "[Recommendation] Predicted Rating: 4.67 → Recommend\n",
            "Anne Frank Remembered (1995)                  Predicted Rating: 4.67\n",
            "\n",
            "[Debug] Similarity Weights for User 174949 on Target Movie 137:\n",
            "1207    0.0\n",
            "2671    0.0\n",
            "dtype: float64\n",
            "[Debug] Corresponding Ratings:\n",
            "1207    5.0\n",
            "2671    3.0\n",
            "Name: 174949, dtype: float64\n",
            "[Fallback] No similarity weights found. Using global average for movieId 137.\n",
            "[Recommendation] Predicted Rating: 5.00 → Recommend\n",
            "Man of the Year (1995)                        Predicted Rating: 5.00\n",
            "\n",
            "[Debug] Similarity Weights for User 174949 on Target Movie 162:\n",
            "1207    0.0\n",
            "2671    0.0\n",
            "dtype: float64\n",
            "[Debug] Corresponding Ratings:\n",
            "1207    5.0\n",
            "2671    3.0\n",
            "Name: 174949, dtype: float64\n",
            "[Fallback] No similarity weights found. Using global average for movieId 162.\n",
            "[Recommendation] Predicted Rating: 4.03 → Recommend\n",
            "Crumb (1994)                                  Predicted Rating: 4.03\n",
            "\n",
            "[Debug] Similarity Weights for User 174949 on Target Movie 206:\n",
            "1207    0.0\n",
            "2671    0.0\n",
            "dtype: float64\n",
            "[Debug] Corresponding Ratings:\n",
            "1207    5.0\n",
            "2671    3.0\n",
            "Name: 174949, dtype: float64\n",
            "[Fallback] No similarity weights found. Using global average for movieId 206.\n",
            "[Recommendation] Predicted Rating: 3.50 → Recommend\n",
            "Unzipped (1995)                               Predicted Rating: 3.50\n",
            "\n",
            "[Debug] Similarity Weights for User 174949 on Target Movie 246:\n",
            "1207    0.0\n",
            "2671    0.0\n",
            "dtype: float64\n",
            "[Debug] Corresponding Ratings:\n",
            "1207    5.0\n",
            "2671    3.0\n",
            "Name: 174949, dtype: float64\n",
            "[Fallback] No similarity weights found. Using global average for movieId 246.\n",
            "[Recommendation] Predicted Rating: 4.12 → Recommend\n",
            "Hoop Dreams (1994)                            Predicted Rating: 4.12\n",
            "\n",
            "[Debug] Similarity Weights for User 174949 on Target Movie 363:\n",
            "1207    0.0\n",
            "2671    0.0\n",
            "dtype: float64\n",
            "[Debug] Corresponding Ratings:\n",
            "1207    5.0\n",
            "2671    3.0\n",
            "Name: 174949, dtype: float64\n",
            "[Fallback] No similarity weights found. Using global average for movieId 363.\n",
            "[Recommendation] Predicted Rating: 3.50 → Recommend\n",
            "Wonderful, Horrible Life of Leni Riefenstahl, The (Macht der Bilder: Leni Riefenstahl, Die) (1993) Predicted Rating: 3.50\n",
            "\n",
            "[Debug] Similarity Weights for User 174949 on Target Movie 556:\n",
            "1207    0.0\n",
            "2671    0.0\n",
            "dtype: float64\n",
            "[Debug] Corresponding Ratings:\n",
            "1207    5.0\n",
            "2671    3.0\n",
            "Name: 174949, dtype: float64\n",
            "[Fallback] No similarity weights found. Using global average for movieId 556.\n",
            "[Recommendation] Predicted Rating: 3.25 → Not Recommended\n",
            "War Room, The (1993)                          Predicted Rating: 3.25\n",
            "\n",
            "[Debug] Similarity Weights for User 174949 on Target Movie 581:\n",
            "1207    0.0\n",
            "2671    0.0\n",
            "dtype: float64\n",
            "[Debug] Corresponding Ratings:\n",
            "1207    5.0\n",
            "2671    3.0\n",
            "Name: 174949, dtype: float64\n",
            "[Fallback] No similarity weights found. Using global average for movieId 581.\n",
            "[Recommendation] Predicted Rating: 4.17 → Recommend\n",
            "Celluloid Closet, The (1995)                  Predicted Rating: 4.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hybrid Recommender: Genre-Weighted Collaborative Filtering**\n",
        "\n",
        "This code predicts a user's rating for a movie by combining collaborative filtering and genre-based similarity. Here's how it works:\n",
        "\n",
        "### **Step-by-Step Explanation**\n",
        "\n",
        "**1. Data Preparation**\n",
        "\n",
        "* It loads the `ratings` and `movies` datasets.\n",
        "* The user-movie ratings matrix is built using `.pivot()` (rows = users, columns = movies, values = ratings).\n",
        "* Movie genres are split and one-hot encoded using `MultiLabelBinarizer`.\n",
        "* Genre vectors are normalized to enable cosine similarity comparison.\n",
        "\n",
        "**2. Genre Similarity Calculation**\n",
        "\n",
        "* Cosine similarity is computed between normalized genre vectors of all movies.\n",
        "* This generates a matrix showing how similar each pair of movies is based on genre.\n",
        "\n",
        "**3. `hybrid_predict()` Function:**\n",
        "This is the main prediction function. Here's what it does:\n",
        "\n",
        "* **Step 1**: Skips invalid user/movie inputs.\n",
        "* **Step 2**: Loops through all other users (excluding the target user).\n",
        "* **Step 3**: For each user, checks if they rated the target movie.\n",
        "* **Step 4**: Collects that user's other rated movies and looks up genre similarity between those and the target movie.\n",
        "* **Step 5**: Uses a weighted average of the other user's ratings on similar movies, weighted by genre similarity.\n",
        "* **Step 6**: Averages all such weighted predictions from other users to generate the final prediction.\n",
        "* **Fallback**: If no useful ratings are found, it falls back to the global average rating for the movie.\n",
        "\n",
        "**4. Prediction Execution**\n",
        "\n",
        "* The code sets `user_id = 174949` and `target_movie = 'O.J.: Made in America (2016)'`.\n",
        "* It retrieves the `movieId` and runs the `hybrid_predict()` function.\n",
        "* Finally, it prints the predicted rating for that user and movie.\n",
        "\n",
        "*This approach combines user behavior (collaborative filtering) with genre-based content similarity to improve prediction accuracy, especially for sparse data or cold-start problems.*\n"
      ],
      "metadata": {
        "id": "XQMqGRzZkhcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Vectorized NumPy Logic – Genre-Based Hybrid Prediction"
      ],
      "metadata": {
        "id": "mJupUOM_Ia4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Load subset datasets\n",
        "!wget -q https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/movies_subset.csv\n",
        "!wget -q https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_subset.csv\n",
        "\n",
        "ratings = pd.read_csv(\"ratings_subset.csv\")\n",
        "movies = pd.read_csv(\"movies_subset.csv\")\n",
        "\n",
        "# Step 1: Prepare Data\n",
        "ratings_filtered = ratings.copy()\n",
        "user_movie_matrix = ratings_filtered.pivot(index='userId', columns='movieId', values='rating')\n",
        "\n",
        "movies['genres'] = movies['genres'].apply(lambda x: x.split('|') if isinstance(x, str) else [])\n",
        "valid_movie_ids = user_movie_matrix.columns\n",
        "movies_filtered = movies[movies['movieId'].isin(valid_movie_ids)].copy()\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "genre_matrix = mlb.fit_transform(movies_filtered['genres'])\n",
        "genre_matrix_normalized = normalize(genre_matrix, norm='l2')\n",
        "\n",
        "unique_movies = movies_filtered.reset_index(drop=True)\n",
        "movieId_to_index = dict(zip(unique_movies['movieId'], unique_movies.index))\n",
        "index_to_movieId = dict(zip(unique_movies.index, unique_movies['movieId']))\n",
        "movie_idx = pd.Series(unique_movies.index, index=unique_movies['title']).drop_duplicates()\n",
        "\n",
        "# Step 2: Vectorized Hybrid Prediction Function (Genre-only Weighted)\n",
        "def vectorized_hybrid_predict(user_id, target_movie_id, k=10):\n",
        "    if user_id not in user_movie_matrix.index or target_movie_id not in movieId_to_index:\n",
        "        return np.nan\n",
        "\n",
        "    target_idx = movieId_to_index[target_movie_id]\n",
        "    sim_vector = cosine_similarity(genre_matrix_normalized[target_idx].reshape(1, -1), genre_matrix_normalized)[0]\n",
        "\n",
        "    user_ratings = user_movie_matrix.loc[user_id].dropna()\n",
        "    rated_movie_ids = user_ratings.index.intersection(user_movie_matrix.columns)\n",
        "    rated_indices = [movieId_to_index[mid] for mid in rated_movie_ids if mid in movieId_to_index]\n",
        "\n",
        "    sim_scores = sim_vector[rated_indices]\n",
        "    ratings_values = user_ratings.loc[rated_movie_ids].values\n",
        "\n",
        "    if len(sim_scores) == 0 or np.sum(sim_scores) == 0:\n",
        "        return np.nan\n",
        "\n",
        "    top_k_indices = np.argsort(sim_scores)[-k:]\n",
        "    sim_top = sim_scores[top_k_indices]\n",
        "    rating_top = ratings_values[top_k_indices]\n",
        "\n",
        "    return np.dot(sim_top, rating_top) / np.sum(sim_top)\n",
        "\n",
        "# Example Usage\n",
        "# user_id = 174949\n",
        "valid_user = None\n",
        "for uid in user_movie_matrix.index:\n",
        "    rated_movies = user_movie_matrix.loc[uid].dropna().index\n",
        "    if rated_movies.intersection(movieId_to_index.keys()).any():\n",
        "        valid_user = uid\n",
        "        break  # Exit the loop immediately once a valid user is found\n",
        "\n",
        "user_id = valid_user\n",
        "\n",
        "\n",
        "target_movie = 'O.J.: Made in America (2016)'\n",
        "target_movie_id = unique_movies.loc[movie_idx[target_movie], 'movieId']\n",
        "pred = vectorized_hybrid_predict(user_id, target_movie_id, k=100)\n",
        "print(f\"Predicted rating for '{target_movie}' by user {user_id}: {pred:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpqEFAeGElOp",
        "outputId": "3cf1ab91-c533-4e94-e808-9d0ee4d9366a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted rating for 'O.J.: Made in America (2016)' by user 10: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Blended Hybrid (Genre + Collaborative Filtering) with Precomputed Hybrid Similarity"
      ],
      "metadata": {
        "id": "isnJMUbUIea3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Load subset datasets\n",
        "!wget -q https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/movies_subset.csv\n",
        "!wget -q https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_subset.csv\n",
        "\n",
        "# Step 1: Prepare Data\n",
        "ratings_filtered = ratings.copy()\n",
        "user_movie_matrix = ratings_filtered.pivot(index='userId', columns='movieId', values='rating')\n",
        "\n",
        "movies['genres'] = movies['genres'].apply(lambda x: x.split('|') if isinstance(x, str) else [])\n",
        "valid_movie_ids = user_movie_matrix.columns\n",
        "movies_filtered = movies[movies['movieId'].isin(valid_movie_ids)].copy()\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "genre_matrix = mlb.fit_transform(movies_filtered['genres'])\n",
        "genre_matrix_normalized = normalize(genre_matrix, norm='l2')\n",
        "\n",
        "unique_movies = movies_filtered.reset_index(drop=True)\n",
        "movieId_to_index = dict(zip(unique_movies['movieId'], unique_movies.index))\n",
        "index_to_movieId = dict(zip(unique_movies.index, unique_movies['movieId']))\n",
        "movie_idx = pd.Series(unique_movies.index, index=unique_movies['title']).drop_duplicates()\n",
        "\n",
        "# Step 2: Compute Hybrid Similarity Matrix\n",
        "genre_sim = cosine_similarity(genre_matrix_normalized)\n",
        "user_movie_centered = user_movie_matrix.sub(user_movie_matrix.mean(axis=1), axis=0).fillna(0)\n",
        "item_sim = cosine_similarity(user_movie_centered.T.fillna(0))\n",
        "\n",
        "# Ensure both matrices are same shape\n",
        "alpha = 0.5  # genre-collab blend weight\n",
        "hybrid_sim = alpha * genre_sim + (1 - alpha) * item_sim\n",
        "\n",
        "# Step 3: Prediction Function Using Hybrid Similarity\n",
        "def blended_hybrid_predict(user_id, target_movie_id, k=10):\n",
        "    if user_id not in user_movie_matrix.index or target_movie_id not in movieId_to_index:\n",
        "        return np.nan\n",
        "\n",
        "    target_idx = movieId_to_index[target_movie_id]\n",
        "    sim_vector = hybrid_sim[target_idx]\n",
        "\n",
        "    user_ratings = user_movie_matrix.loc[user_id].dropna()\n",
        "    rated_movie_ids = user_ratings.index.intersection(user_movie_matrix.columns)\n",
        "    rated_indices = [movieId_to_index[mid] for mid in rated_movie_ids if mid in movieId_to_index]\n",
        "\n",
        "    sim_scores = sim_vector[rated_indices]\n",
        "    ratings_values = user_ratings.loc[rated_movie_ids].values\n",
        "\n",
        "    if len(sim_scores) == 0 or np.sum(sim_scores) == 0:\n",
        "        return np.nan\n",
        "\n",
        "    top_k_indices = np.argsort(sim_scores)[-k:]\n",
        "    sim_top = sim_scores[top_k_indices]\n",
        "    rating_top = ratings_values[top_k_indices]\n",
        "\n",
        "    return np.dot(sim_top, rating_top) / np.sum(sim_top)\n",
        "\n",
        "# Example Usage\n",
        "user_id = 174949\n",
        "target_movie = 'O.J.: Made in America (2016)'\n",
        "target_movie_id = unique_movies.loc[movie_idx[target_movie], 'movieId']\n",
        "pred = blended_hybrid_predict(user_id, target_movie_id, k=100)\n",
        "print(f\"Predicted rating for '{target_movie}' by user {user_id}: {pred:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ak3yfaL-IkP_",
        "outputId": "4033a60d-ed0b-47e4-84db-3508d9b24491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 feature(s) (shape=(11190, 0)) while a minimum of 1 is required by the normalize function.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1584051649>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mgenre_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genres'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mgenre_matrix_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0munique_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovies_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1963\u001b[0m     \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1966\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0;34m\"Found array with %d feature(s) (shape=%s) while\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(11190, 0)) while a minimum of 1 is required by the normalize function."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of Overlapping and Divergent Recommendations\n",
        "\n",
        "Both methods returned several overlapping recommendations, but also differed in meaningful ways:\n",
        "\n",
        "#### Similar Recommendations from Both Methods\n",
        "\n",
        "* **Assassins (1995)**\n",
        "* **Net, The (1995)**\n",
        "\n",
        "These consistent suggestions indicate that both the pure content-based and hybrid genre-weighted models identify core genre traits effectively.\n",
        "\n",
        "#### Recommendations Unique to Each Method\n",
        "\n",
        "**Only in Content-Based (Cosine Genre Similarity):**\n",
        "\n",
        "* *Die Hard (1988)*\n",
        "* *Batman (1989)*\n",
        "* *U.S. Marshals (1998)*\n",
        "\n",
        "**Only in Hybrid Model (Genre + Ratings Fallback):**\n",
        "\n",
        "* *Sin City: A Dame to Kill For (2014)*\n",
        "* *John Wick: Chapter Two (2017)*\n",
        "* *Transporter 2 (2005)*\n",
        "\n",
        "These differences show that the hybrid method is able to introduce newer or slightly more nuanced genre matches, even when rating data for a specific user is missing and fallback mechanisms are triggered.\n"
      ],
      "metadata": {
        "id": "Z6_Z7TyTwAhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimized Jaccard Similarity for Content-Based Filtering\n",
        "\n",
        "This block introduces a more efficient method for computing **Jaccard similarity** between movies based on their genre information. Unlike the traditional nested-loop approach, this implementation uses the `pdist()` function from `scipy.spatial.distance` to compute all pairwise Jaccard distances in a **fully vectorized** manner. The result is a symmetric similarity matrix, which is then used to identify the most similar movies to a given title. This optimization drastically reduces computation time and is highly recommended for medium-to-large datasets.\n",
        "\n",
        "using `scipy.spatial.distance.pdist()` **does calculate all pairwise similarities**, but it does so much more efficiently than a manual loop.\n",
        "\n",
        "Here’s how it works:\n",
        "\n",
        "* `pdist(binary_matrix, metric='jaccard')` computes the **Jaccard distance** (which is `1 - Jaccard similarity`) between **all unique pairs** of rows (i.e., movies) in the binary genre matrix.\n",
        "* The output is a **condensed distance matrix** — a flat array containing the upper triangle of the full pairwise distance matrix.\n",
        "* This condensed matrix is converted back into a full square **symmetric matrix** using `squareform()`, giving us the distance between all pairs.\n",
        "* We then compute similarity as `1 - distance`.\n",
        "\n",
        "Every possible movie-to-movie similarity is calculated — but with optimized vectorized operations under the hood, which is much faster than nested Python loops.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZK-ho_H6hrOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import pdist, squareform\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Prepare genre binary matrix\n",
        "unique_movies = movies.copy().reset_index(drop=True)\n",
        "genre_cols = all_genres\n",
        "genre_matrix = unique_movies[genre_cols].astype(bool).astype(int).values  # ensure binary format\n",
        "\n",
        "# Step 2: Compute Jaccard distance (1 - similarity)\n",
        "# pdist returns a condensed distance matrix; squareform converts it to square form\n",
        "jaccard_distance = pdist(genre_matrix, metric='jaccard')  # returns 1 - Jaccard similarity\n",
        "jaccard_sim_matrix = 1 - squareform(jaccard_distance)      # convert to full similarity matrix\n",
        "\n",
        "# Step 3: Create mapping from title to matrix index\n",
        "movie_idx = pd.Series(unique_movies.index, index=unique_movies['title']).drop_duplicates()\n",
        "\n",
        "# Step 4: Define recommendation function\n",
        "def get_recommendations_jaccard(title, topN=10):\n",
        "    if title not in movie_idx:\n",
        "        return f\"Movie '{title}' not found in dataset.\"\n",
        "\n",
        "    idx = movie_idx[title]\n",
        "    sim_scores = list(enumerate(jaccard_sim_matrix[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:topN+1]  # exclude self\n",
        "    top_indices = [i[0] for i in sim_scores]\n",
        "    return unique_movies['title'].iloc[top_indices]\n",
        "\n",
        "# Step 5: Try a sample movie\n",
        "print(\"Sample titles:\", unique_movies['title'].sample(5, random_state=42).to_list())\n",
        "print(f\"\\nJaccard Recommendations for {random_title}:\")\n",
        "# print(get_recommendations_jaccard(\"Heat (1995)\"))\n",
        "print(get_recommendations_jaccard(random_title))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l-M-JJekRl-",
        "outputId": "78ad6b33-26d9-4626-81e6-99833f9abce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample titles: ['Murder on the Orient Express (2017)', 'Rhapsody in August (Hachi-gatsu no kyôshikyoku) (1991)', 'First Position (2011)', 'Wait Until Dark (1967)', 'Coffy (1973)']\n",
            "\n",
            "Jaccard Recommendations for Youth (2015):\n",
            "25                      Othello (1995)\n",
            "30              Dangerous Minds (1995)\n",
            "38     Cry, the Beloved Country (1995)\n",
            "41                  Restoration (1995)\n",
            "51                      Georgia (1995)\n",
            "52        Home for the Holidays (1995)\n",
            "57           Mr. Holland's Opus (1995)\n",
            "62                     Two Bits (1995)\n",
            "103           Margaret's Museum (1995)\n",
            "108    Boys of St. Vincent, The (1992)\n",
            "Name: title, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison of Content-Based Recommendations: Cosine vs. Jaccard Similarity\n",
        "\n",
        "Both the **cosine similarity** and **Jaccard similarity** methods returned *identical top-10 movie recommendations* for the query movie **\"Heat (1995)\"**. This indicates that in the context of the MovieLens genre-based content filtering:\n",
        "\n",
        "* **Both methods effectively captured the same neighborhood of similar films**.\n",
        "* The movies recommended (e.g., *Assassins*, *Die Hard*, *The Net*, *Natural Born Killers*) suggest that the genre combinations for these titles closely match those of *Heat (1995)*.\n",
        "* While **cosine similarity** operates on normalized multi-hot vectors and measures angular proximity,\n",
        "  **Jaccard similarity** measures the overlap in genre tags directly.\n",
        "\n",
        "### Key Takeaway:\n",
        "\n",
        "Despite their different mathematical underpinnings, both methods **produced the same results** because:\n",
        "\n",
        "* The genre vectors are binary (multi-hot encoded), where normalization (in cosine) doesn’t distort information significantly.\n",
        "* The dominant factor influencing similarity is the **overlap of genre labels**, which both metrics capture well.\n",
        "\n",
        "However:\n",
        "\n",
        "* **Cosine similarity is computationally faster** and more scalable.\n",
        "* **Jaccard similarity is slower** when computed pairwise using loops, though vectorized solutions like `pdist()` improve it significantly.\n",
        "\n",
        "You can safely use either in this binary genre context, but for large-scale systems, cosine is typically preferred for efficiency.\n"
      ],
      "metadata": {
        "id": "RLrKOSN4mnK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User-User Collaborative Filtering with Bias Adjustment and Fallback Logic\n",
        "\n",
        "This recommender system applies a user-user collaborative filtering approach enhanced with user and item bias adjustments and robust fallback logic to ensure stable and interpretable predictions. The method is designed to make personalized movie rating predictions even in cases of sparse data.\n",
        "\n",
        "**1. Data Sampling and Matrix Construction**\n",
        "A sample of 10,000 users is drawn randomly from the full ratings dataset to manage memory and computational requirements. A user-movie matrix is constructed using these ratings, where each cell represents the rating a user has given to a movie. The system computes average ratings per user (user bias) and per movie (item bias) to help model baseline tendencies.\n",
        "\n",
        "**2. Centering and Similarity Calculation**\n",
        "To isolate users' preferences from their general rating behavior, the user-movie matrix is centered by subtracting each user's average rating. This centered matrix is then used to calculate cosine similarity between users, generating a user-user similarity matrix that quantifies how closely users' preferences align.\n",
        "\n",
        "**3. Predicting Ratings Using Top-k Neighbors**\n",
        "To predict a rating for a given user and movie, the system:\n",
        "\n",
        "* Identifies the top-k most similar users who have rated the target movie.\n",
        "* Computes the deviation of these neighbors’ ratings from their respective means.\n",
        "* Uses a weighted average of these deviations, weighted by similarity, and adds it to the target user’s mean to produce the prediction.\n",
        "\n",
        "This formula is:\n",
        "\n",
        "$$\n",
        "\\hat{r}_{u,i} = \\mu_u + \\frac{\\sum_{v \\in N(u)} \\text{sim}(u,v) \\cdot (r_{v,i} - \\mu_v)}{\\sum_{v \\in N(u)} \\text{sim}(u,v)}\n",
        "$$\n",
        "\n",
        "**4. Bias-Based Fallback Logic**\n",
        "If the user has no similar neighbors who have rated the movie, or if the similarity weights sum to zero, the system falls back to a bias-based estimate:\n",
        "\n",
        "$$\n",
        "\\hat{r}_{u,i} = \\mu_u + \\mu_i - \\mu_{global}\n",
        "$$\n",
        "\n",
        "This combines the user’s and the item’s average rating, adjusted by subtracting the global mean to avoid double-counting. If either the user or item bias is unavailable, the system defaults to the global average rating.\n",
        "\n",
        "**5. Clamping Predictions to Rating Scale**\n",
        "All final predictions are clamped to the valid rating range \\[0.5, 5.0] to ensure they remain realistic and consistent with actual rating values.\n",
        "\n",
        "**6. Fallback Testing for Cold-Start Scenarios**\n",
        "The system includes a test routine to simulate cold-start scenarios by selecting users who have not rated the target movie. This verifies that the fallback mechanism generates meaningful predictions even when minimal user-item interaction data is available.\n",
        "\n",
        "This hybrid approach ensures personalized predictions while remaining resilient in sparse data conditions, making it suitable for practical recommender systems.\n"
      ],
      "metadata": {
        "id": "lwSEuluhhOuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling and Computing Cosine Similarity for User-Based Collaborative Filtering\n",
        "\n",
        "This code prepares a smaller, manageable dataset from a larger ratings file and computes a user-user cosine similarity matrix to be used in a recommender system. Each step has a clear purpose:\n",
        "\n",
        "**1. Load Full Ratings Data**\n",
        "*Purpose: To retrieve the entire dataset of user-movie ratings for processing.*\n",
        "The code loads the full ratings dataset from a remote source and reports how many unique users and movies are present.\n",
        "\n",
        "**2. Sample 10,000 Unique Users**\n",
        "*Purpose: To reduce computational load by working with a representative subset of the data.*\n",
        "The code randomly selects 10,000 unique users and filters the ratings dataset to include only those users. This sampled dataset is then saved for future use.\n",
        "\n",
        "**3. Create User-Movie Matrix**\n",
        "*Purpose: To structure the data into a matrix format suitable for similarity calculations.*\n",
        "A pivot table is created where rows are users, columns are movies, and values are the corresponding ratings. This format allows for pairwise comparisons between users.\n",
        "\n",
        "**4. Center Ratings**\n",
        "*Purpose: To normalize user behavior by removing individual rating biases.*\n",
        "The code subtracts each user's average rating from their rated movies. This centers the data around zero and ensures that similarity is based on rating patterns rather than absolute values.\n",
        "\n",
        "**5. Compute Cosine Similarity**\n",
        "*Purpose: To quantify how similar users are based on their centered rating patterns.*\n",
        "Using the centered matrix, the cosine similarity is calculated between every pair of users. This measures how aligned users are in terms of their movie preferences.\n",
        "\n",
        "**6. Save Similarity Matrix**\n",
        "*Purpose: To preserve the computed similarity matrix for use in building and testing recommendation algorithms.*\n",
        "The resulting cosine similarity matrix is converted into a labeled DataFrame and saved as a CSV file for later use in prediction models.\n",
        "\n",
        "This process builds a scalable foundation for collaborative filtering by focusing on user similarity based on normalized preferences.\n"
      ],
      "metadata": {
        "id": "BHO8ItD5Gwfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Load full ratings data\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_subset.csv\")\n",
        "print(f\"Loaded dataset with {ratings['userId'].nunique()} users and {ratings['movieId'].nunique()} movies.\")\n",
        "\n",
        "# Step 2: Sample 10,000 unique users\n",
        "sampled_user_ids = ratings['userId'].drop_duplicates().sample(n=10000, random_state=42)\n",
        "ratings_sampled = ratings[ratings['userId'].isin(sampled_user_ids)]\n",
        "ratings_sampled.to_csv(\"ratings_sampled.csv\", index=False)\n",
        "print(f\"Sampled dataset saved with {ratings_sampled['userId'].nunique()} users and {ratings_sampled['movieId'].nunique()} movies.\")\n",
        "\n",
        "# Step 3: Create user-movie matrix from sampled data\n",
        "user_movie_matrix = ratings_sampled.pivot(index='userId', columns='movieId', values='rating')\n",
        "user_means = user_movie_matrix.mean(axis=1)\n",
        "user_ids = user_movie_matrix.index.tolist()\n",
        "\n",
        "# Step 4: Center ratings\n",
        "user_movie_centered = user_movie_matrix.sub(user_means, axis=0).fillna(0)\n",
        "\n",
        "# Step 5: Compute cosine similarity\n",
        "print(\"Computing cosine similarity matrix for sampled users...\")\n",
        "cosine_sim_matrix = cosine_similarity(user_movie_centered.values)\n",
        "\n",
        "# Step 6: Convert to DataFrame and save\n",
        "cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=user_ids, columns=user_ids)\n",
        "cosine_sim_df.to_csv(\"cosine_user_similarity_sampled.csv\")\n",
        "print(\"Cosine similarity matrix saved as 'cosine_user_similarity_sampled.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzj1U0-yLa8v",
        "outputId": "34865b84-572c-4f8b-aa2e-a283581dd113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 59029 users and 11190 movies.\n",
            "Sampled dataset saved with 10000 users and 4933 movies.\n",
            "Computing cosine similarity matrix for sampled users...\n",
            "Cosine similarity matrix saved as 'cosine_user_similarity_sampled.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "geZnz5ZcI7SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import jaccard_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Load full ratings data\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_subset.csv\")\n",
        "print(f\"Loaded dataset with {ratings['userId'].nunique()} users and {ratings['movieId'].nunique()} movies.\")\n",
        "\n",
        "# Step 2: Sample 10,000 unique users\n",
        "sampled_user_ids = ratings['userId'].drop_duplicates().sample(n=10000, random_state=42)\n",
        "ratings_sampled = ratings[ratings['userId'].isin(sampled_user_ids)]\n",
        "ratings_sampled.to_csv(\"ratings_sampled.csv\", index=False)\n",
        "print(f\"Sampled dataset saved with {ratings_sampled['userId'].nunique()} users and {ratings_sampled['movieId'].nunique()} movies.\")\n",
        "\n",
        "# Step 3: Create binary user-movie matrix (1 if rated, 0 if not)\n",
        "user_movie_matrix = ratings_sampled.pivot(index='userId', columns='movieId', values='rating')\n",
        "user_movie_binary = user_movie_matrix.notna().astype(int)\n",
        "user_ids = user_movie_binary.index.tolist()\n",
        "\n",
        "# Step 4: Compute Jaccard similarity\n",
        "print(\"Computing Jaccard similarity matrix for sampled users...\")\n",
        "\n",
        "jaccard_sim_matrix = np.zeros((len(user_ids), len(user_ids)))\n",
        "\n",
        "for i, user_i in enumerate(tqdm(user_ids)):\n",
        "    for j in range(i, len(user_ids)):\n",
        "        user_j = user_ids[j]\n",
        "        sim = jaccard_score(user_movie_binary.loc[user_i], user_movie_binary.loc[user_j])\n",
        "        jaccard_sim_matrix[i, j] = sim\n",
        "        jaccard_sim_matrix[j, i] = sim  # symmetric\n",
        "\n",
        "# Step 5: Convert to DataFrame and save\n",
        "jaccard_sim_df = pd.DataFrame(jaccard_sim_matrix, index=user_ids, columns=user_ids)\n",
        "jaccard_sim_df.to_csv(\"jaccard_user_similarity_sampled.csv\")\n",
        "print(\"Jaccard similarity matrix saved as 'jaccard_user_similarity_sampled.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "foep345RI77P",
        "outputId": "cddd825b-95c0-41af-f666-986e18c1fecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 59029 users and 11190 movies.\n",
            "Sampled dataset saved with 10000 users and 4933 movies.\n",
            "Computing Jaccard similarity matrix for sampled users...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 8/10000 [04:33<94:50:02, 34.17s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-443740656>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0muser_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_movie_binary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_movie_binary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mjaccard_sim_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mjaccard_sim_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m  \u001b[0;31m# symmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mjaccard_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.33\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m     \"\"\"\n\u001b[0;32m--> 923\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m     \u001b[0msamplewise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     MCM = multilabel_confusion_matrix(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Check that we don't mix label format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mys_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mys_types\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mys_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Check that we don't mix label format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mys_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mys_types\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mys_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name, raise_unknown)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Complex data not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m def check_array(\n\u001b[0m\u001b[1;32m    737\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User-Based Collaborative Filtering with Cosine Similarity and Bias Adjustment\n",
        "\n",
        "This system predicts how a user might rate a movie they haven’t seen, using the behavior of similar users. It employs a user-based collaborative filtering approach, enhanced with cosine similarity and bias adjustment, and includes fallback logic to handle missing data. Below is a breakdown of the methodology with the purpose of each step.\n",
        "\n",
        "**1. Data Preparation**\n",
        "*Purpose: To structure the raw data into a usable format for similarity computation and rating prediction.*\n",
        "\n",
        "* Loads the movie metadata and user ratings datasets.\n",
        "* Constructs a user-movie matrix, where rows represent users and columns represent movies.\n",
        "* Calculates:\n",
        "\n",
        "  * Each user’s average rating (to normalize personal biases)\n",
        "  * Each movie’s average rating (used in fallback logic)\n",
        "  * The global average rating (used as a last-resort fallback)\n",
        "\n",
        "**2. Similarity Matrix Handling**\n",
        "*Purpose: To determine how similar each user is to every other user, based on shared rating behavior.*\n",
        "This step ensures that a valid user-user cosine similarity matrix is available by following one of three approaches:\n",
        "\n",
        "* **Check for a Local File:**\n",
        "  If the matrix already exists on the local machine, it is loaded directly for efficiency.\n",
        "\n",
        "* **Download from Cloud Storage:**\n",
        "  If the local file is missing, the system attempts to download a precomputed matrix from Google Drive.\n",
        "\n",
        "* **Compute Similarity Manually:**\n",
        "  If downloading fails:\n",
        "\n",
        "  * User ratings are centered by subtracting their individual means\n",
        "  * Missing ratings are filled with zeros to allow matrix operations\n",
        "  * Cosine similarity is computed between users\n",
        "  * The resulting matrix is saved locally for future reuse\n",
        "\n",
        "This three-step fallback ensures the system is flexible and always functional, regardless of file availability.\n",
        "\n",
        "**3. Rating Prediction with Bias Adjustment**\n",
        "*Purpose: To predict how a specific user would rate a specific movie using insights from similar users.*\n",
        "\n",
        "* Identifies users who have rated the target movie.\n",
        "* Measures similarity between the target user and those users using cosine similarity.\n",
        "* Selects the top *k* most similar users (neighbors).\n",
        "* Calculates how much each neighbor’s rating deviates from their average and weighs it by their similarity score.\n",
        "* Adjusts the target user’s mean rating by the weighted deviation to produce a prediction.\n",
        "* Applies fallback rules using movie mean or global mean if not enough neighbors are found or similarity is too low.\n",
        "* Prediction is capped between 0.5 and 5.0 to stay within valid rating bounds.\n",
        "\n",
        "**4. Random Test Pair Selection**\n",
        "*Purpose: To automatically select a valid (user, movie) pair for prediction testing.*\n",
        "\n",
        "* Randomly picks a user who has not rated a given movie.\n",
        "* Ensures that at least *k* other users have rated the movie to allow meaningful prediction.\n",
        "* Returns a user-movie pair for evaluation of the recommender system.\n",
        "\n",
        "**5. Prediction Test and Fallback Demonstration**\n",
        "*Purpose: To test and demonstrate the prediction capability and the fallback mechanism.*\n",
        "\n",
        "* Predicts a rating for the selected user-movie pair using the cosine similarity method.\n",
        "* Also tests a fallback scenario where a user has not rated the movie and may lack sufficient neighbor data.\n",
        "* This helps verify that the system can return predictions even when data is sparse.\n",
        "\n",
        "**Conclusion**\n",
        "Each step in this system is designed to make the recommender engine both accurate and resilient. The approach prioritizes reusability and speed (by checking local files first), enhances prediction quality through bias correction, and ensures coverage with intelligent fallback strategies. The result is a scalable and dependable collaborative filtering system for personalized movie recommendations.\n"
      ],
      "metadata": {
        "id": "8Faoy4mQGUwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Load full dataset (already sampled before upload)\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_sampled.csv\")\n",
        "movies = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/movies_subset.csv\")\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# Step 2: Use all users (no sampling)\n",
        "ratings_full = ratings.copy()\n",
        "print(f\"Using {ratings_full['userId'].nunique()} users and {ratings_full['movieId'].nunique()} movies.\")\n",
        "\n",
        "# Step 3: Create user-movie matrix\n",
        "user_movie_matrix = ratings_full.pivot(index='userId', columns='movieId', values='rating')\n",
        "user_means = user_movie_matrix.mean(axis=1)\n",
        "item_means = user_movie_matrix.mean(axis=0)\n",
        "global_mean = ratings_full['rating'].mean()\n",
        "user_ids = user_movie_matrix.index.tolist()\n",
        "\n",
        "# Step 4: Compute or load cosine similarity matrix\n",
        "try:\n",
        "    import gdown\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"gdown\"])\n",
        "    import gdown\n",
        "\n",
        "cosine_sim_file_local = \"cosine_user_similarity_sampled.csv\"\n",
        "cosine_file_drive_id = \"1YMOWK5Acsf9hxDfPHng4k9T0AcO0aQtn\"\n",
        "gdown_url = f\"https://drive.google.com/uc?id={cosine_file_drive_id}\"\n",
        "\n",
        "if not os.path.exists(cosine_sim_file_local):\n",
        "    print(\"Cosine similarity file not found locally. Attempting download...\")\n",
        "    try:\n",
        "        gdown.download(gdown_url, cosine_sim_file_local, quiet=False)\n",
        "    except Exception as e:\n",
        "        print(\"Download failed. Computing cosine similarity matrix...\")\n",
        "        user_movie_centered = user_movie_matrix.sub(user_means, axis=0).fillna(0)\n",
        "        cosine_sim_matrix = cosine_similarity(user_movie_centered.values)\n",
        "        cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=user_ids, columns=user_ids)\n",
        "        cosine_sim_df.to_csv(cosine_sim_file_local)\n",
        "        print(\"Cosine similarity matrix computed and saved locally.\")\n",
        "else:\n",
        "    print(\"File already exists locally.\")\n",
        "    cosine_sim_df = pd.read_csv(cosine_sim_file_local, index_col=0)\n",
        "    cosine_sim_df.columns = cosine_sim_df.columns.astype(int)\n",
        "    cosine_sim_df.index = cosine_sim_df.index.astype(int)\n",
        "\n",
        "# Load the full similarity matrix\n",
        "cosine_sim_df = pd.read_csv(cosine_sim_file_local, index_col=0)\n",
        "cosine_sim_df.columns = cosine_sim_df.columns.astype(int)\n",
        "cosine_sim_df.index = cosine_sim_df.index.astype(int)\n",
        "\n",
        "# Ensure matrix is restricted to the actual users in the data (in case of mismatches)\n",
        "cosine_sim_df = cosine_sim_df.loc[user_ids, user_ids]\n",
        "\n",
        "# Step 5: Define prediction function with bias fallback\n",
        "def predict_user_user_cosine_with_bias(user_id, movie_id, k=10):\n",
        "    if user_id not in user_movie_matrix.index or movie_id not in user_movie_matrix.columns:\n",
        "        return global_mean\n",
        "\n",
        "    user_mean = user_means[user_id]\n",
        "    sims = cosine_sim_df[user_id]\n",
        "\n",
        "    neighbors = user_movie_matrix[movie_id].dropna()\n",
        "    neighbors = neighbors[neighbors.index != user_id]\n",
        "    if neighbors.empty:\n",
        "        return user_mean + item_means.get(movie_id, global_mean) - global_mean\n",
        "\n",
        "    neighbor_sims = sims[neighbors.index]\n",
        "    neighbor_means = user_means[neighbors.index]\n",
        "    neighbor_ratings = neighbors\n",
        "\n",
        "    top_neighbors = neighbor_sims.sort_values(ascending=False).head(k)\n",
        "    top_ratings = neighbor_ratings[top_neighbors.index]\n",
        "    top_means = neighbor_means[top_neighbors.index]\n",
        "\n",
        "    deviations = top_ratings - top_means\n",
        "    weighted_sum = np.dot(top_neighbors, deviations)\n",
        "    sim_sum = np.abs(top_neighbors).sum()\n",
        "\n",
        "    if sim_sum > 0:\n",
        "        prediction = user_mean + (weighted_sum / sim_sum)\n",
        "    else:\n",
        "        prediction = user_mean + item_means.get(movie_id, global_mean) - global_mean\n",
        "\n",
        "    return max(0.5, min(prediction, 5.0))\n",
        "\n",
        "# Step 6: Find testable (user, movie) pair\n",
        "def find_random_user_movie_pair(k=10):\n",
        "    users = user_movie_matrix.index.tolist()\n",
        "    random.shuffle(users)\n",
        "\n",
        "    for user_id in users:\n",
        "        rated = user_movie_matrix.loc[user_id].dropna().index\n",
        "        unrated = user_movie_matrix.columns.difference(rated)\n",
        "        unrated = unrated.tolist()\n",
        "        random.shuffle(unrated)\n",
        "        for movie_id in unrated:\n",
        "            if user_movie_matrix[movie_id].count() > k:\n",
        "                return user_id, movie_id\n",
        "    return None, None\n",
        "\n",
        "\n",
        "# Step 7: Run prediction test\n",
        "user_id, movie_id = find_random_user_movie_pair(k=10)\n",
        "\n",
        "if user_id and movie_id:\n",
        "    pred = predict_user_user_cosine_with_bias(user_id, movie_id, k=10)\n",
        "    movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]\n",
        "    print(f\"\\nPredicted rating for user {user_id} on movie '{movie_title}' (movieId {movie_id}): {pred:.2f}\")\n",
        "else:\n",
        "    print(\"No suitable user-movie pair found.\")\n",
        "\n",
        "# Step 8: Fallback test\n",
        "def test_fallback_same_movie(movie_id, k=10):\n",
        "    eligible_users = user_movie_matrix.index.difference(user_movie_matrix[movie_id].dropna().index)\n",
        "    if eligible_users.empty:\n",
        "        print(\"No eligible users for fallback.\")\n",
        "        return\n",
        "\n",
        "    random_user = random.choice(eligible_users.tolist())\n",
        "    pred = predict_user_user_cosine_with_bias(random_user, movie_id, k=k)\n",
        "    movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]\n",
        "    print(f\"[Fallback] Predicted rating for random user {random_user} on movie '{movie_title}' (movieId {movie_id}): {pred:.2f}\")\n",
        "\n",
        "if movie_id:\n",
        "    test_fallback_same_movie(movie_id, k=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS6Q_NNLBSXN",
        "outputId": "5008f87e-1885-41c9-e7b6-ed7ec59dc042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Using 10000 users and 4933 movies.\n",
            "Downloading cosine similarity matrix using gdown...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1YMOWK5Acsf9hxDfPHng4k9T0AcO0aQtn\n",
            "From (redirected): https://drive.google.com/uc?id=1YMOWK5Acsf9hxDfPHng4k9T0AcO0aQtn&confirm=t&uuid=20e4f4d7-f6c4-4fda-bfdd-6496755249a0\n",
            "To: /content/cosine_user_similarity_sampled.csv\n",
            "100%|██████████| 401M/401M [00:03<00:00, 121MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted rating for user 141770 on movie 'Outbreak (1995)' (movieId 292): 3.51\n",
            "[Fallback] Predicted rating for random user 197720 on movie 'Outbreak (1995)' (movieId 292): 3.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Item-Item Collaborative Filtering Using Jaccard Similarity with Bias-Based Fallback\n",
        "\n",
        "This methodology implements a recommender system based on **item-item collaborative filtering**. It leverages **Jaccard similarity** between items and incorporates a **bias-adjusted fallback mechanism** to produce robust rating predictions in sparse or cold-start scenarios. The approach focuses on whether users have interacted with items rather than how they rated them, making it suitable when explicit feedback is limited.\n",
        "\n",
        "### 1. **Data Preparation and Sampling**\n",
        "\n",
        "To reduce memory and computational overhead, a random sample of 10,000 users is extracted from the original ratings dataset. The system then constructs a **user-movie rating matrix**, where rows represent users, columns represent movies, and values represent ratings.\n",
        "\n",
        "From this matrix, the following statistics are calculated:\n",
        "\n",
        "* **User means** – average rating per user\n",
        "* **Item means** – average rating per item\n",
        "* **Global mean** – overall average rating in the dataset\n",
        "\n",
        "These statistics serve as fallback predictors when sufficient similarity-based signals are not available.\n",
        "\n",
        "### 2. **Binary Matrix and Jaccard Similarity**\n",
        "\n",
        "A binary matrix is generated where:\n",
        "\n",
        "* A value of 1 indicates that a user rated a movie.\n",
        "* A value of 0 indicates no rating.\n",
        "\n",
        "This binary matrix is transposed to form a **movie-user matrix**, which is used to compute **Jaccard similarity** between all pairs of movies:\n",
        "\n",
        "$$\n",
        "\\text{Jaccard}(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $A$ and $B$ are sets of users who rated movies A and B, respectively.\n",
        "* The intersection represents the number of users who rated both.\n",
        "* The union represents users who rated either.\n",
        "\n",
        "To improve efficiency:\n",
        "\n",
        "* The similarity matrix is cached to a local file.\n",
        "* If the file exists or is downloadable from Google Drive, it's reused to avoid recomputation.\n",
        "\n",
        "### 3. **Item-Item Rating Prediction with Bias Adjustment**\n",
        "\n",
        "To predict a user’s rating for a movie using **item-item collaborative filtering**, the algorithm follows these steps:\n",
        "\n",
        "1. Retrieve all movies the user has rated.\n",
        "2. Compute similarity scores between the target movie and these rated movies.\n",
        "3. Select the top-k most similar movies.\n",
        "4. Take a **similarity-weighted average** of the user's ratings for those movies:\n",
        "\n",
        "$$\n",
        "\\hat{r}_{u,i} = \\frac{\\sum_{j \\in N(i)} \\text{sim}(i,j) \\cdot r_{u,j}}{\\sum_{j \\in N(i)} \\text{sim}(i,j)}\n",
        "$$\n",
        "\n",
        "If the denominator (sum of similarities) is zero, indicating no informative neighbors, the system falls back to a **bias-adjusted estimate**:\n",
        "\n",
        "$$\n",
        "\\hat{r}_{u,i} = \\mu_u + \\mu_i - \\mu_{\\text{global}}\n",
        "$$\n",
        "\n",
        "This combines the user’s average rating ($\\mu_u$) and the item’s average rating ($\\mu_i$), offset by the global average to reduce bias accumulation.\n",
        "\n",
        "All predictions are **clamped** to the valid rating range $0.5, 5.0$.\n",
        "\n",
        "### 4. **Cold-Start and Fallback Simulation**\n",
        "\n",
        "A fallback test is included to simulate **cold-start scenarios**, where a user has not rated the target movie. In such cases:\n",
        "\n",
        "* A random user who hasn’t rated the movie is selected.\n",
        "* The prediction function is run with fallback logic engaged.\n",
        "* This ensures the system remains functional even in sparse user-item interaction environments.\n",
        "\n",
        "### 5. **Conclusion**\n",
        "\n",
        "This hybrid item-item collaborative filtering system blends **Jaccard-based similarity** with **statistical bias correction**, ensuring:\n",
        "\n",
        "* Interpretability from co-engagement patterns.\n",
        "* Resilience to sparse data.\n",
        "* Compatibility with binary interaction datasets.\n",
        "\n",
        "It is well-suited for systems where users’ presence or absence (rather than rating intensity) carries the signal of interest — such as click, view, or purchase histories.\n"
      ],
      "metadata": {
        "id": "vAmNpSW9-Jra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "\n",
        "# Step 1: Load data\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_subset.csv\")\n",
        "movies = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/movies_subset.csv\")\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# Step 2: Sample 10,000 users\n",
        "sampled_user_ids = ratings['userId'].drop_duplicates().sample(n=10000, random_state=41)\n",
        "ratings_small = ratings[ratings['userId'].isin(sampled_user_ids)]\n",
        "print(f\"Using {ratings_small['userId'].nunique()} users and {ratings_small['movieId'].nunique()} movies.\")\n",
        "\n",
        "# Step 3: Create user-movie matrix\n",
        "user_movie_matrix = ratings_small.pivot(index='userId', columns='movieId', values='rating')\n",
        "user_means = user_movie_matrix.mean(axis=1)\n",
        "item_means = user_movie_matrix.mean(axis=0)\n",
        "global_mean = ratings_small['rating'].mean()\n",
        "\n",
        "# Step 4: Convert to binary matrix (rated=1, unrated=0), then transpose\n",
        "movie_user_binary = user_movie_matrix.notna().astype(int).T\n",
        "movie_ids = movie_user_binary.index.tolist()\n",
        "\n",
        "# Step 5: Load or compute Jaccard similarity matrix\n",
        "# sim_file = \"jaccard_similarity.csv\"\n",
        "\n",
        "# Step 1: Try loading Jaccard similarity matrix from Google Drive\n",
        "sim_file_drive_id = \"1z-VAYMQF9ZQSuJgkflDg3vJH9m60jwr8\"\n",
        "sim_file_drive_url = f\"https://drive.google.com/uc?export=download&id={sim_file_drive_id}\"\n",
        "sim_file_local = \"jaccard_similarity.csv\"\n",
        "\n",
        "try:\n",
        "    print(\"Trying to load Jaccard similarity matrix from Google Drive...\")\n",
        "    jaccard_sim_df = pd.read_csv(sim_file_drive_url, index_col=0)\n",
        "    jaccard_sim_df.columns = jaccard_sim_df.columns.astype(str).astype(int)\n",
        "    jaccard_sim_df.index = jaccard_sim_df.index.astype(str).astype(int)\n",
        "    print(\"Successfully loaded Jaccard similarity matrix from Google Drive.\")\n",
        "except Exception as e:\n",
        "    if os.path.exists(sim_file_local):\n",
        "        print(\"Failed to load from Drive. Loading from local file...\")\n",
        "        jaccard_sim_df = pd.read_csv(sim_file_local, index_col=0)\n",
        "        jaccard_sim_df.columns = jaccard_sim_df.columns.astype(str).astype(int)\n",
        "        jaccard_sim_df.index = jaccard_sim_df.index.astype(str).astype(int)\n",
        "    else:\n",
        "        print(\"Computing Jaccard similarity matrix...\")\n",
        "        binary_array = movie_user_binary.values.astype(bool)\n",
        "        intersection = np.dot(binary_array, binary_array.T)\n",
        "        row_sums = binary_array.sum(axis=1, keepdims=True)\n",
        "        union = row_sums + row_sums.T - intersection\n",
        "        jaccard_sim_matrix = intersection / np.maximum(union, 1)\n",
        "        jaccard_sim_df = pd.DataFrame(jaccard_sim_matrix, index=movie_ids, columns=movie_ids)\n",
        "        jaccard_sim_df.to_csv(sim_file_local)\n",
        "        print(\"Jaccard similarity matrix computed and saved locally.\")\n",
        "\n",
        "\n",
        "# if os.path.exists(sim_file):\n",
        "#     print(\"Loading Jaccard similarity matrix from file...\")\n",
        "#     jaccard_sim_df = pd.read_csv(sim_file, index_col=0)\n",
        "#     jaccard_sim_df.columns = jaccard_sim_df.columns.astype(str).astype(int)\n",
        "#     jaccard_sim_df.index = jaccard_sim_df.index.astype(str).astype(int)\n",
        "# else:\n",
        "#     print(\"Computing Jaccard similarity matrix...\")\n",
        "#     binary_array = movie_user_binary.values.astype(bool)\n",
        "#     intersection = np.dot(binary_array, binary_array.T)\n",
        "#     row_sums = binary_array.sum(axis=1, keepdims=True)\n",
        "#     union = row_sums + row_sums.T - intersection\n",
        "#     jaccard_sim_matrix = intersection / np.maximum(union, 1)\n",
        "#     jaccard_sim_df = pd.DataFrame(jaccard_sim_matrix, index=movie_ids, columns=movie_ids)\n",
        "#     jaccard_sim_df.to_csv(sim_file)\n",
        "#     print(\"Jaccard similarity matrix computed and saved to file.\")\n",
        "\n",
        "# Step 6: Define prediction function\n",
        "def predict_item_item_jaccard_with_bias(user_id, movie_id, k=10):\n",
        "    if user_id not in user_movie_matrix.index or movie_id not in user_movie_matrix.columns:\n",
        "        return global_mean\n",
        "\n",
        "    user_ratings = user_movie_matrix.loc[user_id].dropna()\n",
        "    if user_ratings.empty or movie_id not in jaccard_sim_df.index:\n",
        "        return global_mean\n",
        "\n",
        "    sims = jaccard_sim_df.loc[movie_id, user_ratings.index]\n",
        "    top_items = sims.sort_values(ascending=False).head(k)\n",
        "    top_ratings = user_ratings[top_items.index]\n",
        "\n",
        "    if top_items.sum() > 0:\n",
        "        prediction = np.dot(top_items, top_ratings) / top_items.sum()\n",
        "    else:\n",
        "        prediction = user_means.get(user_id, global_mean) + item_means.get(movie_id, 0) - global_mean\n",
        "\n",
        "    return max(0.5, min(prediction, 5.0))\n",
        "\n",
        "# Step 7: Find a predictable pair\n",
        "def find_predictable_pair(k=10):\n",
        "    for user_id in user_movie_matrix.index:\n",
        "        rated = user_movie_matrix.loc[user_id].dropna().index\n",
        "        unrated = user_movie_matrix.columns.difference(rated)\n",
        "        for movie_id in unrated:\n",
        "            if user_movie_matrix[movie_id].count() > k:\n",
        "                return user_id, movie_id\n",
        "    return None, None\n",
        "\n",
        "# Step 8: Test prediction\n",
        "user_id, movie_id = find_predictable_pair(k=10)\n",
        "if user_id and movie_id:\n",
        "    pred = predict_item_item_jaccard_with_bias(user_id, movie_id, k=10)\n",
        "    movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]\n",
        "    print(f\"\\nPredicted rating for user {user_id} on movie '{movie_title}' (movieId {movie_id}): {pred:.2f}\")\n",
        "else:\n",
        "    print(\"No suitable user-movie pair found.\")\n",
        "\n",
        "# Step 9: Fallback test\n",
        "def test_bias_fallback_same_movie(movie_id, k=10):\n",
        "    eligible_users = user_movie_matrix.index.difference(user_movie_matrix[movie_id].dropna().index)\n",
        "    if eligible_users.empty:\n",
        "        print(\"No eligible users for fallback.\")\n",
        "        return\n",
        "    random_user = random.choice(eligible_users.tolist())\n",
        "    pred = predict_item_item_jaccard_with_bias(random_user, movie_id, k=k)\n",
        "    movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]\n",
        "    print(f\"[Fallback] Predicted rating for random user {random_user} on movie '{movie_title}' (movieId {movie_id}): {pred:.2f}\")\n",
        "\n",
        "if movie_id:\n",
        "    test_bias_fallback_same_movie(movie_id, k=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBnUyThIvBRG",
        "outputId": "fd0b55d0-637c-470b-c3f6-e6f577a120cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Using 10000 users and 5036 movies.\n",
            "Trying to load Jaccard similarity matrix from Google Drive...\n",
            "Successfully loaded Jaccard similarity matrix from Google Drive.\n",
            "\n",
            "Predicted rating for user 34 on movie 'Toy Story (1995)' (movieId 1): 4.09\n",
            "[Fallback] Predicted rating for random user 146618 on movie 'Toy Story (1995)' (movieId 1): 3.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Item-Item Collaborative Filtering Using Cosine Similarity with Bias-Aware Fallback\n",
        "\n",
        "This recommender system employs item-item collaborative filtering powered by cosine similarity and enhanced with bias-aware fallback logic. It is designed to deliver personalized movie rating predictions while ensuring robustness in cases of sparse data or cold-start users.\n",
        "\n",
        "**1. Data Preparation and Sampling**\n",
        "A random sample of 10,000 users is selected from the ratings dataset to reduce computational load. A user-movie matrix is then constructed with user IDs as rows, movie IDs as columns, and rating values as the matrix entries.\n",
        "From this matrix, the following statistics are computed:\n",
        "\n",
        "* User mean: the average rating each user gives\n",
        "* Item mean: the average rating each movie receives\n",
        "* Global mean: the overall average rating across all users and movies\n",
        "\n",
        "These serve as the baseline for fallback predictions.\n",
        "\n",
        "**2. Cosine Similarity Matrix Construction**\n",
        "The user-movie matrix is transposed to obtain a movie-user matrix. Missing values are filled with zeros so that cosine similarity can be calculated between every pair of movies. Cosine similarity measures how similar two movies are based on users who rated them both, using the formula:\n",
        "\n",
        "cosine(A, B) = (A ⋅ B) / (||A|| × ||B||)\n",
        "\n",
        "Where A and B are rating vectors for two movies.\n",
        "To avoid recomputation, the similarity matrix is saved locally or loaded from a Google Drive file when available.\n",
        "\n",
        "**3. Predicting Ratings Using Top-k Similar Movies**\n",
        "To estimate how a user would rate a movie they haven’t seen, the system:\n",
        "\n",
        "* Identifies the set of movies the user has already rated\n",
        "* Retrieves cosine similarities between the target movie and those rated movies\n",
        "* Selects the top-k most similar movies\n",
        "* Calculates a weighted average of the user’s ratings for those top-k movies using their similarity scores as weights\n",
        "\n",
        "The predicted rating is computed as:\n",
        "\n",
        "r̂(u,i) = ∑ sim(i,j) × r(u,j) / ∑ sim(i,j)\n",
        "\n",
        "If the similarity weights sum to zero, the system triggers the fallback mechanism.\n",
        "\n",
        "**4. Bias-Based Fallback Strategy**\n",
        "When there are no similar movies rated by the user, or if similarity weights are zero, a bias-aware fallback formula is used:\n",
        "\n",
        "r̂(u,i) = μ\\_u + μ\\_i − μ\\_global\n",
        "\n",
        "Where:\n",
        "\n",
        "* μ\\_u is the user’s average rating\n",
        "* μ\\_i is the movie’s average rating\n",
        "* μ\\_global is the global average rating\n",
        "\n",
        "This ensures that even without similarity-based support, the model can make meaningful predictions. All predictions are clamped to the valid range \\[0.5, 5.0].\n",
        "\n",
        "**5. Cold-Start Testing and Prediction Validation**\n",
        "The system includes functionality to identify suitable user-movie pairs for prediction testing, as well as simulate cold-start conditions by selecting users who haven’t rated a specific movie. This allows for evaluation of the fallback mechanism under realistic sparse data scenarios.\n",
        "\n",
        "**Conclusion**\n",
        "This item-item collaborative filtering approach with cosine similarity provides an interpretable and resilient recommendation system. By incorporating user, item, and global bias in its fallback logic, the model remains functional and reliable even in the absence of strong similarity signals.\n"
      ],
      "metadata": {
        "id": "KI7DFfRk2SHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Load data\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/ratings_subset.csv\")\n",
        "movies = pd.read_csv(\"https://raw.githubusercontent.com/hawa1983/DATA-612/refs/heads/main/movies_subset.csv\")\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# Step 2: Sample 10,000 users\n",
        "sampled_user_ids = ratings['userId'].drop_duplicates().sample(n=10000, random_state=41)\n",
        "ratings_small = ratings[ratings['userId'].isin(sampled_user_ids)]\n",
        "print(f\"Using {ratings_small['userId'].nunique()} users and {ratings_small['movieId'].nunique()} movies.\")\n",
        "\n",
        "# Step 3: Create user-movie matrix\n",
        "user_movie_matrix = ratings_small.pivot(index='userId', columns='movieId', values='rating')\n",
        "user_means = user_movie_matrix.mean(axis=1)\n",
        "item_means = user_movie_matrix.mean(axis=0)\n",
        "global_mean = ratings_small['rating'].mean()\n",
        "\n",
        "# Step 4: Transpose and fill NA with 0 for item-item similarity\n",
        "movie_user_matrix = user_movie_matrix.T.fillna(0)\n",
        "movie_ids = movie_user_matrix.index.tolist()\n",
        "\n",
        "# Step 5: Load or compute cosine similarity matrix\n",
        "# Google Drive File ID for cosine similarity matrix\n",
        "cosine_file_drive_id = \"1z-VAYMQF9ZQSuJgkflDg3vJH9m60jwr8\"\n",
        "cosine_file_drive_url = f\"https://drive.google.com/uc?export=download&id={cosine_file_drive_id}\"\n",
        "cosine_sim_file_local = \"cosine_similarity.csv\"\n",
        "\n",
        "try:\n",
        "    print(\"Trying to load cosine similarity matrix from Google Drive...\")\n",
        "    cosine_sim_df = pd.read_csv(cosine_file_drive_url, index_col=0)\n",
        "    cosine_sim_df.columns = cosine_sim_df.columns.astype(int)\n",
        "    cosine_sim_df.index = cosine_sim_df.index.astype(int)\n",
        "    print(\"Successfully loaded cosine similarity matrix from Google Drive.\")\n",
        "except Exception as e:\n",
        "    if os.path.exists(cosine_sim_file_local):\n",
        "        print(\"Failed to load from Drive. Loading from local file...\")\n",
        "        cosine_sim_df = pd.read_csv(cosine_sim_file_local, index_col=0)\n",
        "        cosine_sim_df.columns = cosine_sim_df.columns.astype(int)\n",
        "        cosine_sim_df.index = cosine_sim_df.index.astype(int)\n",
        "    else:\n",
        "        print(\"Computing cosine similarity matrix...\")\n",
        "        cosine_sim_matrix = cosine_similarity(movie_user_matrix.values)\n",
        "        cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=movie_ids, columns=movie_ids)\n",
        "        cosine_sim_df.to_csv(cosine_sim_file_local)\n",
        "        print(\"Cosine similarity matrix computed and saved locally.\")\n",
        "\n",
        "# sim_file = \"cosine_item_similarity.csv\"\n",
        "# if os.path.exists(sim_file):\n",
        "#     print(\"Loading cosine similarity matrix from file...\")\n",
        "#     cosine_sim_df = pd.read_csv(sim_file, index_col=0)\n",
        "#     cosine_sim_df.columns = cosine_sim_df.columns.astype(int)\n",
        "#     cosine_sim_df.index = cosine_sim_df.index.astype(int)\n",
        "# else:\n",
        "#     print(\"Computing cosine similarity matrix...\")\n",
        "#     cosine_sim_matrix = cosine_similarity(movie_user_matrix.values)\n",
        "#     cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=movie_ids, columns=movie_ids)\n",
        "#     cosine_sim_df.to_csv(sim_file)\n",
        "#     print(\"Cosine similarity matrix computed and saved to file.\")\n",
        "\n",
        "# Step 6: Define prediction function\n",
        "def predict_item_item_cosine_with_bias(user_id, movie_id, k=10):\n",
        "    if user_id not in user_movie_matrix.index or movie_id not in user_movie_matrix.columns:\n",
        "        return global_mean\n",
        "\n",
        "    user_ratings = user_movie_matrix.loc[user_id].dropna()\n",
        "    if user_ratings.empty or movie_id not in cosine_sim_df.index:\n",
        "        return global_mean\n",
        "\n",
        "    sims = cosine_sim_df.loc[movie_id, user_ratings.index]\n",
        "    top_items = sims.sort_values(ascending=False).head(k)\n",
        "    top_ratings = user_ratings[top_items.index]\n",
        "\n",
        "    if top_items.sum() > 0:\n",
        "        prediction = np.dot(top_items, top_ratings) / top_items.sum()\n",
        "    else:\n",
        "        prediction = user_means.get(user_id, global_mean) + item_means.get(movie_id, 0) - global_mean\n",
        "\n",
        "    return max(0.5, min(prediction, 5.0))\n",
        "\n",
        "# Step 7: Find a predictable pair\n",
        "def find_predictable_pair(k=10):\n",
        "    for user_id in user_movie_matrix.index:\n",
        "        rated = user_movie_matrix.loc[user_id].dropna().index\n",
        "        unrated = user_movie_matrix.columns.difference(rated)\n",
        "        for movie_id in unrated:\n",
        "            if user_movie_matrix[movie_id].count() > k:\n",
        "                return user_id, movie_id\n",
        "    return None, None\n",
        "\n",
        "# Step 8: Test prediction\n",
        "user_id, movie_id = find_predictable_pair(k=10)\n",
        "if user_id and movie_id:\n",
        "    pred = predict_item_item_cosine_with_bias(user_id, movie_id, k=10)\n",
        "    movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]\n",
        "    print(f\"\\nPredicted rating for user {user_id} on movie '{movie_title}' (movieId {movie_id}): {pred:.2f}\")\n",
        "else:\n",
        "    print(\"No suitable user-movie pair found.\")\n",
        "\n",
        "# Step 9: Fallback test\n",
        "def test_bias_fallback_same_movie(movie_id, k=10):\n",
        "    eligible_users = user_movie_matrix.index.difference(user_movie_matrix[movie_id].dropna().index)\n",
        "    if eligible_users.empty:\n",
        "        print(\"No eligible users for fallback.\")\n",
        "        return\n",
        "    random_user = random.choice(eligible_users.tolist())\n",
        "    pred = predict_item_item_cosine_with_bias(random_user, movie_id, k=k)\n",
        "    movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]\n",
        "    print(f\"[Fallback] Predicted rating for random user {random_user} on movie '{movie_title}' (movieId {movie_id}): {pred:.2f}\")\n",
        "\n",
        "if movie_id:\n",
        "    test_bias_fallback_same_movie(movie_id, k=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2IM660b2YDS",
        "outputId": "087c3233-60f5-4638-89be-6eea410cbf3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Using 10000 users and 5036 movies.\n",
            "Trying to load cosine similarity matrix from Google Drive...\n",
            "Successfully loaded cosine similarity matrix from Google Drive.\n",
            "\n",
            "Predicted rating for user 34 on movie 'Toy Story (1995)' (movieId 1): 4.09\n",
            "[Fallback] Predicted rating for random user 110110 on movie 'Toy Story (1995)' (movieId 1): 4.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluation: RMSE Comparison"
      ],
      "metadata": {
        "id": "B31CO5UG5xCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data for evaluation\n",
        "train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Predict ratings using both collaborative methods\n",
        "user_preds = test.apply(lambda row: predict_user_user(row['userId'], row['movieId']), axis=1)\n",
        "item_preds = test.apply(lambda row: predict_item_item(row['userId'], row['movieId']), axis=1)\n",
        "\n",
        "# Calculate RMSE\n",
        "user_rmse = np.sqrt(mean_squared_error(test['rating'].dropna(), user_preds.dropna()))\n",
        "item_rmse = np.sqrt(mean_squared_error(test['rating'].dropna(), item_preds.dropna()))\n",
        "\n",
        "# Plot RMSE comparison\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(['User-User (Pearson)', 'Item-Item (Adjusted Cosine)'], [user_rmse, item_rmse], color=['blue', 'green'])\n",
        "plt.title(\"RMSE Comparison of Collaborative Filtering Models\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2KIVFVMX51_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Summary Output"
      ],
      "metadata": {
        "id": "2PLdfswj55KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Summary ---\")\n",
        "print(f\"User-User RMSE (Pearson): {user_rmse:.4f}\")\n",
        "print(f\"Item-Item RMSE (Adjusted Cosine): {item_rmse:.4f}\")\n",
        "print(\"Content-Based filtering used L2-normalized cosine similarity on genre vectors.\")\n",
        "print(\"User-user filtering used Pearson correlation and centered ratings.\")\n",
        "print(\"Item-item filtering used adjusted cosine similarity with user-centered item vectors.\")\n"
      ],
      "metadata": {
        "id": "SCyM-ObK590E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}