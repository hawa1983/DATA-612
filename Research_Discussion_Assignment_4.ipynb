{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEYTh3mVfHWXNHfW3MKVy1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-612/blob/main/Research_Discussion_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mitigating the Harm of Recommender Systems\n",
        "\n",
        "Recommender systems have become central to how we consume content, make purchasing decisions, and even shape our political views. However, their influence has raised critical concerns about bias, polarization, and fairness. This discussion synthesizes several academic and journalistic sources that highlight the risks of recommendation engines and offer strategies for mitigating their harms.\n",
        "\n",
        "Renee Diresta’s article “Up Next: A Better Recommendation System” highlights how YouTube’s autoplay algorithm steers users from relatively benign content into more extreme material. For example, watching health videos could soon lead users down a rabbit hole of anti-vaccine or conspiracy content. She proposes adding friction into the recommendation process—such as pauses or disclaimers—and incorporating quality and diversity considerations into what gets recommended. Zeynep Tufekci's “YouTube, the Great Radicalizer” echoes this concern, pointing out how the platform’s engagement-driven algorithm incentivizes extremity. Her analysis calls for greater transparency and more control in how users interact with recommender systems, particularly by offering explanations and alternative recommendation paths.\n",
        "\n",
        "Sanjay Krishnan and colleagues dive into a more technical but equally important issue: social influence bias. Their research shows that when users are shown average or median ratings, they tend to conform to those scores, reducing rating diversity. This conformity affects downstream algorithms trained on that data, ultimately reinforcing a cycle of homogeneity. To mitigate this, the authors propose a three-phase methodology: first, capturing pre- and post-rating data; second, using statistical tests like the Wilcoxon signed-rank to quantify the bias; and third, applying polynomial regression to correct skewed ratings. Their model, when applied to a real-world civic engagement platform, was able to reverse 76% of the observed bias.\n",
        "\n",
        "Additional studies provide complementary insights. Gunawardana and Shani emphasize the importance of aligning evaluation metrics with the system’s task. They point out that metrics like RMSE are often misapplied to ranking problems, leading to poor performance in real-world recommender settings. Instead, they advocate for utility-based and ranking-aware metrics depending on whether the system aims to predict ratings or recommend top-N items. Moreira et al. investigate the distortion caused by overly popular items, finding that offline evaluations often overestimate performance because of these popularity effects. By filtering out the most popular items and focusing on the long tail, they achieved better alignment between offline and online performance metrics, suggesting that true relevance lies not in what is most clicked, but in what is personally meaningful and overlooked.\n",
        "\n",
        "The Wired (2021) article “Creating Ethical Recommendation Engines” ties many of these threads together by arguing that recommender systems should be designed with ethics at their core. The piece critiques the standard engagement-optimization approach and argues for embedding fairness, user agency, and transparency into system design. Practical recommendations include offering users “knobs” to tune their recommendations, opening up algorithmic logic for public auditing, and involving ethicists and social scientists in the design process.\n",
        "\n",
        "Across all sources, a few consistent themes emerge. First, recommender systems should be optimized not just for clicks or watch time but for long-term user well-being and informational diversity. Second, evaluation methodologies need to be updated to reflect real-world goals—whether that means accounting for bias, mitigating conformity, or measuring usefulness. Third, users need greater visibility into and control over how recommendations are generated. Finally, developers and researchers need to accept that recommender systems have social and political implications that go well beyond technical performance.\n",
        "\n",
        "As data scientists, we must recognize that our models don’t operate in a vacuum. Every ranking function, similarity metric, and training dataset we choose can either mitigate or magnify societal harms. It is our responsibility to design systems that are not only accurate but also fair, accountable, and resistant to manipulation. The future of recommendation depends on how seriously we take these ethical responsibilities today."
      ],
      "metadata": {
        "id": "MhU5p26TJQlL"
      }
    }
  ]
}