{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi9SvqsLwP46QFw851xDOs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-612/blob/main/Research_Discussion_Assignment_3_Bias_in_Recommender_Systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research Discussion Assignment 3**\n",
        "- **Bias in Recommender Systems**\n",
        "- ***Fomba Kassoh***\n",
        "- *June 24, 2025*\n",
        "\n",
        "DATA 612: Research Discussion Assignment 3\n",
        "After working with different recommender system methods like Matrix Factorization (ALS and SVD), Content-Based Filtering, User-User Collaborative Filtering, and Item-Item Collaborative Filtering, I started to notice that these models can do more than just make good suggestions. They can also reflect and reinforce human bias.\n",
        "\n",
        "A lot of this comes down to the data. If the data already has bias in it, the model is likely to learn and repeat that. One example from Evan Estola's talk (2016) described how Meetup had to make sure their recommendation system did not assume women were not interested in tech events. The data showed fewer women in tech meetups, but that was due to larger social issues, not personal preferences. If the model had learned from that pattern, it would have kept making the problem worse.\n",
        "\n",
        "This kind of issue shows up in different ways depending on the method. User-User and Item-Item Collaborative Filtering both rely on what similar users have done. That can easily mean underrepresented groups get excluded, just because people like them had limited access or exposure to certain items in the past. The model is not intentionally biased, but the result can still be unfair.\n",
        "Matrix Factorization methods like ALS and SVD reduce the data into hidden features. These latent factors might seem neutral, but they can still capture bias in subtle ways that are hard to detect. Content-Based Filtering is a little different because it uses item features and past user preferences, but it can still trap users in a narrow view. If you liked a certain type of item before, the model will keep recommending more of the same, which may limit discovery and diversity.\n",
        "\n",
        "The paper by Hardt, Price, and Srebro (2016) talks about a better way to handle fairness. They introduce the idea of \"equal opportunity,\" which means people who are qualified should have the same chance of being recommended something, regardless of race, gender, or other protected attributes. Their approach is useful because it does not require changing the whole training process. Instead, you can apply a correction step afterward that balances fairness with accuracy.\n",
        "\n",
        "Overall, I think recommender systems need to be checked carefully. The methods we use can seem neutral, but they are learning from the past. If that past includes inequality, the models can make it worse. Fairness tools like post-processing adjustments or audits by group can help. Most importantly, we should ask whether the system is helping people discover new things or just repeating the same patterns they have already seen.\n",
        "\n"
      ],
      "metadata": {
        "id": "POaa6Y-kdVpR"
      }
    }
  ]
}