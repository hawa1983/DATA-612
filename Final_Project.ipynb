{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-612/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose**\n",
        "\n",
        "The purpose of this script is to enrich the MovieLens movie dataset (`movies.dat`) with detailed movie metadata from The Movie Database (TMDB) API. This metadata includes movie overviews, genres, poster and backdrop image URLs, cast and director information, keywords, user ratings, and trailer links. The enriched dataset will serve as the foundation for building content-based, collaborative, and hybrid recommender systems.\n",
        "\n",
        "### **Methodology**\n",
        "\n",
        "1. **Load MovieLens Movie Data**\n",
        "   The script loads the `movies.dat` file, which contains basic movie information including `movieId`, `title`, and `genres`.\n",
        "\n",
        "2. **Clean Titles and Extract Years**\n",
        "   It processes the movie titles to remove the year from the title string and separately extracts the release year to improve search accuracy when querying TMDB.\n",
        "\n",
        "3. **Query TMDB API**\n",
        "   For each movie, it sends a search request to TMDB using the cleaned title and release year. If a match is found, it retrieves the movieâ€™s TMDB ID.\n",
        "\n",
        "4. **Retrieve Detailed Metadata**\n",
        "   Using the TMDB ID, the script fetches:\n",
        "\n",
        "   * Overview (plot summary)\n",
        "   * Poster and backdrop image paths\n",
        "   * Genre IDs, which are then mapped to readable genre names\n",
        "   * Top 3 cast members\n",
        "   * Director(s)\n",
        "   * Associated keywords\n",
        "   * YouTube trailer link (if available)\n",
        "\n",
        "5. **Construct and Save Enriched Dataset**\n",
        "   All metadata is compiled into a structured format and merged with the original MovieLens data. The final dataset is saved as `movies_enriched_full.csv` for downstream use in recommendation models.\n"
      ],
      "metadata": {
        "id": "OptAJ12iBc8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "7qJ0maYGaxtJ",
        "outputId": "2cab814c-7502-488a-f415-fd6922272d72"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'movies.dat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-494969615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Load movies.dat - format: MovieID::Title::Genres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmovies_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"movies.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"genres\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# ---------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movies.dat'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ---------------------------------------\n",
        "# CONFIG\n",
        "# ---------------------------------------\n",
        "BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "IMAGE_BASE = \"https://image.tmdb.org/t/p/w500\"\n",
        "\n",
        "# Use your TMDB Bearer Token (v4)\n",
        "HEADERS = {\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyZGZlNjMwMGMzYjIzMjc2NzExNjQ0N2JhNzhiMjM5MyIsIm5iZiI6MTc1MTkyMjA3Ni4xMzUsInN1YiI6IjY4NmMzNTljMzc4NjllOGEyNDUxZTM0OSIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.S773ddH3FiIHtokPW4sYpJog0mXWS1o4OPov1KZneUw\"\n",
        "}\n",
        "\n",
        "# TMDB genre ID to name mapping\n",
        "GENRE_ID_TO_NAME = {\n",
        "    28: \"Action\", 12: \"Adventure\", 16: \"Animation\", 35: \"Comedy\", 80: \"Crime\",\n",
        "    99: \"Documentary\", 18: \"Drama\", 10751: \"Family\", 14: \"Fantasy\", 36: \"History\",\n",
        "    27: \"Horror\", 10402: \"Music\", 9648: \"Mystery\", 10749: \"Romance\", 878: \"Science Fiction\",\n",
        "    10770: \"TV Movie\", 53: \"Thriller\", 10752: \"War\", 37: \"Western\"\n",
        "}\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 1: Load MovieLens .dat Files\n",
        "# ---------------------------------------\n",
        "\n",
        "# Load movies.dat - format: MovieID::Title::Genres\n",
        "movies_df = pd.read_csv(\"movies.dat\", sep=\"::\", engine='python', header=None, names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 2: Clean Movie Titles and Extract Year\n",
        "# ---------------------------------------\n",
        "\n",
        "def extract_year(title):\n",
        "    if \"(\" in title:\n",
        "        try:\n",
        "            return int(title.strip()[-5:-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_title(title):\n",
        "    if \"(\" in title:\n",
        "        return title[:title.rfind(\"(\")].strip()\n",
        "    return title.strip()\n",
        "\n",
        "movies_df[\"year\"] = movies_df[\"title\"].apply(extract_year)\n",
        "movies_df[\"clean_title\"] = movies_df[\"title\"].apply(clean_title)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 3: TMDB Metadata Functions\n",
        "# ---------------------------------------\n",
        "\n",
        "# Search for movie in TMDB\n",
        "def search_tmdb(title, year):\n",
        "    url = f\"{BASE_URL}/search/movie\"\n",
        "    params = {\"query\": title, \"year\": year}\n",
        "    response = requests.get(url, headers=HEADERS, params=params)\n",
        "    r = response.json()\n",
        "    if r.get(\"results\"):\n",
        "        return r[\"results\"][0]\n",
        "    return None\n",
        "\n",
        "# Get full metadata from TMDB\n",
        "def get_full_tmdb_metadata(tmdb_id):\n",
        "    metadata = {}\n",
        "\n",
        "    # Credits (cast, crew)\n",
        "    credits = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/credits\", headers=HEADERS).json()\n",
        "    cast = [c[\"name\"] for c in credits.get(\"cast\", [])[:3]]\n",
        "    directors = [c[\"name\"] for c in credits.get(\"crew\", []) if c.get(\"job\") == \"Director\"]\n",
        "\n",
        "    # Keywords\n",
        "    keywords = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/keywords\", headers=HEADERS).json()\n",
        "    keyword_list = [k[\"name\"] for k in keywords.get(\"keywords\", [])]\n",
        "\n",
        "    # Videos (trailers)\n",
        "    videos = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/videos\", headers=HEADERS).json()\n",
        "    trailer_links = [\n",
        "        f\"https://www.youtube.com/watch?v={v['key']}\"\n",
        "        for v in videos.get(\"results\", [])\n",
        "        if v[\"site\"] == \"YouTube\" and v[\"type\"] == \"Trailer\"\n",
        "    ]\n",
        "\n",
        "    # Final metadata dictionary\n",
        "    metadata[\"top_3_cast\"] = \", \".join(cast)\n",
        "    metadata[\"directors\"] = \", \".join(directors)\n",
        "    metadata[\"keywords\"] = \", \".join(keyword_list)\n",
        "    metadata[\"trailer_link\"] = trailer_links[0] if trailer_links else None\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 4: Enrich Movie Data\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched = []\n",
        "\n",
        "for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
        "    movie_data = search_tmdb(row[\"clean_title\"], row[\"year\"])\n",
        "\n",
        "    if movie_data:\n",
        "        tmdb_id = movie_data[\"id\"]\n",
        "        extra = get_full_tmdb_metadata(tmdb_id)\n",
        "\n",
        "        genre_ids = movie_data.get(\"genre_ids\", [])\n",
        "        genre_names = [GENRE_ID_TO_NAME.get(gid, str(gid)) for gid in genre_ids]\n",
        "\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": tmdb_id,\n",
        "            \"overview\": movie_data.get(\"overview\", \"\"),\n",
        "            \"poster_path\": IMAGE_BASE + movie_data.get(\"poster_path\", \"\") if movie_data.get(\"poster_path\") else None,\n",
        "            \"backdrop_path\": IMAGE_BASE + movie_data.get(\"backdrop_path\", \"\") if movie_data.get(\"backdrop_path\") else None,\n",
        "            \"vote_average\": movie_data.get(\"vote_average\", None),\n",
        "            \"vote_count\": movie_data.get(\"vote_count\", None),\n",
        "            \"tmdb_genres\": \", \".join(genre_names),\n",
        "            **extra\n",
        "        })\n",
        "    else:\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": None,\n",
        "            \"overview\": None,\n",
        "            \"poster_path\": None,\n",
        "            \"backdrop_path\": None,\n",
        "            \"vote_average\": None,\n",
        "            \"vote_count\": None,\n",
        "            \"tmdb_genres\": None,\n",
        "            \"top_3_cast\": None,\n",
        "            \"directors\": None,\n",
        "            \"keywords\": None,\n",
        "            \"trailer_link\": None\n",
        "        })\n",
        "\n",
        "    time.sleep(0.25)  # Respect TMDB API rate limits\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 5: Save Final Dataset\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched)\n",
        "final_df = pd.concat([movies_df, enriched_df], axis=1)\n",
        "final_df.to_csv(\"movies_enriched_full.csv\", index=False)\n",
        "\n",
        "print(\"DONE: Saved as 'movies_enriched_full.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Personalized Content-Based Movie Recommendation System**\n",
        "\n",
        "This Python script implements a **Content-Based Filtering (CBF)** system enhanced with **personalized recommendations** using user-specific rating profiles. Built using the MovieLens 1M dataset and enriched metadata, the pipeline performs vectorization, similarity computation, and profile-based predictions.\n",
        "\n",
        "**What This Script Does**\n",
        "\n",
        "* **Module 1â€“2**: Load essential libraries and enriched movie data.\n",
        "* **Module 3**: Load user ratings and demographics.\n",
        "* **Module 4**: Engineer features combining genres, cast, crew, keywords, and movie overviews.\n",
        "* **Module 5**: Transform content into TF-IDF, Count, or Binary vectors, and compute pairwise similarities using Cosine or Jaccard metrics.\n",
        "* **Module 6**: Construct a weighted content profile per user based on past ratings.\n",
        "* **Module 7**: Recommend top-N movies similar to the user profile, excluding already seen titles.\n",
        "\n",
        "**Techniques Used**\n",
        "\n",
        "* **Text Vectorization**: TF-IDF, CountVectorizer, Binary Count\n",
        "* **Similarity Metrics**: Cosine Similarity, Jaccard Similarity\n",
        "* **Personalization**: Weighted vector averaging based on each userâ€™s rated items\n",
        "* **Parallelization**: Speeds up Jaccard similarity computation using joblib\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "* Personalized recommendations for new users with a few ratings (cold-start)\n",
        "* Improving diversity and relevance in suggested movies\n",
        "* Generating fallback content suggestions in hybrid recommender systems"
      ],
      "metadata": {
        "id": "c7KNZkOThu7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 1: Imports & Configuration\n",
        "# ==============================\n",
        "# Purpose: Load all required libraries and set global display settings.\n",
        "# Application: Enables necessary tools for data manipulation, modeling, and visualization.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# ==============================\n",
        "# Module 2: Load Movie Data\n",
        "# ==============================\n",
        "# Purpose: Load enriched movie metadata from a CSV file.\n",
        "# Application: Used as the content base for movie features in recommender systems.\n",
        "\n",
        "def load_movie_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    print(f\"Loaded {len(df)} movies.\")\n",
        "    return df\n",
        "\n",
        "# ==============================\n",
        "# Module 3: Load User Ratings and Demographics\n",
        "# ==============================\n",
        "# Purpose: Load MovieLens ratings and user demographic info.\n",
        "# Application: Used to identify which movies users rated and to segment users by profile.\n",
        "\n",
        "def load_user_data(ratings_path, users_path):\n",
        "    ratings = pd.read_csv(ratings_path, sep=\"::\", engine=\"python\",\n",
        "                          names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "    users = pd.read_csv(users_path, sep=\"::\", engine=\"python\",\n",
        "                        names=[\"userId\", \"gender\", \"age\", \"occupation\", \"zip\"])\n",
        "    print(f\"Loaded {len(ratings)} ratings and {len(users)} users.\")\n",
        "    return ratings, users\n",
        "\n",
        "# ==============================\n",
        "# Module 4: Feature Engineering\n",
        "# ==============================\n",
        "# Purpose: Merge movie metadata into a single feature string.\n",
        "# Application: Used as input to text vectorization for computing similarity.\n",
        "\n",
        "def create_feature_string(df):\n",
        "    def split_and_clean(col, delimiter='|'):\n",
        "        return col.fillna('').str.replace(r'\\s+', '', regex=True).str.split(delimiter)\n",
        "\n",
        "    genre_list_1 = split_and_clean(df['genres'], delimiter='|')\n",
        "    genre_list_2 = split_and_clean(df['tmdb_genres'], delimiter=',')\n",
        "    merged_genres = [\n",
        "        ' '.join(sorted(set(g1 or []) | set(g2 or [])))\n",
        "        for g1, g2 in zip(genre_list_1, genre_list_2)\n",
        "    ]\n",
        "\n",
        "    def clean_text(col):\n",
        "        return col.fillna('').str.replace(r'\\s+', '', regex=True).str.replace(',', ' ')\n",
        "\n",
        "    overview_clean = df['overview'].fillna('').str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
        "\n",
        "    df['cbf_features'] = (\n",
        "        pd.Series(merged_genres) + ' ' +\n",
        "        clean_text(df['keywords']) + ' ' +\n",
        "        clean_text(df['top_3_cast']) + ' ' +\n",
        "        clean_text(df['directors']) + ' ' +\n",
        "        overview_clean\n",
        "    )\n",
        "\n",
        "    return df[['movieId', 'title', 'cbf_features']]\n",
        "\n",
        "# ==============================\n",
        "# Module 5: Vectorization & Similarity\n",
        "# ==============================\n",
        "# Purpose: Convert feature strings into vectors and compute pairwise similarity.\n",
        "# Application: Supports similarity scoring between movies.\n",
        "\n",
        "def vectorize_features(text_series, method='tfidf'):\n",
        "    if method == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    elif method == 'count':\n",
        "        vectorizer = CountVectorizer(stop_words='english')\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'tfidf' or 'count'\")\n",
        "\n",
        "    matrix = vectorizer.fit_transform(text_series)\n",
        "    print(f\"{method.upper()} vectorization complete. Shape: {matrix.shape}\")\n",
        "    return matrix, vectorizer\n",
        "\n",
        "def binary_vectorize(text_series):\n",
        "    vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
        "    matrix = vectorizer.fit_transform(text_series)\n",
        "    print(f\"Binary Count vectorization complete. Shape: {matrix.shape}\")\n",
        "    return matrix.toarray(), vectorizer\n",
        "\n",
        "def compute_cosine_similarity(matrix):\n",
        "    sim = cosine_similarity(matrix)\n",
        "    print(\"Cosine similarity computed.\")\n",
        "    return sim\n",
        "\n",
        "def jaccard_pairwise_parallel(matrix):\n",
        "    n = matrix.shape[0]\n",
        "    sim_matrix = np.zeros((n, n))\n",
        "\n",
        "    def jaccard_row(i):\n",
        "        a = matrix[i]\n",
        "        row_sim = np.zeros(n)\n",
        "        for j in range(i, n):\n",
        "            b = matrix[j]\n",
        "            intersection = np.logical_and(a, b).sum()\n",
        "            union = np.logical_or(a, b).sum()\n",
        "            score = intersection / union if union > 0 else 0.0\n",
        "            row_sim[j] = score\n",
        "        return i, row_sim\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(\n",
        "        delayed(jaccard_row)(i) for i in tqdm(range(n), desc=\"Jaccard Similarity\")\n",
        "    )\n",
        "\n",
        "    for i, row in results:\n",
        "        sim_matrix[i, i:] = row[i:]\n",
        "        sim_matrix[i:, i] = row[i:]\n",
        "\n",
        "    print(\"Jaccard similarity matrix built.\")\n",
        "    return sim_matrix\n",
        "\n",
        "def save_matrix(matrix, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(matrix, f)\n",
        "    print(f\"Saved similarity matrix to: {filename}\")\n",
        "\n",
        "# ==============================\n",
        "# Module 6: Build User Profile\n",
        "# ==============================\n",
        "# Purpose: Create a personalized vector from a user's rated movies.\n",
        "# Application: Encapsulates a user's preferences for use in recommendation.\n",
        "\n",
        "def build_user_profile(user_id, ratings, tfidf_matrix, movie_df):\n",
        "    user_ratings = ratings[ratings['userId'] == user_id]\n",
        "    rated_movies = movie_df[movie_df['movieId'].isin(user_ratings['movieId'])]\n",
        "    indices = rated_movies.index.tolist()\n",
        "    weights = user_ratings.set_index('movieId').loc[rated_movies['movieId']]['rating'].values\n",
        "    profile = np.average(tfidf_matrix[indices].toarray(), axis=0, weights=weights)\n",
        "    return profile.reshape(1, -1)\n",
        "\n",
        "# ==============================\n",
        "# Module 7: Personalized Recommendation\n",
        "# ==============================\n",
        "# Purpose: Generate recommendations personalized to the user profile.\n",
        "# Application: This module uses user ratings to build a content preference profile and recommend similar movies.\n",
        "\n",
        "def recommend_movies(user_id, ratings, tfidf_matrix, movie_df, top_n=50):\n",
        "    user_profile = build_user_profile(user_id, ratings, tfidf_matrix, movie_df)\n",
        "    sims = cosine_similarity(user_profile, tfidf_matrix).flatten()\n",
        "    user_seen = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(user_seen)].index\n",
        "    top_indices = unseen_indices[np.argsort(sims[unseen_indices])[-top_n:][::-1]]\n",
        "    return movie_df.iloc[top_indices][['movieId', 'title']], sims[top_indices]\n"
      ],
      "metadata": {
        "id": "3T-K1RtG-IUk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***1. Run CBF Data Preparation***\n"
      ],
      "metadata": {
        "id": "N2ba6kbAlAdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run CBF Data Preparation ---\n",
        "movie_df = load_movie_data('movies_enriched_full.csv')\n",
        "movie_df = create_feature_string(movie_df)\n",
        "\n",
        "# Optional: preview one example\n",
        "print(\"\\nSample feature string:\")\n",
        "print(movie_df.loc[0, ['title', 'cbf_features']])\n"
      ],
      "metadata": {
        "id": "l_eO7-QZlGLY",
        "outputId": "d904ff8c-f18c-402e-f372-5b5c3caec2a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3883 movies.\n",
            "\n",
            "Sample feature string:\n",
            "title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Toy Story (1995)\n",
            "cbf_features    Adventure Animation Children's Comedy Family rescue friendship mission jealousy villain bullying elementaryschool rivalry anthropomorphism friends computeranimation buddy walkietalkie toycar boynextdoor newtoy neighborhood toycomestolife resourcefulness toy TomHanks TimAllen DonRickles JohnLasseter led by woody andys toys live happily in his room until andys birthday brings buzz lightyear onto the scene afraid of losing his place in andys heart woody plots against buzz but when circumstances separate buzz and woody from their owner the duo eventually learns to put aside their differences\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***2. Run Vectorization & Similarity Calculation***\n"
      ],
      "metadata": {
        "id": "vj82RTA_ldyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run TF-IDF Vectorization and Cosine Similarity ---\n",
        "tfidf_matrix, tfidf_vectorizer = vectorize_features(movie_df['cbf_features'], method='tfidf')\n",
        "cosine_sim_matrix = compute_cosine_similarity(tfidf_matrix)\n",
        "save_matrix(cosine_sim_matrix, 'cbf_cosine_similarity_tfidf.pkl')\n",
        "\n",
        "# --- Run Count Vectorization and Cosine Similarity (Optional) ---\n",
        "count_matrix, count_vectorizer = vectorize_features(movie_df['cbf_features'], method='count')\n",
        "cosine_sim_count = compute_cosine_similarity(count_matrix)\n",
        "save_matrix(cosine_sim_count, 'cbf_cosine_similarity_count.pkl')\n",
        "\n",
        "# --- Run Binary Vectorization and Jaccard Similarity ---\n",
        "binary_matrix, _ = binary_vectorize(movie_df['cbf_features'])\n",
        "jaccard_sim_matrix = jaccard_pairwise_parallel(binary_matrix)\n",
        "save_matrix(jaccard_sim_matrix, 'cbf_jaccard_similarity.pkl')\n"
      ],
      "metadata": {
        "id": "1eW3s2j-ltx4",
        "outputId": "5573a207-60c0-4a02-fbd6-681def7a2c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFIDF vectorization complete. Shape: (3883, 33424)\n",
            "Cosine similarity computed.\n",
            "Saved similarity matrix to: cbf_cosine_similarity_tfidf.pkl\n",
            "COUNT vectorization complete. Shape: (3883, 33424)\n",
            "Cosine similarity computed.\n",
            "Saved similarity matrix to: cbf_cosine_similarity_count.pkl\n",
            "Binary Count vectorization complete. Shape: (3883, 33424)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jaccard Similarity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3883/3883 [17:31<00:00,  3.69it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity matrix built.\n",
            "Saved similarity matrix to: cbf_jaccard_similarity.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***3. Run Personalized Recommendation for a User***\n"
      ],
      "metadata": {
        "id": "uJl7EPALl55_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load User Data ---\n",
        "ratings, users = load_user_data('ratings.dat', 'users.dat')\n",
        "\n",
        "# --- Generate Recommendations for a Sample User ---\n",
        "user_id = 5549\n",
        "recommended_df, similarity_scores = recommend_movies(user_id, ratings, tfidf_matrix, movie_df, top_n=50)\n",
        "\n",
        "print(f\"\\nTop 50 personalized movie recommendations for User {user_id}:\")\n",
        "for i, (title, score) in enumerate(zip(recommended_df['title'], similarity_scores)):\n",
        "    print(f\"{i+1}. {title} â€” Score: {score:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "85JyzrqKmDmC",
        "outputId": "b8d5eee6-ad35-4e26-8c00-a9b63e38f912",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1000209 ratings and 6040 users.\n",
            "\n",
            "Top 50 personalized movie recommendations for User 5549:\n",
            "1. Professional, The (a.k.a. Leon: The Professional) (1994) â€” Score: 0.3010\n",
            "2. Blood In, Blood Out (a.k.a. Bound by Honor) (1993) â€” Score: 0.2915\n",
            "3. F/X 2 (1992) â€” Score: 0.2337\n",
            "4. Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) â€” Score: 0.2261\n",
            "5. Othello (1952) â€” Score: 0.2261\n",
            "6. White Boys (1999) â€” Score: 0.2261\n",
            "7. Phantom Love (Ai No Borei) (1978) â€” Score: 0.2261\n",
            "8. All the Rage (a.k.a. It's the Rage) (1999) â€” Score: 0.2261\n",
            "9. Black Tights (Les Collants Noirs) (1960) â€” Score: 0.2261\n",
            "10. Forbidden Christ, The (Cristo proibito, Il) (1950) â€” Score: 0.2261\n",
            "11. Story of Xinghua, The (1993) â€” Score: 0.2261\n",
            "12. Day the Sun Turned Cold, The (Tianguo niezi) (1994) â€” Score: 0.2261\n",
            "13. Ciao, Professore! (Io speriamo che me la cavo ) (1993) â€” Score: 0.2261\n",
            "14. Two Moon Juction (1988) â€” Score: 0.2261\n",
            "15. Naturally Native (1998) â€” Score: 0.2261\n",
            "16. Jules and Jim (Jules et Jim) (1961) â€” Score: 0.2261\n",
            "17. Silence of the Palace, The (Saimt el Qusur) (1994) â€” Score: 0.2261\n",
            "18. Ballad of Narayama, The (Narayama Bushiko) (1982) â€” Score: 0.2261\n",
            "19. Boys of St. Vincent, The (1993) â€” Score: 0.2261\n",
            "20. And God Created Woman (Et Dieu&#8230;CrÃ©a la Femme) (1956) â€” Score: 0.2261\n",
            "21. Terrorist, The (Malli) (1998) â€” Score: 0.2261\n",
            "22. Ten Benny (1997) â€” Score: 0.2261\n",
            "23. Stacy's Knights (1982) â€” Score: 0.2261\n",
            "24. Law, The (Le Legge) (1958) â€” Score: 0.2261\n",
            "25. Men Cry Bullets (1997) â€” Score: 0.2261\n",
            "26. Get Over It (1996) â€” Score: 0.2261\n",
            "27. Desert Winds (1995) â€” Score: 0.2261\n",
            "28. Invitation, The (Zaproszenie) (1986) â€” Score: 0.2261\n",
            "29. Costa Brava (1946) â€” Score: 0.2261\n",
            "30. Two or Three Things I Know About Her (1966) â€” Score: 0.2261\n",
            "31. Storefront Hitchcock (1997) â€” Score: 0.2261\n",
            "32. Empty Mirror, The (1999) â€” Score: 0.2261\n",
            "33. Children Are Watching us, The (Bambini ci guardano, I) (1942) â€” Score: 0.2261\n",
            "34. It Happened Here (1961) â€” Score: 0.2261\n",
            "35. It's My Party (1995) â€” Score: 0.2261\n",
            "36. Smiling Fish and Goat on Fire (1999) â€” Score: 0.2261\n",
            "37. Hurricane Streets (1998) â€” Score: 0.2261\n",
            "38. Children of Heaven, The (Bacheha-Ye Aseman) (1997) â€” Score: 0.2261\n",
            "39. 24-hour Woman (1998) â€” Score: 0.2261\n",
            "40. Prince of Central Park, The (1999) â€” Score: 0.2261\n",
            "41. Boy Called Hate, A (1995) â€” Score: 0.2261\n",
            "42. Diebinnen (1995) â€” Score: 0.2261\n",
            "43. Bread and Chocolate (Pane e cioccolata) (1973) â€” Score: 0.2261\n",
            "44. Uninvited Guest, An (2000) â€” Score: 0.2261\n",
            "45. Goya in Bordeaux (Goya en Bodeos) (1999) â€” Score: 0.2261\n",
            "46. Being Human (1993) â€” Score: 0.2261\n",
            "47. Lilian's Story (1995) â€” Score: 0.2261\n",
            "48. Somewhere in the City (1997) â€” Score: 0.2261\n",
            "49. Waking the Dead (1999) â€” Score: 0.2261\n",
            "50. East Palace West Palace (Dong gong xi gong) (1997) â€” Score: 0.2261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collaborative Filtering (UBCF, IBCF, SVD, ALS)**\n",
        "\n",
        "**Objective:**\n",
        "This notebook implements **Collaborative Filtering** techniques for personalized movie recommendation. Unlike Content-Based Filtering, Collaborative Filtering relies on patterns in *user behavior*â€”how users rate moviesâ€”and not on the content of the movies themselves.\n",
        "\n",
        "**Key Techniques Covered:**\n",
        "\n",
        "* **Memory-Based Filtering:**\n",
        "\n",
        "  * *User-User Collaborative Filtering (UBCF)*: Finds users with similar tastes.\n",
        "  * *Item-Item Collaborative Filtering (IBCF)*: Finds items similar to what a user has liked.\n",
        "  \n",
        "* **Model-Based Filtering:**\n",
        "\n",
        "  * *SVD (Surprise)*: Learns latent features from the rating matrix.\n",
        "  * *ALS (PySpark)*: Scalable factorization method for large datasets.\n",
        "\n",
        "**Core Tasks:**\n",
        "\n",
        "* Build user-item rating matrices\n",
        "* Compute cosine similarity for UBCF/IBCF\n",
        "* Tune and evaluate SVD and ALS models\n",
        "* Handle missing data and sparse matrix issues\n",
        "* Save predictions and evaluation scores for future use\n",
        "\n",
        "**Application:**\n",
        "These methods serve as the backbone for Netflix-style recommendation engines, helping tailor suggestions based on the tastes of millions of users.\n"
      ],
      "metadata": {
        "id": "R6d3SP0OolH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Day 3: Collaborative Filtering (UBCF)\n",
        "# ==============================\n",
        "# Purpose: Implement memory-based and model-based collaborative filtering approaches.\n",
        "# Application: Recommend movies by identifying similar users (User-User), similar items (Item-Item), or through latent factor models (SVD and ALS).\n",
        "\n",
        "# ==============================\n",
        "# Module 8: Memory-Based Collaborative Filtering\n",
        "# ==============================\n",
        "# Purpose: Compute User-User and Item-Item similarity.\n",
        "# Application: Memory-based collaborative filtering using cosine similarity.\n",
        "\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "\n",
        "# --- Create User-Item Rating Matrix ---\n",
        "def create_user_item_matrix(ratings):\n",
        "    user_item_matrix = ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "    return user_item_matrix.fillna(0)\n",
        "\n",
        "# --- Compute Similarity ---\n",
        "def compute_similarity(matrix, kind='user'):\n",
        "    if kind == 'user':\n",
        "        sim = 1 - pairwise_distances(matrix, metric='cosine')\n",
        "    elif kind == 'item':\n",
        "        sim = 1 - pairwise_distances(matrix.T, metric='cosine')\n",
        "    else:\n",
        "        raise ValueError(\"kind must be 'user' or 'item'\")\n",
        "    print(f\"{kind.title()}-based similarity computed.\")\n",
        "    return sim\n",
        "\n",
        "# ==============================\n",
        "# Module 9: Model-Based CF using Surprise (SVD)\n",
        "# ==============================\n",
        "# Purpose: Use matrix factorization to learn latent user and item features.\n",
        "# Application: Model-based collaborative filtering using SVD and tuning.\n",
        "\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split, GridSearchCV\n",
        "from surprise.accuracy import rmse\n",
        "\n",
        "# --- Prepare Surprise Dataset ---\n",
        "def prepare_surprise_data(ratings):\n",
        "    reader = Reader(rating_scale=(0.5, 5.0))\n",
        "    return Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# --- Train and Tune SVD Model ---\n",
        "def tune_svd_model(data):\n",
        "    param_grid = {'n_factors': [50, 100], 'lr_all': [0.005, 0.01], 'reg_all': [0.02, 0.1]}\n",
        "    gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
        "    gs.fit(data)\n",
        "    print(f\"Best RMSE: {gs.best_score['rmse']} with params: {gs.best_params['rmse']}\")\n",
        "    return gs.best_estimator['rmse']\n",
        "\n",
        "# --- Evaluate SVD ---\n",
        "def evaluate_svd(model, data):\n",
        "    trainset, testset = train_test_split(data, test_size=0.2)\n",
        "    model.fit(trainset)\n",
        "    predictions = model.test(testset)\n",
        "    score = rmse(predictions)\n",
        "    return predictions, score\n",
        "\n",
        "# ==============================\n",
        "# Module 10: Model-Based CF using PySpark ALS\n",
        "# ==============================\n",
        "# Purpose: Implement ALS model using PySpark for scalability.\n",
        "# Application: Large-scale collaborative filtering with alternating least squares.\n",
        "\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# --- Create Spark Session ---\n",
        "def start_spark():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"ALSModel\") \\\n",
        "        .getOrCreate()\n",
        "    return spark\n",
        "\n",
        "# --- Prepare DataFrame for ALS ---\n",
        "def prepare_als_data(spark, ratings):\n",
        "    return spark.createDataFrame(ratings[['userId', 'movieId', 'rating']])\n",
        "\n",
        "# --- Train ALS Model ---\n",
        "def train_als_model(data):\n",
        "    als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "              rank=10, maxIter=10, regParam=0.1, coldStartStrategy=\"drop\",\n",
        "              nonnegative=True)\n",
        "    model = als.fit(data)\n",
        "    return model\n",
        "\n",
        "# --- Evaluate ALS ---\n",
        "def evaluate_als(model, data):\n",
        "    predictions = model.transform(data)\n",
        "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                    predictionCol=\"prediction\")\n",
        "    rmse_val = evaluator.evaluate(predictions)\n",
        "    print(f\"ALS RMSE: {rmse_val}\")\n",
        "    return predictions, rmse_val\n",
        "\n",
        "# ==============================\n",
        "# Module 11: Save Predictions and Scores\n",
        "# ==============================\n",
        "# Purpose: Save output predictions and evaluation metrics.\n",
        "# Application: Reuse and visualize predictions for report or dashboard.\n",
        "\n",
        "def save_predictions(predictions, filename):\n",
        "    import pickle\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(predictions, f)\n",
        "    print(f\"Saved predictions to {filename}\")\n"
      ],
      "metadata": {
        "id": "JqElLBtOolyP",
        "outputId": "6cc0af7d-2cc7-4d54-c672-738e20fe37d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'surprise'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-3612997228.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Application: Model-based collaborative filtering using SVD and tuning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'surprise'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Module 8: Memory-Based Collaborative Filtering***\n",
        "\n"
      ],
      "metadata": {
        "id": "buItPj9HspOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 8: Memory-Based Collaborative Filtering\n",
        "# ==============================\n",
        "# Purpose: Compute similarity between users (User-User CF) or items (Item-Item CF) based on their ratings.\n",
        "# Application: This module enables collaborative filtering without training complex models by leveraging cosine similarity\n",
        "# on the user-item rating matrix. This is useful for smaller datasets and provides interpretable recommendations.\n",
        "\n",
        "\"\"\"\n",
        "This module builds the foundational logic for memory-based collaborative filtering.\n",
        "It creates a sparse user-item matrix from MovieLens ratings data and computes pairwise\n",
        "similarity between either users or items using cosine similarity.\n",
        "\n",
        "Use this for:\n",
        "- Generating recommendations by identifying similar users (UBCF)\n",
        "- Identifying related items (IBCF) based on user preferences\n",
        "- Quick prototyping and interpretable baseline models\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Step 1: Create User-Item Rating Matrix ---\n",
        "def create_user_item_matrix(ratings_df):\n",
        "    \"\"\"\n",
        "    Convert ratings DataFrame to a pivoted user-item matrix.\n",
        "    Missing ratings are filled with 0 (sparse assumption).\n",
        "    \"\"\"\n",
        "    user_item_matrix = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
        "    return user_item_matrix.fillna(0)\n",
        "\n",
        "# --- Step 2: Cosine Similarity Calculation (Manual) ---\n",
        "def cosine_similarity_manual(A):\n",
        "    \"\"\"\n",
        "    Compute pairwise cosine similarity between rows of matrix A using basic arithmetic.\n",
        "    A: numpy array of shape (n_samples, n_features)\n",
        "    Returns: cosine similarity matrix (n_samples x n_samples)\n",
        "    \"\"\"\n",
        "    dot_product = np.dot(A, A.T)\n",
        "    norm = np.linalg.norm(A, axis=1, keepdims=True)\n",
        "    denominator = np.dot(norm, norm.T)\n",
        "\n",
        "    # To avoid divide-by-zero, replace zeros in denominator with small epsilon\n",
        "    denominator[denominator == 0] = 1e-9\n",
        "    similarity = dot_product / denominator\n",
        "    return similarity\n",
        "\n",
        "# --- Step 3: Compute Similarity Matrix ---\n",
        "def compute_memory_based_similarity(user_item_matrix, kind='user'):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between users or items.\n",
        "    For users, applies user-level centering before computing similarity.\n",
        "    \"\"\"\n",
        "    if kind == 'user':\n",
        "        # --- Step A: Replace 0s with NaNs for mean calculation ---\n",
        "        centered = user_item_matrix.replace(0, np.nan)\n",
        "        user_means = centered.mean(axis=1)\n",
        "\n",
        "        # --- Step B: Center ratings around user means and fill NaNs with 0 ---\n",
        "        matrix = centered.sub(user_means, axis=0).fillna(0).values\n",
        "\n",
        "        sim = cosine_similarity_manual(matrix)\n",
        "        print(\"User-User similarity computed using cosine on centered ratings.\")\n",
        "\n",
        "    elif kind == 'item':\n",
        "        matrix = user_item_matrix.values.T\n",
        "        sim = cosine_similarity_manual(matrix)\n",
        "        print(\"Item-Item similarity computed using cosine.\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"kind must be 'user' or 'item'\")\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        sim,\n",
        "        index=user_item_matrix.index if kind == 'user' else user_item_matrix.columns,\n",
        "        columns=user_item_matrix.index if kind == 'user' else user_item_matrix.columns\n",
        "    )\n",
        "\n",
        "# --- Optional: Recommend Top-K Similar Users or Items ---\n",
        "def get_top_k_similar(sim_matrix, entity_id, k=5):\n",
        "    \"\"\"\n",
        "    Given a similarity matrix, return the top-k most similar users/items to the specified entity_id.\n",
        "    \"\"\"\n",
        "    sim_scores = sim_matrix.loc[entity_id].drop(entity_id)  # exclude self\n",
        "    return sim_scores.sort_values(ascending=False).head(k)\n",
        "\n",
        "# --- Usage ---\n",
        "ubcf_user_item = create_user_item_matrix(ratings)\n",
        "ubcf_user_sim = compute_memory_based_similarity(ubcf_user_item, kind='user')\n",
        "ubcf_top_k_users = get_top_k_similar(ubcf_user_sim, entity_id=1, k=50)\n",
        "\n"
      ],
      "metadata": {
        "id": "c8thtxWYs6Q8",
        "outputId": "d4b0fa56-d4d5-49c3-af08-3563adc14bf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-User similarity computed using cosine on centered ratings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Top-50 recommendations using User-Based Collaborative Filtering (UBCF) for user_id = 5549.*"
      ],
      "metadata": {
        "id": "mOyUwNQ6yrI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module X: Predict UBCF Scores with Movie Titles\n",
        "# ==============================\n",
        "# Purpose: Generate UBCF-based predicted ratings for unseen movies, with optional display of movie titles.\n",
        "# Application: Produces top-N personalized movie recommendations using cosine similarity of user-user interactions.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Load movie titles mapping ---\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")  # Adjust path if needed\n",
        "movie_id_to_title = dict(zip(movies['movieId'], movies['title']))\n",
        "\n",
        "# --- UBCF Prediction Function with Movie Titles ---\n",
        "def predict_ubcf_scores(user_id, user_item_matrix, similarity_matrix, movie_id_to_title=None):\n",
        "    \"\"\"\n",
        "    Predict scores for unseen movies for a given user using UBCF (cosine similarity).\n",
        "    Temporarily removes the user to compute weighted average, then restores them.\n",
        "\n",
        "    Parameters:\n",
        "    - user_id: int, target user\n",
        "    - user_item_matrix: DataFrame, user-item rating matrix\n",
        "    - similarity_matrix: DataFrame, cosine similarity between users\n",
        "    - movie_id_to_title: dict (optional), maps movieId to title for display\n",
        "\n",
        "    Returns:\n",
        "    - Series of predicted scores for unseen movies, optionally with movie titles as index\n",
        "    \"\"\"\n",
        "    # Backup user data\n",
        "    user_vector = user_item_matrix.loc[user_id]\n",
        "    sim_vector = similarity_matrix.loc[user_id]\n",
        "\n",
        "    # Drop user to avoid self-similarity\n",
        "    user_item_matrix_dropped = user_item_matrix.drop(index=user_id)\n",
        "    sim_scores_dropped = sim_vector.drop(index=user_id)\n",
        "\n",
        "    # Align index\n",
        "    sim_scores_dropped = sim_scores_dropped.loc[user_item_matrix_dropped.index]\n",
        "\n",
        "    # Mask unrated items\n",
        "    unrated_mask = user_vector == 0\n",
        "    neighbor_ratings = user_item_matrix_dropped\n",
        "\n",
        "    # Compute weighted average prediction\n",
        "    numerator = np.dot(sim_scores_dropped.values, neighbor_ratings.values)\n",
        "    denominator = np.abs(sim_scores_dropped.values).sum()\n",
        "    denominator = denominator if denominator != 0 else 1e-9\n",
        "    predicted_scores = numerator / denominator\n",
        "\n",
        "    # Filter for unrated movies only\n",
        "    pred_series = pd.Series(predicted_scores, index=user_item_matrix.columns)\n",
        "    unseen_preds = pred_series[unrated_mask].sort_values(ascending=False)\n",
        "\n",
        "    # Reinsert user data\n",
        "    user_item_matrix.loc[user_id] = user_vector\n",
        "    similarity_matrix.loc[user_id] = sim_vector\n",
        "    similarity_matrix[user_id] = sim_vector\n",
        "\n",
        "    # Map movieId to title if available\n",
        "    if movie_id_to_title:\n",
        "        unseen_preds.index = [movie_id_to_title.get(mid, f\"MovieID {mid}\") for mid in unseen_preds.index]\n",
        "\n",
        "    return unseen_preds\n",
        "\n",
        "# --- Generate Top 50 Recommendations ---\n",
        "ubcf_top_50_recommendations = predict_ubcf_scores(\n",
        "    user_id=5549,\n",
        "    user_item_matrix=ubcf_user_item,\n",
        "    similarity_matrix=ubcf_user_sim,\n",
        "    movie_id_to_title=movie_id_to_title\n",
        ").head(50)\n",
        "\n",
        "# --- Display Recommendations ---\n",
        "# --- Convert to DataFrame for Display ---\n",
        "ubcf_recommendation_df = pd.DataFrame({\n",
        "    \"Rank\": range(1, len(ubcf_top_50_recommendations) + 1),\n",
        "    \"Movie Title\": ubcf_top_50_recommendations.index,\n",
        "    \"Predicted Rating\": ubcf_top_50_recommendations.values\n",
        "})\n",
        "\n",
        "# --- Display DataFrame ---\n",
        "print(\"\\nTop 50 UBCF Recommendations for User 5549:\\n\")\n",
        "print(ubcf_recommendation_df.to_string(index=False))\n",
        "\n"
      ],
      "metadata": {
        "id": "u4DFww01yyc-",
        "outputId": "e56d144d-9ba8-486e-c0c8-bf52da6090cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 50 UBCF Recommendations for User 5549:\n",
            "\n",
            " Rank                               Movie Title  Predicted Rating\n",
            "    1            Godfather: Part II, The (1974)          0.433388\n",
            "    2 Butch Cassidy and the Sundance Kid (1969)          0.326463\n",
            "    3                           Die Hard (1988)          0.306014\n",
            "    4          Hunt for Red October, The (1990)          0.301117\n",
            "    5                      Fugitive, The (1993)          0.299904\n",
            "    6                  Wizard of Oz, The (1939)          0.280176\n",
            "    7 Indiana Jones and the Last Crusade (1989)          0.276143\n",
            "    8                              Rocky (1976)          0.275978\n",
            "    9                         Casablanca (1942)          0.259601\n",
            "   10                 Dances with Wolves (1990)          0.259482\n",
            "   11 Star Wars: Episode IV - A New Hope (1977)          0.255633\n",
            "   12                      Lethal Weapon (1987)          0.247489\n",
            "   13                     Cool Hand Luke (1967)          0.243840\n",
            "   14                            Witness (1985)          0.239308\n",
            "   15                            M*A*S*H (1970)          0.235673\n",
            "   16                   Schindler's List (1993)          0.232587\n",
            "   17                 African Queen, The (1951)          0.229049\n",
            "   18            When Harry Met Sally... (1989)          0.226399\n",
            "   19                 Gone with the Wind (1939)          0.225696\n",
            "   20                  Great Escape, The (1963)          0.223807\n",
            "   21              To Kill a Mockingbird (1962)          0.220013\n",
            "   22                      Graduate, The (1967)          0.218267\n",
            "   23                        Rear Window (1954)          0.216368\n",
            "   24                       Animal House (1978)          0.216044\n",
            "   25                       Forrest Gump (1994)          0.215814\n",
            "   26      Bridge on the River Kwai, The (1957)          0.213138\n",
            "   27                       Citizen Kane (1941)          0.207258\n",
            "   28                            Ben-Hur (1959)          0.206503\n",
            "   29                               Jaws (1975)          0.205872\n",
            "   30                Saving Private Ryan (1998)          0.205390\n",
            "   31             French Connection, The (1971)          0.204628\n",
            "   32                            Amadeus (1984)          0.203189\n",
            "   33                              Glory (1989)          0.198074\n",
            "   34                 Lawrence of Arabia (1962)          0.197211\n",
            "   35                             Patton (1970)          0.197044\n",
            "   36              It's a Wonderful Life (1946)          0.196865\n",
            "   37                  American Graffiti (1973)          0.196802\n",
            "   38                Romancing the Stone (1984)          0.196148\n",
            "   39                   Right Stuff, The (1983)          0.195780\n",
            "   40          Silence of the Lambs, The (1991)          0.195017\n",
            "   41                         Goldfinger (1964)          0.193414\n",
            "   42                   Dirty Dozen, The (1967)          0.192227\n",
            "   43          Shawshank Redemption, The (1994)          0.191776\n",
            "   44                    Few Good Men, A (1992)          0.187533\n",
            "   45                     Apocalypse Now (1979)          0.181295\n",
            "   46                              Speed (1994)          0.178500\n",
            "   47                          Apollo 13 (1995)          0.173583\n",
            "   48                      Patriot Games (1992)          0.173149\n",
            "   49                       My Fair Lady (1964)          0.172712\n",
            "   50           Clear and Present Danger (1994)          0.172539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Module 9: Model-Based Collaborative Filtering using Surprise (SVD)**"
      ],
      "metadata": {
        "id": "fx72fXU_s8nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 9: Model-Based CF using Surprise (SVD)\n",
        "# ==============================\n",
        "# Purpose: Apply matrix factorization using Singular Value Decomposition (SVD) to uncover latent features in user-item interactions.\n",
        "# Application: This module creates a powerful and compact user-item interaction model, ideal for mid-sized datasets and tuning.\n",
        "\n",
        "\"\"\"\n",
        "This module uses the Surprise library to implement and tune SVD models. It converts user ratings\n",
        "into latent feature space representations, capturing hidden patterns in user preferences and item attributes.\n",
        "\n",
        "Use this for:\n",
        "- Highly personalized recommendations\n",
        "- Compact representation of large rating matrices\n",
        "- Performing hyperparameter tuning for optimal performance (e.g., via GridSearchCV)\n",
        "- Getting performance insights via RMSE evaluation\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "wa_wcw8itGw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Step 4: Convert to Sparse Format***"
      ],
      "metadata": {
        "id": "YwA4UWDjJH7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# --- Step 1: Replace 0s with NaN for centering ---\n",
        "matrix_centered = ubcf_user_item.replace(0, np.nan)\n",
        "user_means = matrix_centered.mean(axis=1)\n",
        "\n",
        "# --- Step 2: Subtract user mean to center the ratings ---\n",
        "matrix_centered = matrix_centered.sub(user_means, axis=0).fillna(0)\n",
        "\n",
        "# --- Step 3: Convert to sparse matrix format for SVD ---\n",
        "sparse_matrix = csr_matrix(matrix_centered.values)\n",
        "\n",
        "# --- Step 4: Output shape ---\n",
        "print(f\"Sparse Matrix Shape: {sparse_matrix.shape}\")\n"
      ],
      "metadata": {
        "id": "5YKRhDQ3JAda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Function: Split Sparse Matrix into Train/Test ---\n",
        "def train_test_split_sparse(matrix, test_ratio=0.2, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    train = matrix.copy().toarray()\n",
        "    test = np.zeros_like(train)\n",
        "\n",
        "    for user in range(matrix.shape[0]):\n",
        "        rated_items = matrix[user].nonzero()[1]\n",
        "        if len(rated_items) > 1:\n",
        "            test_size = max(1, int(len(rated_items) * test_ratio))\n",
        "            test_items = np.random.choice(rated_items, size=test_size, replace=False)\n",
        "            train[user, test_items] = 0.0\n",
        "            test[user, test_items] = matrix[user, test_items].toarray()\n",
        "\n",
        "    return csr_matrix(train), csr_matrix(test)\n",
        "\n",
        "# --- Function: Compute RMSE ---\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def compute_rmse(pred_matrix, test_matrix):\n",
        "    test_nonzero = test_matrix.nonzero()\n",
        "\n",
        "    pred = pred_matrix[test_nonzero].flatten()\n",
        "    actual = np.array(test_matrix[test_nonzero]).flatten()\n",
        "\n",
        "    # print(f\"Shape of pred: {pred.shape}\")\n",
        "    # print(f\"Shape of actual: {actual.shape}\")\n",
        "\n",
        "    return np.sqrt(mean_squared_error(actual, pred))\n",
        "\n",
        "\n",
        "# --- Step 1: Split Centered Sparse Matrix ---\n",
        "train_matrix, test_matrix = train_test_split_sparse(sparse_matrix)\n",
        "\n",
        "# --- Step 2: Evaluate SVD across k-values ---\n",
        "# Generate k_values from 10 to 300\n",
        "k_values = list(range(10, 101, 10)) + list(range(120, 501, 20))\n",
        "\n",
        "rmse_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    U, sigma, Vt = svds(train_matrix, k=k)\n",
        "    sigma_diag = np.diag(sigma)\n",
        "    pred_matrix = np.dot(np.dot(U, sigma_diag), Vt)\n",
        "\n",
        "    # Restore user means (aligned by row order, not userId)\n",
        "    pred_matrix += user_means.values[:, np.newaxis]\n",
        "\n",
        "    # Evaluate RMSE on test set\n",
        "    rmse = compute_rmse(pred_matrix, test_matrix)\n",
        "    rmse_scores.append(rmse)\n",
        "    # print(f\"k = {k}, RMSE = {rmse:.4f}\")\n",
        "\n",
        "# --- Step 3: Plot ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, rmse_scores, marker='o')\n",
        "plt.title(\"SVD: Number of Latent Factors vs. RMSE\")\n",
        "plt.xlabel(\"Number of Latent Factors (k)\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "abr4iW4oKd4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def train_test_split_sparse(matrix, test_ratio=0.2, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    train = matrix.copy().toarray()\n",
        "    test = np.zeros_like(train)\n",
        "\n",
        "    for user in range(matrix.shape[0]):\n",
        "        rated_items = matrix[user].nonzero()[1]\n",
        "        if len(rated_items) > 1:\n",
        "            test_size = max(1, int(len(rated_items) * test_ratio))\n",
        "            test_items = np.random.choice(rated_items, size=test_size, replace=False)\n",
        "            train[user, test_items] = 0.0\n",
        "            test[user, test_items] = matrix[user, test_items].toarray()\n",
        "\n",
        "    return csr_matrix(train), csr_matrix(test)\n",
        "\n",
        "def compute_rmse(pred_matrix, test_matrix):\n",
        "    test_nonzero = test_matrix.nonzero()\n",
        "    pred = pred_matrix[test_nonzero].flatten()\n",
        "    actual = np.array(test_matrix[test_nonzero]).flatten()\n",
        "    return np.sqrt(mean_squared_error(actual, pred))\n",
        "\n",
        "# --- Assume you already have: sparse_matrix, user_means ---\n",
        "train_matrix, test_matrix = train_test_split_sparse(sparse_matrix)\n",
        "\n",
        "# --- k values ---\n",
        "k_values = list(range(10, 101, 10)) + list(range(120, 301, 20))\n",
        "results = []\n",
        "\n",
        "for k in k_values:\n",
        "    U, sigma, Vt = svds(train_matrix, k=k)\n",
        "    sigma_diag = np.diag(sigma)\n",
        "    pred_matrix = np.dot(np.dot(U, sigma_diag), Vt)\n",
        "    pred_matrix += user_means.values[:, np.newaxis]\n",
        "\n",
        "    rmse = compute_rmse(pred_matrix, test_matrix)\n",
        "    results.append((k, rmse, sigma))\n",
        "    # print(f\"k = {k}, RMSE = {rmse:.4f}\")\n",
        "\n",
        "# --- Extract Singular Values from Highest-k SVD ---\n",
        "sigma_full = []\n",
        "for k, rmse, sigma_list in sorted(results, key=lambda x: x[0], reverse=True):\n",
        "    if len(sigma_list) > 0:\n",
        "        sigma_full = sorted(sigma_list, reverse=True)\n",
        "        break\n",
        "\n",
        "# --- Cumulative Energy ---\n",
        "sigma_full = np.array(sigma_full)\n",
        "total_energy = np.sum(sigma_full)\n",
        "explained_variance_ratio = sigma_full / total_energy\n",
        "cumulative_energy = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# --- DataFrame for Plotting ---\n",
        "energy_df = pd.DataFrame({\n",
        "    'k': np.arange(1, len(sigma_full) + 1),\n",
        "    'SingularValue': sigma_full,\n",
        "    'ExplainedVarianceRatio': explained_variance_ratio,\n",
        "    'CumulativeEnergy': cumulative_energy\n",
        "})\n",
        "\n",
        "# --- Plot ---\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.lineplot(data=energy_df, x='k', y='CumulativeEnergy', marker='o')\n",
        "plt.title(\"Cumulative Energy vs. Number of Latent Dimensions\", loc='left')\n",
        "plt.xlabel(\"Number of Latent Dimensions (k)\")\n",
        "plt.ylabel(\"Cumulative Energy (Explained Variance)\")\n",
        "plt.axhline(y=0.9, color='red', linestyle='--', label='90% Variance')\n",
        "plt.axhline(y=0.95, color='green', linestyle='--', label='95% Variance')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v7XiDRoETPGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rmse(pred_matrix, test_matrix):\n",
        "    # Get test matrix nonzero indices\n",
        "    test_nonzero = test_matrix.nonzero()\n",
        "\n",
        "    # Extract predicted and actual ratings\n",
        "    pred = pred_matrix[test_nonzero]\n",
        "    actual = test_matrix[test_nonzero]\n",
        "\n",
        "    # Print debug info\n",
        "    print(f\"Shape of pred: {pred.shape}\")\n",
        "    print(f\"Shape of actual: {actual.shape}\")\n",
        "    print(f\"First 5 pred values: {pred[:5]}\")\n",
        "    print(f\"First 5 actual values: {actual[:5]}\")\n",
        "\n",
        "    # Check for consistent length\n",
        "    if pred.shape != actual.shape:\n",
        "        raise ValueError(f\"Shape mismatch: pred {pred.shape}, actual {actual.shape}\")\n",
        "\n",
        "    # Compute RMSE\n",
        "    return np.sqrt(mean_squared_error(actual, pred))\n"
      ],
      "metadata": {
        "id": "64ZQqsOzPt33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Module 10: Model-Based Collaborative Filtering using PySpark ALS***"
      ],
      "metadata": {
        "id": "fQKkn0A2tMZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 10: Model-Based CF using PySpark ALS\n",
        "# ==============================\n",
        "# Purpose: Implement Alternating Least Squares (ALS) for collaborative filtering in distributed environments using PySpark.\n",
        "# Application: Designed for scalability, this module is best suited for handling very large datasets with missing values and\n",
        "# sparse interactions.\n",
        "\n",
        "\"\"\"\n",
        "This module implements ALS using PySpark's MLlib, making it suitable for distributed computing.\n",
        "ALS alternates between fixing user or item factors and solving least-squares problems, optimizing for\n",
        "the best factorization of the user-item rating matrix.\n",
        "\n",
        "Use this for:\n",
        "- Training collaborative filtering models on large-scale datasets\n",
        "- Handling cold-start issues using 'drop' strategy\n",
        "- Running evaluations on distributed clusters using RMSE\n",
        "- Integration with large production systems requiring scalable solutions\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "FZdQjLjFtSap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Module 11: Save Predictions and Scores***"
      ],
      "metadata": {
        "id": "yQVm3YKNtVmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 11: Save Predictions and Scores\n",
        "# ==============================\n",
        "# Purpose: Persist model prediction outputs and evaluation metrics for downstream use.\n",
        "# Application: Enables reuse of trained models and their predictions across systems, visualizations, or report pipelines.\n",
        "\n",
        "\"\"\"\n",
        "This utility module provides a function to save predictions or evaluation results to disk\n",
        "using Pythonâ€™s pickle serialization. It ensures you can reuse prediction results without rerunning\n",
        "expensive model training or inference steps.\n",
        "\n",
        "Use this for:\n",
        "- Storing model outputs for dashboards or final reporting\n",
        "- Supporting reproducibility and offline analysis\n",
        "- Sharing results with stakeholders or across systems\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "KiXm9hwwtcTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}