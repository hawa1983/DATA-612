{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-612/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose**\n",
        "\n",
        "The purpose of this script is to enrich the MovieLens movie dataset (`movies.dat`) with detailed movie metadata from The Movie Database (TMDB) API. This metadata includes movie overviews, genres, poster and backdrop image URLs, cast and director information, keywords, user ratings, and trailer links. The enriched dataset will serve as the foundation for building content-based, collaborative, and hybrid recommender systems.\n",
        "\n",
        "### **Methodology**\n",
        "\n",
        "1. **Load MovieLens Movie Data**\n",
        "   The script loads the `movies.dat` file, which contains basic movie information including `movieId`, `title`, and `genres`.\n",
        "\n",
        "2. **Clean Titles and Extract Years**\n",
        "   It processes the movie titles to remove the year from the title string and separately extracts the release year to improve search accuracy when querying TMDB.\n",
        "\n",
        "3. **Query TMDB API**\n",
        "   For each movie, it sends a search request to TMDB using the cleaned title and release year. If a match is found, it retrieves the movie’s TMDB ID.\n",
        "\n",
        "4. **Retrieve Detailed Metadata**\n",
        "   Using the TMDB ID, the script fetches:\n",
        "\n",
        "   * Overview (plot summary)\n",
        "   * Poster and backdrop image paths\n",
        "   * Genre IDs, which are then mapped to readable genre names\n",
        "   * Top 3 cast members\n",
        "   * Director(s)\n",
        "   * Associated keywords\n",
        "   * YouTube trailer link (if available)\n",
        "\n",
        "5. **Construct and Save Enriched Dataset**\n",
        "   All metadata is compiled into a structured format and merged with the original MovieLens data. The final dataset is saved as `movies_enriched_full.csv` for downstream use in recommendation models.\n"
      ],
      "metadata": {
        "id": "OptAJ12iBc8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "7qJ0maYGaxtJ",
        "outputId": "2cab814c-7502-488a-f415-fd6922272d72"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'movies.dat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-494969615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Load movies.dat - format: MovieID::Title::Genres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmovies_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"movies.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"genres\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# ---------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movies.dat'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ---------------------------------------\n",
        "# CONFIG\n",
        "# ---------------------------------------\n",
        "BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "IMAGE_BASE = \"https://image.tmdb.org/t/p/w500\"\n",
        "\n",
        "# Use your TMDB Bearer Token (v4)\n",
        "HEADERS = {\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyZGZlNjMwMGMzYjIzMjc2NzExNjQ0N2JhNzhiMjM5MyIsIm5iZiI6MTc1MTkyMjA3Ni4xMzUsInN1YiI6IjY4NmMzNTljMzc4NjllOGEyNDUxZTM0OSIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.S773ddH3FiIHtokPW4sYpJog0mXWS1o4OPov1KZneUw\"\n",
        "}\n",
        "\n",
        "# TMDB genre ID to name mapping\n",
        "GENRE_ID_TO_NAME = {\n",
        "    28: \"Action\", 12: \"Adventure\", 16: \"Animation\", 35: \"Comedy\", 80: \"Crime\",\n",
        "    99: \"Documentary\", 18: \"Drama\", 10751: \"Family\", 14: \"Fantasy\", 36: \"History\",\n",
        "    27: \"Horror\", 10402: \"Music\", 9648: \"Mystery\", 10749: \"Romance\", 878: \"Science Fiction\",\n",
        "    10770: \"TV Movie\", 53: \"Thriller\", 10752: \"War\", 37: \"Western\"\n",
        "}\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 1: Load MovieLens .dat Files\n",
        "# ---------------------------------------\n",
        "\n",
        "# Load movies.dat - format: MovieID::Title::Genres\n",
        "movies_df = pd.read_csv(\"movies.dat\", sep=\"::\", engine='python', header=None, names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 2: Clean Movie Titles and Extract Year\n",
        "# ---------------------------------------\n",
        "\n",
        "def extract_year(title):\n",
        "    if \"(\" in title:\n",
        "        try:\n",
        "            return int(title.strip()[-5:-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_title(title):\n",
        "    if \"(\" in title:\n",
        "        return title[:title.rfind(\"(\")].strip()\n",
        "    return title.strip()\n",
        "\n",
        "movies_df[\"year\"] = movies_df[\"title\"].apply(extract_year)\n",
        "movies_df[\"clean_title\"] = movies_df[\"title\"].apply(clean_title)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 3: TMDB Metadata Functions\n",
        "# ---------------------------------------\n",
        "\n",
        "# Search for movie in TMDB\n",
        "def search_tmdb(title, year):\n",
        "    url = f\"{BASE_URL}/search/movie\"\n",
        "    params = {\"query\": title, \"year\": year}\n",
        "    response = requests.get(url, headers=HEADERS, params=params)\n",
        "    r = response.json()\n",
        "    if r.get(\"results\"):\n",
        "        return r[\"results\"][0]\n",
        "    return None\n",
        "\n",
        "# Get full metadata from TMDB\n",
        "def get_full_tmdb_metadata(tmdb_id):\n",
        "    metadata = {}\n",
        "\n",
        "    # Credits (cast, crew)\n",
        "    credits = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/credits\", headers=HEADERS).json()\n",
        "    cast = [c[\"name\"] for c in credits.get(\"cast\", [])[:3]]\n",
        "    directors = [c[\"name\"] for c in credits.get(\"crew\", []) if c.get(\"job\") == \"Director\"]\n",
        "\n",
        "    # Keywords\n",
        "    keywords = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/keywords\", headers=HEADERS).json()\n",
        "    keyword_list = [k[\"name\"] for k in keywords.get(\"keywords\", [])]\n",
        "\n",
        "    # Videos (trailers)\n",
        "    videos = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/videos\", headers=HEADERS).json()\n",
        "    trailer_links = [\n",
        "        f\"https://www.youtube.com/watch?v={v['key']}\"\n",
        "        for v in videos.get(\"results\", [])\n",
        "        if v[\"site\"] == \"YouTube\" and v[\"type\"] == \"Trailer\"\n",
        "    ]\n",
        "\n",
        "    # Final metadata dictionary\n",
        "    metadata[\"top_3_cast\"] = \", \".join(cast)\n",
        "    metadata[\"directors\"] = \", \".join(directors)\n",
        "    metadata[\"keywords\"] = \", \".join(keyword_list)\n",
        "    metadata[\"trailer_link\"] = trailer_links[0] if trailer_links else None\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 4: Enrich Movie Data\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched = []\n",
        "\n",
        "for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
        "    movie_data = search_tmdb(row[\"clean_title\"], row[\"year\"])\n",
        "\n",
        "    if movie_data:\n",
        "        tmdb_id = movie_data[\"id\"]\n",
        "        extra = get_full_tmdb_metadata(tmdb_id)\n",
        "\n",
        "        genre_ids = movie_data.get(\"genre_ids\", [])\n",
        "        genre_names = [GENRE_ID_TO_NAME.get(gid, str(gid)) for gid in genre_ids]\n",
        "\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": tmdb_id,\n",
        "            \"overview\": movie_data.get(\"overview\", \"\"),\n",
        "            \"poster_path\": IMAGE_BASE + movie_data.get(\"poster_path\", \"\") if movie_data.get(\"poster_path\") else None,\n",
        "            \"backdrop_path\": IMAGE_BASE + movie_data.get(\"backdrop_path\", \"\") if movie_data.get(\"backdrop_path\") else None,\n",
        "            \"vote_average\": movie_data.get(\"vote_average\", None),\n",
        "            \"vote_count\": movie_data.get(\"vote_count\", None),\n",
        "            \"tmdb_genres\": \", \".join(genre_names),\n",
        "            **extra\n",
        "        })\n",
        "    else:\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": None,\n",
        "            \"overview\": None,\n",
        "            \"poster_path\": None,\n",
        "            \"backdrop_path\": None,\n",
        "            \"vote_average\": None,\n",
        "            \"vote_count\": None,\n",
        "            \"tmdb_genres\": None,\n",
        "            \"top_3_cast\": None,\n",
        "            \"directors\": None,\n",
        "            \"keywords\": None,\n",
        "            \"trailer_link\": None\n",
        "        })\n",
        "\n",
        "    time.sleep(0.25)  # Respect TMDB API rate limits\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 5: Save Final Dataset\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched)\n",
        "final_df = pd.concat([movies_df, enriched_df], axis=1)\n",
        "final_df.to_csv(\"movies_enriched_full.csv\", index=False)\n",
        "\n",
        "print(\"DONE: Saved as 'movies_enriched_full.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Personalized Content-Based Movie Recommendation System**\n",
        "\n",
        "This Python script implements a **Content-Based Filtering (CBF)** system enhanced with **personalized recommendations** using user-specific rating profiles. Built using the MovieLens 1M dataset and enriched metadata, the pipeline performs vectorization, similarity computation, and profile-based predictions.\n",
        "\n",
        "**What This Script Does**\n",
        "\n",
        "* **Module 1–2**: Load essential libraries and enriched movie data.\n",
        "* **Module 3**: Load user ratings and demographics.\n",
        "* **Module 4**: Engineer features combining genres, cast, crew, keywords, and movie overviews.\n",
        "* **Module 5**: Transform content into TF-IDF, Count, or Binary vectors, and compute pairwise similarities using Cosine or Jaccard metrics.\n",
        "* **Module 6**: Construct a weighted content profile per user based on past ratings.\n",
        "* **Module 7**: Recommend top-N movies similar to the user profile, excluding already seen titles.\n",
        "\n",
        "**Techniques Used**\n",
        "\n",
        "* **Text Vectorization**: TF-IDF, CountVectorizer, Binary Count\n",
        "* **Similarity Metrics**: Cosine Similarity, Jaccard Similarity\n",
        "* **Personalization**: Weighted vector averaging based on each user’s rated items\n",
        "* **Parallelization**: Speeds up Jaccard similarity computation using joblib\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "* Personalized recommendations for new users with a few ratings (cold-start)\n",
        "* Improving diversity and relevance in suggested movies\n",
        "* Generating fallback content suggestions in hybrid recommender systems"
      ],
      "metadata": {
        "id": "c7KNZkOThu7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9GeB2GWMyfPL",
        "outputId": "1a83295e-0732-481d-fb15-1ad9119fe0dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Collecting scikit-surprise==1.1.4 (from -r requirements.txt (line 3))\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==1.4.2 (from -r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting matplotlib==3.8.4 (from -r requirements.txt (line 5))\n",
            "  Downloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
            "Collecting tqdm==4.66.4 (from -r requirements.txt (line 7))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise==1.1.4->-r requirements.txt (line 3)) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 2)) (1.17.0)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2469536 sha256=3ce2477b290291177985412993b120fa885c4e9d44675ed99612d24ea751f6b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: tqdm, numpy, scikit-surprise, scikit-learn, matplotlib\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.2 requires tqdm>=4.67, but you have tqdm 4.66.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.8.4 numpy-1.26.4 scikit-learn-1.4.2 scikit-surprise-1.1.4 tqdm-4.66.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "sklearn",
                  "tqdm"
                ]
              },
              "id": "8f732c94594b4b3db6d4de3ebe5ba802"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 1: Imports & Configuration\n",
        "# ==============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# ==============================\n",
        "# Module 2: Load Movie Data\n",
        "# ==============================\n",
        "def load_movie_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    print(f\"Loaded {len(df)} movies.\")\n",
        "    return df\n",
        "\n",
        "# ==============================\n",
        "# Module 3: Load User Ratings and Demographics\n",
        "# ==============================\n",
        "def load_user_data(ratings_path, users_path):\n",
        "    ratings = pd.read_csv(ratings_path, sep=\"::\", engine=\"python\",\n",
        "                          names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "    users = pd.read_csv(users_path, sep=\"::\", engine=\"python\",\n",
        "                        names=[\"userId\", \"gender\", \"age\", \"occupation\", \"zip\"])\n",
        "    print(f\"Loaded {len(ratings)} ratings and {len(users)} users.\")\n",
        "    return ratings, users\n",
        "\n",
        "# ==============================\n",
        "# Module 4: Feature Engineering\n",
        "# ==============================\n",
        "def create_feature_string(df):\n",
        "    def split_and_clean(col, delimiter='|'):\n",
        "        return col.fillna('').str.replace(r'\\s+', '', regex=True).str.split(delimiter)\n",
        "\n",
        "    genre_list_1 = split_and_clean(df['genres'], delimiter='|')\n",
        "    genre_list_2 = split_and_clean(df['tmdb_genres'], delimiter=',')\n",
        "    merged_genres = [\n",
        "        ' '.join(sorted(set(g1 or []) | set(g2 or [])))\n",
        "        for g1, g2 in zip(genre_list_1, genre_list_2)\n",
        "    ]\n",
        "\n",
        "    def clean_text(col):\n",
        "        return col.fillna('').str.replace(r'\\s+', '', regex=True).str.replace(',', ' ')\n",
        "\n",
        "    overview_clean = df['overview'].fillna('').str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
        "    year_str = df['year'].astype(str).fillna('')\n",
        "\n",
        "    df['cbf_features'] = (\n",
        "        pd.Series(merged_genres) + ' ' +\n",
        "        clean_text(df['keywords']) + ' ' +\n",
        "        clean_text(df['top_3_cast']) + ' ' +\n",
        "        clean_text(df['directors']) + ' ' +\n",
        "        overview_clean + ' ' +\n",
        "        year_str\n",
        "    )\n",
        "\n",
        "    return df[['movieId', 'title', 'cbf_features']]\n",
        "\n",
        "# ==============================\n",
        "# Module 5: Vectorization & Similarity\n",
        "# ==============================\n",
        "def vectorize_features(text_series, method='tfidf'):\n",
        "    if method == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    elif method == 'count':\n",
        "        vectorizer = CountVectorizer(stop_words='english')\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'tfidf' or 'count'\")\n",
        "    matrix = vectorizer.fit_transform(text_series)\n",
        "    print(f\"{method.upper()} vectorization complete. Shape: {matrix.shape}\")\n",
        "    return matrix, vectorizer\n",
        "\n",
        "def binary_vectorize(text_series):\n",
        "    vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
        "    matrix = vectorizer.fit_transform(text_series)\n",
        "    print(f\"Binary Count vectorization complete. Shape: {matrix.shape}\")\n",
        "    return matrix.toarray(), vectorizer\n",
        "\n",
        "def compute_cosine_similarity(matrix):\n",
        "    sim = cosine_similarity(matrix)\n",
        "    print(\"Cosine similarity computed.\")\n",
        "    return sim\n",
        "\n",
        "def jaccard_pairwise_parallel(matrix):\n",
        "    n = matrix.shape[0]\n",
        "    sim_matrix = np.zeros((n, n))\n",
        "\n",
        "    def jaccard_row(i):\n",
        "        a = matrix[i]\n",
        "        row_sim = np.zeros(n)\n",
        "        for j in range(i, n):\n",
        "            b = matrix[j]\n",
        "            intersection = np.logical_and(a, b).sum()\n",
        "            union = np.logical_or(a, b).sum()\n",
        "            score = intersection / union if union > 0 else 0.0\n",
        "            row_sim[j] = score\n",
        "        return i, row_sim\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(\n",
        "        delayed(jaccard_row)(i) for i in tqdm(range(n), desc=\"Jaccard Similarity\")\n",
        "    )\n",
        "\n",
        "    for i, row in results:\n",
        "        sim_matrix[i, i:] = row[i:]\n",
        "        sim_matrix[i:, i] = row[i:]\n",
        "\n",
        "    print(\"Jaccard similarity matrix built.\")\n",
        "    return sim_matrix\n",
        "\n",
        "def jaccard_pairwise_parallel(A):\n",
        "    A = A.astype(bool).astype(int)  # Ensure binary\n",
        "    intersection = A @ A.T\n",
        "    row_sums = A.sum(axis=1).A1  # Convert to 1D array\n",
        "    union = row_sums[:, None] + row_sums[None, :] - intersection\n",
        "    jaccard = intersection / np.maximum(union, 1e-10)  # Prevent divide by zero\n",
        "    return jaccard\n",
        "\n",
        "def save_matrix(matrix, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(matrix, f)\n",
        "    print(f\"Saved similarity matrix to: {filename}\")\n",
        "\n",
        "# ==============================\n",
        "# Module 6: Build User Profile\n",
        "# ==============================\n",
        "def build_user_profile(user_id, ratings, tfidf_matrix, movie_df):\n",
        "    user_ratings = ratings[ratings['userId'] == user_id]\n",
        "    rated_movies = movie_df[movie_df['movieId'].isin(user_ratings['movieId'])]\n",
        "    indices = rated_movies.index.tolist()\n",
        "    weights = user_ratings.set_index('movieId').loc[rated_movies['movieId']]['rating'].values\n",
        "    profile = np.average(tfidf_matrix[indices].toarray(), axis=0, weights=weights)\n",
        "    return profile.reshape(1, -1)\n",
        "\n",
        "# ==============================\n",
        "# Module 7: Personalized Recommendation\n",
        "# ==============================\n",
        "def recommend_movies(user_id, ratings, tfidf_matrix, movie_df, top_n=50):\n",
        "    user_profile = build_user_profile(user_id, ratings, tfidf_matrix, movie_df)\n",
        "    sims = cosine_similarity(user_profile, tfidf_matrix).flatten()\n",
        "    user_seen = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(user_seen)].index\n",
        "    top_indices = unseen_indices[np.argsort(sims[unseen_indices])[-top_n:][::-1]]\n",
        "    return movie_df.iloc[top_indices][['movieId', 'title', 'year']], sims[top_indices]\n"
      ],
      "metadata": {
        "id": "3T-K1RtG-IUk"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Content-Based Similarity Recommendations***\n",
        "\n",
        "Purpose:\n",
        "Generate item recommendations using multiple content-based similarity strategies. Each set of recommendations is labeled by model type for downstream evaluation and comparison.\n",
        "\n",
        "Methodology:\n",
        "1. Load enriched movie metadata and user ratings.\n",
        "2. Create combined feature strings using genres, keywords, cast, directors, and overview.\n",
        "3. Vectorize the features using three methods: TF-IDF, Count, and Binary.\n",
        "4. Compute pairwise similarity:\n",
        "   - Cosine similarity for TF-IDF and Count vectors\n",
        "   - Jaccard similarity for binary vectors\n",
        "5. For a given user, identify previously seen movies and score unseen ones based on average similarity to the seen set.\n",
        "6. Return top-N recommendations as labeled DataFrames including: movieId, title, predicted score, and model name.\n",
        "\n"
      ],
      "metadata": {
        "id": "vj82RTA_ldyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# ==============================\n",
        "# Load Data\n",
        "# ==============================\n",
        "movie_df = load_movie_data(\"movies_enriched_full.csv\")\n",
        "ratings, users = load_user_data(\"ratings.dat\", \"users.dat\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "movie_df.drop(columns=['trailer_link_y', 'backdrop_path_y', 'poster_path_y'], errors='ignore', inplace=True)\n",
        "\n",
        "# Rename to expected column names\n",
        "# movie_df.rename(columns={'top_3_cast': 'cast', 'directors': 'director'}, inplace=True)\n",
        "\n",
        "# Now safely create feature string\n",
        "movie_df = create_feature_string(movie_df)\n",
        "\n",
        "# ==============================\n",
        "# Split Data\n",
        "# ==============================\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# ==============================\n",
        "# Vectorize\n",
        "# ==============================\n",
        "tfidf_matrix_tfidf, _ = vectorize_features(movie_df['cbf_features'], method='tfidf')\n",
        "count_matrix_count, _ = vectorize_features(movie_df['cbf_features'], method='count')\n",
        "binary_matrix_binary, _ = binary_vectorize(movie_df['cbf_features'])\n",
        "sim_matrix_binary_jaccard = jaccard_pairwise_parallel(binary_matrix_binary)\n",
        "\n",
        "# ==============================\n",
        "# Recommender Functions\n",
        "# ==============================\n",
        "def predict_and_evaluate(model_label, user_id, sim_matrix=None, feature_matrix=None):\n",
        "    if sim_matrix is not None:\n",
        "        recs = recommend_from_similarity_matrix(user_id, train_ratings, sim_matrix, movie_df, model_label, top_n=50)\n",
        "    else:\n",
        "        recs = recommend_from_profile(user_id, train_ratings, feature_matrix, movie_df, model_label, top_n=50)\n",
        "\n",
        "    merged = pd.merge(recs, test_ratings[test_ratings['userId'] == user_id][['movieId', 'rating']],\n",
        "                      on='movieId', how='inner')\n",
        "    merged['userId'] = user_id  # Add userId explicitly\n",
        "    merged.rename(columns={'rating': 'true_rating', 'score': 'pred_rating'}, inplace=True)\n",
        "\n",
        "\n",
        "    if not merged.empty:\n",
        "        rmse = np.sqrt(mean_squared_error(merged['true_rating'], merged['pred_rating']))\n",
        "        mae = mean_absolute_error(merged['true_rating'], merged['pred_rating'])\n",
        "    else:\n",
        "        rmse = np.nan\n",
        "        mae = np.nan\n",
        "\n",
        "    merged['model'] = model_label\n",
        "    return merged[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']], rmse, mae\n",
        "\n",
        "# ==============================\n",
        "# Run Evaluation for Each Model\n",
        "# ==============================\n",
        "user_id = 5549\n",
        "results = {}\n",
        "\n",
        "# TF-IDF + Cosine\n",
        "tfidf_df, tfidf_rmse, tfidf_mae = predict_and_evaluate(\"TF-IDF + Cosine\", user_id, feature_matrix=tfidf_matrix_tfidf)\n",
        "results['TF-IDF + Cosine'] = (tfidf_df, tfidf_rmse, tfidf_mae)\n",
        "\n",
        "# Count + Cosine\n",
        "count_df, count_rmse, count_mae = predict_and_evaluate(\"Count + Cosine\", user_id, feature_matrix=count_matrix_count)\n",
        "results['Count + Cosine'] = (count_df, count_rmse, count_mae)\n",
        "\n",
        "# Binary + Jaccard\n",
        "jaccard_df, jaccard_rmse, jaccard_mae = predict_and_evaluate(\"Binary + Jaccard\", user_id, sim_matrix=sim_matrix_binary_jaccard)\n",
        "results['Binary + Jaccard'] = (jaccard_df, jaccard_rmse, jaccard_mae)\n",
        "\n",
        "# ==============================\n",
        "# Save Outputs\n",
        "# ==============================\n",
        "output_dir = \"cbf_outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for model_name, (df, rmse, mae) in results.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"  RMSE: {rmse:.4f}\" if not np.isnan(rmse) else \"  RMSE: N/A\")\n",
        "    print(f\"  MAE : {mae:.4f}\" if not np.isnan(mae) else \"  MAE : N/A\")\n",
        "\n",
        "    filename = model_name.lower().replace(\" + \", \"_\").replace(\" \", \"_\") + \"_predictions.csv\"\n",
        "    df.to_csv(os.path.join(output_dir, filename), index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KBKA0t2-F9X",
        "outputId": "cf2a326c-4ec3-4eb5-b18c-1e68896a9e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3883 movies.\n",
            "Loaded 1000209 ratings and 6040 users.\n",
            "TFIDF vectorization complete. Shape: (3883, 33433)\n",
            "COUNT vectorization complete. Shape: (3883, 33433)\n",
            "Binary Count vectorization complete. Shape: (3883, 33433)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(movie_df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "q2zAPpkFY3Is",
        "outputId": "d40b21f3-8fc5-4a11-f70d-a61b8f9c7849",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movieId', 'title', 'genres', 'year', 'clean_title', 'tmdb_id', 'overview', 'poster_path', 'backdrop_path', 'vote_average', 'vote_count', 'tmdb_genres', 'cast', 'director', 'keywords', 'trailer_link']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# ========================\n",
        "# Utility: Save or Load File\n",
        "# ========================\n",
        "def save_or_load_similarity(file_path, compute_func, *args, **kwargs):\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"✅ Loaded existing similarity matrix: {file_path}\")\n",
        "        return joblib.load(file_path)\n",
        "    else:\n",
        "        print(f\"⚙️ Computing and saving similarity matrix: {file_path}\")\n",
        "        sim = compute_func(*args, **kwargs)\n",
        "        joblib.dump(sim, file_path)\n",
        "        return sim\n",
        "\n",
        "# ========================\n",
        "# Vectorize Functions\n",
        "# ========================\n",
        "def vectorize_features(feature_series, method='tfidf'):\n",
        "    if method == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    elif method == 'count':\n",
        "        vectorizer = CountVectorizer()\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'tfidf' or 'count'\")\n",
        "\n",
        "    matrix = vectorizer.fit_transform(feature_series)\n",
        "    return matrix, vectorizer\n",
        "\n",
        "def binary_vectorize(feature_series):\n",
        "    token_lists = feature_series.apply(lambda x: x.split())\n",
        "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "    matrix = mlb.fit_transform(token_lists)\n",
        "    return matrix, mlb\n",
        "\n",
        "def compute_cosine_similarity(matrix):\n",
        "    return cosine_similarity(matrix)\n",
        "\n",
        "def jaccard_pairwise_parallel(matrix):\n",
        "    # Jaccard similarity for sparse binary matrix\n",
        "    A = matrix.astype(bool).astype(int)\n",
        "    intersection = A @ A.T\n",
        "    row_sums = A.sum(axis=1).A1\n",
        "    union = row_sums[:, None] + row_sums[None, :] - intersection.A\n",
        "    jaccard = intersection.A / np.maximum(union, 1e-10)\n",
        "    return jaccard\n",
        "\n",
        "# ========================\n",
        "# Load Data and Features\n",
        "# ========================\n",
        "movie_df = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "movie_df.drop(columns=['trailer_link_y', 'backdrop_path_y', 'poster_path_y'], errors='ignore', inplace=True)\n",
        "\n",
        "def create_feature_string(df):\n",
        "    df['cbf_features'] = df[['genres', 'keywords', 'cast', 'director']].fillna('').agg(' '.join, axis=1)\n",
        "    return df\n",
        "\n",
        "movie_df = create_feature_string(movie_df)\n",
        "\n",
        "# ========================\n",
        "# TF-IDF + Cosine Similarity\n",
        "# ========================\n",
        "tfidf_matrix, vectorizer_tfidf = vectorize_features(movie_df['cbf_features'], method='tfidf')\n",
        "sim_matrix_tfidf_cosine = save_or_load_similarity(\n",
        "    \"sim_matrix_tfidf_cosine.pkl\",\n",
        "    compute_cosine_similarity,\n",
        "    tfidf_matrix\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# Count + Cosine Similarity\n",
        "# ========================\n",
        "count_matrix, vectorizer_count = vectorize_features(movie_df['cbf_features'], method='count')\n",
        "sim_matrix_count_cosine = save_or_load_similarity(\n",
        "    \"sim_matrix_count_cosine.pkl\",\n",
        "    compute_cosine_similarity,\n",
        "    count_matrix\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# Binary + Jaccard Similarity\n",
        "# ========================\n",
        "binary_matrix, _ = binary_vectorize(movie_df['cbf_features'])\n",
        "sim_matrix_binary_jaccard = save_or_load_similarity(\n",
        "    \"sim_matrix_binary_jaccard.pkl\",\n",
        "    jaccard_pairwise_parallel,\n",
        "    binary_matrix\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "QzSEQyQzT59J",
        "outputId": "c3dde7f0-120b-4ee9-e7d2-cece4161ed96"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['cast', 'director'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-1443102473.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mmovie_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feature_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# ========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-32-1443102473.py\u001b[0m in \u001b[0;36mcreate_feature_string\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_feature_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cbf_features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genres'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'keywords'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cast'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'director'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['cast', 'director'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(movie_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wETMOCrsUiZL",
        "outputId": "7895cb7b-61a2-4314-ad78-62d98b68883d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   movieId                               title                        genres  \\\n",
            "0        1                    Toy Story (1995)   Animation|Children's|Comedy   \n",
            "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy   \n",
            "2        3             Grumpier Old Men (1995)                Comedy|Romance   \n",
            "3        4            Waiting to Exhale (1995)                  Comedy|Drama   \n",
            "4        5  Father of the Bride Part II (1995)                        Comedy   \n",
            "\n",
            "   year                  clean_title  tmdb_id  \\\n",
            "0  1995                    Toy Story    862.0   \n",
            "1  1995                      Jumanji   8844.0   \n",
            "2  1995             Grumpier Old Men  15602.0   \n",
            "3  1995            Waiting to Exhale  31357.0   \n",
            "4  1995  Father of the Bride Part II  11862.0   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                      overview  \\\n",
            "0                                                                                              Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.   \n",
            "1  When siblings Judy and Peter discover an enchanted board game that opens the door to a magical world, they unwittingly invite Alan -- an adult who's been trapped inside the game for 26 years -- into their living room. Alan's only hope for freedom is to finish the game, which proves risky as all three find themselves running from giant rhinoceroses, evil monkeys and other terrifying creatures.   \n",
            "2                                                                      A family wedding reignites the ancient feud between next-door neighbors and fishing buddies John and Max. Meanwhile, a sultry Italian divorcée opens a restaurant at the local bait shop, alarming the locals who worry she'll scare the fish away. But she's less interested in seafood than she is in cooking up a hot time with Max.   \n",
            "3                                                                                                                               Cheated on, mistreated and stepped on, the women are holding their breath, waiting for the elusive \"good man\" to break a string of less-than-stellar lovers. Friends and confidants Vannah, Bernie, Glo and Robin talk it all out, determined to find a better way to breathe.   \n",
            "4                                                                                            Just when George Banks has recovered from his daughter's wedding, he receives the news that she's pregnant ... and that George's wife is expecting too. He was planning on selling their home, but that's a plan that—like George—will have to change with the arrival of both a grandchild and a kid of his own.   \n",
            "\n",
            "                                                       poster_path  \\\n",
            "0  https://image.tmdb.org/t/p/w500/uXDfjJbdP4ijW5hWSBrPrlKpxab.jpg   \n",
            "1  https://image.tmdb.org/t/p/w500/p67m5dzwyxWd46a6of2c9IVfQz7.jpg   \n",
            "2  https://image.tmdb.org/t/p/w500/1FSXpj5e8l4KH6nVFO5SPUeraOt.jpg   \n",
            "3  https://image.tmdb.org/t/p/w500/qJU6rfil5xLVb5HpJsmmfeSK254.jpg   \n",
            "4  https://image.tmdb.org/t/p/w500/rj4LBtwQ0uGrpBnCELr716Qo3mw.jpg   \n",
            "\n",
            "                                                     backdrop_path  \\\n",
            "0  https://image.tmdb.org/t/p/w500/3Rfvhy1Nl6sSGJwyjb0QiZzZYlB.jpg   \n",
            "1  https://image.tmdb.org/t/p/w500/pYw10zrqfkdm3yD9JTO6vEGQhKy.jpg   \n",
            "2  https://image.tmdb.org/t/p/w500/1o4vuCHpmd4DXofMYDUwpnhKiuy.jpg   \n",
            "3  https://image.tmdb.org/t/p/w500/jZjoEKXMTDoZAGdkjhAdJaKtXSN.jpg   \n",
            "4  https://image.tmdb.org/t/p/w500/lEsjVrGU21BeJjF5AF9EWsihDpw.jpg   \n",
            "\n",
            "   vote_average  vote_count                           tmdb_genres  \\\n",
            "0         8.000     18939.0  Animation, Adventure, Family, Comedy   \n",
            "1         7.238     10806.0            Adventure, Fantasy, Family   \n",
            "2         6.461       399.0                       Romance, Comedy   \n",
            "3         6.300       173.0                Comedy, Drama, Romance   \n",
            "4         6.200       755.0                        Comedy, Family   \n",
            "\n",
            "                                        top_3_cast        directors  \\\n",
            "0                Tom Hanks, Tim Allen, Don Rickles    John Lasseter   \n",
            "1    Robin Williams, Kirsten Dunst, Bradley Pierce     Joe Johnston   \n",
            "2         Walter Matthau, Jack Lemmon, Ann-Margret    Howard Deutch   \n",
            "3  Whitney Houston, Angela Bassett, Loretta Devine  Forest Whitaker   \n",
            "4         Steve Martin, Diane Keaton, Martin Short    Charles Shyer   \n",
            "\n",
            "                                                                                                                                                                                                                                            keywords  \\\n",
            "0  rescue, friendship, mission, jealousy, villain, bullying, elementary school, rivalry, anthropomorphism, friends, computer animation, buddy, walkie talkie, toy car, boy next door, new toy, neighborhood, toy comes to life, resourcefulness, toy   \n",
            "1                                                                                                                                   giant insect, board game, disappearance, jungle, recluse, stampede, based on young adult novel, jumanji universe   \n",
            "2                                                                                                                                      fishing, sequel, old man, best friend, wedding, italian restaurant, old friends, duringcreditsstinger, pranks   \n",
            "3                                                             based on novel or book, single mother, divorce, anxious, friendship between women, cautionary, adoring, celebratory, comforting, african american romance, african american friendship   \n",
            "4                                                            daughter, baby, parent child relationship, midlife crisis, pregnancy, confidence, aging, sequel, remake, los angeles, california, pregnant woman, contraception, gynecologist, pregnant   \n",
            "\n",
            "                                  trailer_link  \n",
            "0  https://www.youtube.com/watch?v=CxwTLktovTU  \n",
            "1  https://www.youtube.com/watch?v=veszTagaXik  \n",
            "2  https://www.youtube.com/watch?v=rEnOoWs3FuA  \n",
            "3  https://www.youtube.com/watch?v=j9xml1CxgXI  \n",
            "4  https://www.youtube.com/watch?v=yCg8WNQwe0A  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 8: Content-Based Similarity Recommendations (Multi-Model)\n",
        "# ==============================\n",
        "\n",
        "# ==============================\n",
        "# Step 1: Load Movie & User Data\n",
        "# ==============================\n",
        "\n",
        "movie_df = load_movie_data(\"movies_enriched_full.csv\")\n",
        "ratings, users = load_user_data(\"ratings.dat\", \"users.dat\")\n",
        "\n",
        "# Remove redundant columns if they exist\n",
        "movie_df.drop(columns=['trailer_link_y', 'backdrop_path_y', 'poster_path_y'], errors='ignore', inplace=True)\n",
        "\n",
        "# Recreate CBF Features\n",
        "movie_df = create_feature_string(movie_df)\n",
        "\n",
        "# ==============================\n",
        "# Step 2: Vectorize & Compute Similarities\n",
        "# ==============================\n",
        "\n",
        "# --- TF-IDF + Cosine ---\n",
        "tfidf_matrix_tfidf, vectorizer_tfidf = vectorize_features(movie_df['cbf_features'], method='tfidf')\n",
        "sim_matrix_tfidf_cosine = compute_cosine_similarity(tfidf_matrix_tfidf)\n",
        "\n",
        "# --- Count + Cosine ---\n",
        "count_matrix_count, vectorizer_count = vectorize_features(movie_df['cbf_features'], method='count')\n",
        "sim_matrix_count_cosine = compute_cosine_similarity(count_matrix_count)\n",
        "\n",
        "# --- Binary + Jaccard ---\n",
        "binary_matrix_binary, vectorizer_binary = binary_vectorize(movie_df['cbf_features'])\n",
        "sim_matrix_binary_jaccard = jaccard_pairwise_parallel(binary_matrix_binary)\n",
        "\n",
        "# ==============================\n",
        "# Step 3: Recommendation Functions\n",
        "# ==============================\n",
        "\n",
        "def recommend_from_similarity_matrix(user_id, ratings, sim_matrix, movie_df, model_label, top_n=50):\n",
        "    seen_movie_ids = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    seen_indices = movie_df[movie_df['movieId'].isin(seen_movie_ids)].index.tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(seen_movie_ids)].index.tolist()\n",
        "\n",
        "    if not seen_indices:\n",
        "        print(f\"No ratings found for user {user_id}.\")\n",
        "        return pd.DataFrame(columns=['movieId', 'title', 'score', 'model'])\n",
        "\n",
        "    mean_sims = sim_matrix[unseen_indices][:, seen_indices].mean(axis=1)\n",
        "    top_indices = np.argsort(mean_sims)[-top_n:][::-1]\n",
        "    top_movie_indices = np.array(unseen_indices)[top_indices]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'movieId': movie_df.iloc[top_movie_indices]['movieId'].values,\n",
        "        'title': movie_df.iloc[top_movie_indices]['title'].values,\n",
        "        'score': mean_sims[top_indices],\n",
        "        'model': model_label\n",
        "    })\n",
        "\n",
        "def recommend_from_profile(user_id, ratings, tfidf_matrix, movie_df, model_label, top_n=50):\n",
        "    user_profile = build_user_profile(user_id, ratings, tfidf_matrix, movie_df)\n",
        "    sims = cosine_similarity(user_profile, tfidf_matrix).flatten()\n",
        "    user_seen = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(user_seen)].index\n",
        "    top_indices = unseen_indices[np.argsort(sims[unseen_indices])[-top_n:][::-1]]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'movieId': movie_df.iloc[top_indices]['movieId'].values,\n",
        "        'title': movie_df.iloc[top_indices]['title'].values,\n",
        "        'score': sims[top_indices],\n",
        "        'model': model_label\n",
        "    })\n",
        "\n",
        "# ==============================\n",
        "# Step 4: Generate Recommendations (Labeled Outputs)\n",
        "# ==============================\n",
        "\n",
        "user_id = 5549\n",
        "\n",
        "df_tfidf_cosine = recommend_from_profile(\n",
        "    user_id, ratings, tfidf_matrix_tfidf, movie_df,\n",
        "    model_label='TF-IDF + Cosine', top_n=50\n",
        ")\n",
        "\n",
        "df_count_cosine = recommend_from_profile(\n",
        "    user_id, ratings, count_matrix_count, movie_df,\n",
        "    model_label='Count + Cosine', top_n=50\n",
        ")\n",
        "\n",
        "df_binary_jaccard = recommend_from_similarity_matrix(\n",
        "    user_id, ratings, sim_matrix_binary_jaccard, movie_df,\n",
        "    model_label='Binary + Jaccard', top_n=50\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# Step 5: Combine All Model Outputs\n",
        "# ==============================\n",
        "\n",
        "all_recommendations_combined = pd.concat([\n",
        "    df_tfidf_cosine,\n",
        "    df_count_cosine,\n",
        "    df_binary_jaccard\n",
        "], ignore_index=True)\n",
        "\n",
        "print(\"Recommendation generation complete. Combined shape:\", all_recommendations_combined.shape)\n",
        "print(\"\\nRecommendation generation complete. Combined shape:\\n\", all_recommendations_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClwOydOIOtnH",
        "outputId": "edc78d4e-eaab-46d1-a585-a7ae5d16f93d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3883 movies.\n",
            "Loaded 1000209 ratings and 6040 users.\n",
            "TFIDF vectorization complete. Shape: (3883, 33433)\n",
            "Cosine similarity computed.\n",
            "COUNT vectorization complete. Shape: (3883, 33433)\n",
            "Cosine similarity computed.\n",
            "Binary Count vectorization complete. Shape: (3883, 33433)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jaccard Similarity: 100%|██████████| 3883/3883 [17:40<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity matrix built.\n",
            "Recommendation generation complete. Combined shape: (150, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Memory-based collaborative filtering module (UBCF, IBCF)**\n",
        "\n",
        "***Purpose:***\n",
        "\n",
        "This module implements **memory-based collaborative filtering** using **user-user** or **item-item** similarity. It addresses **user bias** by normalizing ratings through mean-centering and optionally **rescaling predictions** to the original rating scale for interpretability.\n",
        "\n",
        "***Methodology:***\n",
        "\n",
        "1. **Rating Matrix Construction**:\n",
        "\n",
        "   * A user-item matrix is built from raw MovieLens-style ratings data.\n",
        "   * For `kind='user'`, ratings are mean-centered per user to reduce bias from lenient or strict raters.\n",
        "   * For `kind='item'`, raw ratings are used directly (no normalization), as the algorithm focuses on item similarities based on a single user's input.\n",
        "\n",
        "2. **Similarity Computation**:\n",
        "\n",
        "   * Cosine similarity is computed either:\n",
        "\n",
        "     * **Across users** for user-based CF (`kind='user'`)\n",
        "     * **Across items** for item-based CF (`kind='item'`)\n",
        "   * `sklearn.metrics.pairwise_distances` is used to derive similarity as `1 - cosine_distance`.\n",
        "\n",
        "3. **Prediction Generation**:\n",
        "\n",
        "   * For **user-based CF**:\n",
        "\n",
        "     * Ratings from similar users are weighted by similarity and averaged.\n",
        "     * The user’s mean rating is **added back** to restore predictions to the original scale (e.g., 1–5).\n",
        "   * For **item-based CF**:\n",
        "\n",
        "     * A user’s own ratings are used to compute scores for similar items.\n",
        "     * No mean is added back, since predictions are already on the correct scale.\n",
        "\n",
        "4. **Top-N Recommendations**:\n",
        "\n",
        "   * The system filters out movies the user has already rated.\n",
        "   * It ranks unseen movies by predicted score and returns the top-N recommendations.\n",
        "   * Each recommendation is labeled with the model type (`User-Based CF` or `Item-Based CF`) for downstream tracking."
      ],
      "metadata": {
        "id": "omg4Y6k5XmSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 8: Memory-Based Collaborative Filtering (Bias-Normalized)\n",
        "# ==============================\n",
        "# Purpose: Compute user-user or item-item similarity from the rating matrix.\n",
        "# Application: Real-time, interpretable recommendations with optional bias correction.\n",
        "\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Create Mean-Centered User-Item Matrix ---\n",
        "def create_normalized_user_item_matrix(ratings):\n",
        "    \"\"\"\n",
        "    Purpose: Create a user-item matrix with ratings mean-centered per user.\n",
        "    Application: Reduces bias from generous or harsh raters.\n",
        "    \"\"\"\n",
        "    matrix = ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "    user_means = matrix.mean(axis=1)\n",
        "    return matrix.sub(user_means, axis=0).fillna(0), user_means\n",
        "\n",
        "# --- Compute Cosine Similarity ---\n",
        "def compute_similarity(matrix, kind='user'):\n",
        "    \"\"\"\n",
        "    Purpose: Compute pairwise cosine similarity between users or items.\n",
        "    Application: Support for User-User or Item-Item collaborative filtering.\n",
        "    \"\"\"\n",
        "    if kind == 'user':\n",
        "        sim = 1 - pairwise_distances(matrix, metric='cosine')\n",
        "    elif kind == 'item':\n",
        "        sim = 1 - pairwise_distances(matrix.T, metric='cosine')\n",
        "    else:\n",
        "        raise ValueError(\"kind must be 'user' or 'item'\")\n",
        "\n",
        "    print(f\"{kind.title()}-based similarity computed. Shape: {sim.shape}\")\n",
        "    return sim\n",
        "\n",
        "# --- Generate Top-N Recommendations with Rescaled Predictions ---\n",
        "def recommend_memory_based(user_id, user_item_matrix, user_means, similarity_matrix, kind='user', top_n=50):\n",
        "    \"\"\"\n",
        "    Purpose: Recommend items using normalized ratings and return predictions on original scale.\n",
        "    Application: User-User or Item-Item CF with appropriate bias handling.\n",
        "    \"\"\"\n",
        "    model_label = f\"{kind.title()}-Based CF\"\n",
        "\n",
        "    if kind == 'user':\n",
        "        # User-based: normalize ratings and add back mean after prediction\n",
        "        user_sim_scores = similarity_matrix[user_id - 1]\n",
        "        normalized_ratings = user_item_matrix.values\n",
        "\n",
        "        weighted_scores = user_sim_scores @ normalized_ratings\n",
        "        sum_weights = np.abs(user_sim_scores).sum()\n",
        "\n",
        "        if sum_weights == 0:\n",
        "            print(\"No similar users found.\")\n",
        "            return pd.DataFrame(columns=['movieId', 'score', 'model'])\n",
        "\n",
        "        predicted_ratings = weighted_scores / sum_weights\n",
        "        user_seen = user_item_matrix.loc[user_id]\n",
        "        unseen_mask = user_seen == 0\n",
        "        recs = pd.Series(predicted_ratings, index=user_item_matrix.columns)[unseen_mask]\\\n",
        "            .sort_values(ascending=False).head(top_n)\n",
        "\n",
        "        # Re-center predictions to original scale\n",
        "        recs += user_means.loc[user_id]\n",
        "\n",
        "    elif kind == 'item':\n",
        "        # Item-based: do NOT add back user mean\n",
        "        user_ratings = user_item_matrix.loc[user_id]\n",
        "        scores = user_ratings @ similarity_matrix\n",
        "        sum_weights = (user_ratings != 0) @ np.abs(similarity_matrix)\n",
        "\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            predicted_ratings = np.true_divide(scores, sum_weights)\n",
        "            predicted_ratings[sum_weights == 0] = 0\n",
        "\n",
        "        unseen_mask = user_ratings == 0\n",
        "        recs = pd.Series(predicted_ratings, index=user_item_matrix.columns)[unseen_mask]\\\n",
        "            .sort_values(ascending=False).head(top_n)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"kind must be 'user' or 'item'\")\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'movieId': recs.index,\n",
        "        'score': recs.values,\n",
        "        'model': model_label\n",
        "    })\n"
      ],
      "metadata": {
        "id": "_vJLkUYeXmnv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Application of UBCF and IBCF***"
      ],
      "metadata": {
        "id": "FFUDwqVQXqC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 1: Create Bias-Normalized Matrix\n",
        "# ==============================\n",
        "\n",
        "user_item_matrix, user_means = create_normalized_user_item_matrix(ratings)\n",
        "\n",
        "# ==============================\n",
        "# Step 2: Compute Similarity Matrices\n",
        "# ==============================\n",
        "\n",
        "user_sim_matrix = compute_similarity(user_item_matrix, kind='user')\n",
        "item_sim_matrix = compute_similarity(user_item_matrix, kind='item')\n",
        "\n",
        "# ==============================\n",
        "# Step 3: Generate Recommendations\n",
        "# ==============================\n",
        "\n",
        "user_cf_recs = recommend_memory_based(\n",
        "    5549,                    # user_id\n",
        "    user_item_matrix,\n",
        "    user_means,\n",
        "    user_sim_matrix,\n",
        "    kind='user',\n",
        "    top_n=50\n",
        ")\n",
        "\n",
        "item_cf_recs = recommend_memory_based(\n",
        "    5549,                    # user_id\n",
        "    user_item_matrix,\n",
        "    user_means,\n",
        "    item_sim_matrix,\n",
        "    kind='item',\n",
        "    top_n=50\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# Step 4: Merge with Movie Titles Only\n",
        "# ==============================\n",
        "\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]  # Retain only movieId and title\n",
        "user_cf_recs = user_cf_recs.merge(movies, on=\"movieId\", how=\"left\")\n",
        "item_cf_recs = item_cf_recs.merge(movies, on=\"movieId\", how=\"left\")\n",
        "\n",
        "# ==============================\n",
        "# Step 5: Display Output\n",
        "# ==============================\n",
        "\n",
        "print(\"\\nTop 50 User-Based CF Recommendations for User 5549:\")\n",
        "print(user_cf_recs[['movieId', 'title', 'score']].head())\n",
        "\n",
        "print(\"\\nTop 50 Item-Based CF Recommendations for User 5549:\")\n",
        "print(item_cf_recs[['movieId', 'title', 'score']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbI1OXxcXqfv",
        "outputId": "6ef8a1a4-91c7-42be-b7b9-0062abc1678f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-based similarity computed. Shape: (6040, 6040)\n",
            "Item-based similarity computed. Shape: (3706, 3706)\n",
            "\n",
            "Top 50 User-Based CF Recommendations for User 5549:\n",
            "   movieId                                      title     score\n",
            "0     1221             Godfather: Part II, The (1974)  3.600872\n",
            "1     1304  Butch Cassidy and the Sundance Kid (1969)  3.567786\n",
            "2      919                   Wizard of Oz, The (1939)  3.566165\n",
            "3     1207               To Kill a Mockingbird (1962)  3.563622\n",
            "4     1262                   Great Escape, The (1963)  3.558363\n",
            "\n",
            "Top 50 Item-Based CF Recommendations for User 5549:\n",
            "   movieId  \\\n",
            "0     3297   \n",
            "1     3209   \n",
            "2     1316   \n",
            "3     2591   \n",
            "4     1555   \n",
            "\n",
            "                                                                title  \\\n",
            "0                                  With Byrd at the South Pole (1930)   \n",
            "1                                         Loves of Carmen, The (1948)   \n",
            "2                                                         Anna (1996)   \n",
            "3  Jeanne and the Perfect Guy (Jeanne et le garçon formidable) (1998)   \n",
            "4                                              To Have, or Not (1995)   \n",
            "\n",
            "      score  \n",
            "0  2.515152  \n",
            "1  2.515152  \n",
            "2  2.515152  \n",
            "3  2.158898  \n",
            "4  1.904133  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid CBF and UBCF Model"
      ],
      "metadata": {
        "id": "zPb1m8iNIdAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Hybrid Recommender: UBCF + CBF\n",
        "# ==============================\n",
        "\n",
        "def hybrid_ubcf_cbf(user_id, user_item_matrix, user_means, user_sim_matrix,\n",
        "                    tfidf_matrix, ratings, movie_df,\n",
        "                    w_cf=0.2, w_cbf=0.8, top_n=50):\n",
        "    \"\"\"\n",
        "    Combine UBCF and CBF scores via weighted average.\n",
        "\n",
        "    Parameters:\n",
        "    - user_id: int\n",
        "    - user_item_matrix: pd.DataFrame (mean-centered matrix)\n",
        "    - user_means: pd.Series\n",
        "    - user_sim_matrix: np.array\n",
        "    - tfidf_matrix: sparse matrix from TF-IDF\n",
        "    - ratings: pd.DataFrame\n",
        "    - movie_df: pd.DataFrame with movieId, title\n",
        "    - w_cf: float, weight for UBCF\n",
        "    - w_cbf: float, weight for CBF\n",
        "    - top_n: int\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with movieId, title, hybrid_score, model\n",
        "    \"\"\"\n",
        "    # --- UBCF predictions ---\n",
        "    ubcf_df = recommend_memory_based(\n",
        "        user_id=user_id,\n",
        "        user_item_matrix=user_item_matrix,\n",
        "        user_means=user_means,\n",
        "        similarity_matrix=user_sim_matrix,\n",
        "        kind='user',\n",
        "        top_n=1000  # keep more to allow intersection\n",
        "    )\n",
        "\n",
        "    # --- CBF predictions ---\n",
        "    cbf_df = recommend_from_profile(\n",
        "        user_id=user_id,\n",
        "        ratings=ratings,\n",
        "        tfidf_matrix=tfidf_matrix,\n",
        "        movie_df=movie_df,\n",
        "        model_label='CBF',  # temporary label\n",
        "        top_n=1000\n",
        "    ).rename(columns={'score': 'cbf_score'})\n",
        "\n",
        "    # --- Merge ---\n",
        "    ubcf_df = ubcf_df.rename(columns={'score': 'ubcf_score'})\n",
        "    merged = pd.merge(ubcf_df, cbf_df, on='movieId')\n",
        "\n",
        "    # --- Combine Scores ---\n",
        "    merged['hybrid_score'] = w_cf * merged['ubcf_score'] + w_cbf * merged['cbf_score']\n",
        "    hybrid_df = merged[['movieId', 'title', 'hybrid_score']].copy()\n",
        "    hybrid_df['model'] = 'Hybrid (UBCF + CBF)'\n",
        "\n",
        "    return hybrid_df.sort_values(by='hybrid_score', ascending=False).head(top_n)[\n",
        "        ['movieId', 'title', 'hybrid_score', 'model']\n",
        "    ]\n",
        "\n",
        "# ==============================\n",
        "# Generate Hybrid Recommendations for User 5549\n",
        "# ==============================\n",
        "\n",
        "hybrid_recs = hybrid_ubcf_cbf(\n",
        "    user_id=5549,\n",
        "    user_item_matrix=user_item_matrix,\n",
        "    user_means=user_means,\n",
        "    user_sim_matrix=user_sim_matrix,\n",
        "    tfidf_matrix=tfidf_matrix_tfidf,\n",
        "    ratings=ratings,\n",
        "    movie_df=movie_df,\n",
        "    w_cf=0.5,\n",
        "    w_cbf=0.5,\n",
        "    top_n=50\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# Display Output\n",
        "# ==============================\n",
        "\n",
        "print(\"\\nTop 50 Hybrid Recommendations for User 5549:\")\n",
        "print(hybrid_recs.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqR9UEVpIlUW",
        "outputId": "b4c5877a-110b-4964-a33b-6af3b59bb71f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 50 Hybrid Recommendations for User 5549:\n",
            "     movieId                              title  hybrid_score  \\\n",
            "0       1221     Godfather: Part II, The (1974)      1.894670   \n",
            "192     3457             Waking the Dead (1999)      1.866184   \n",
            "155     3177                 Next Friday (1999)      1.845677   \n",
            "215     3721                      Trixie (1999)      1.844950   \n",
            "37      2995  House on Haunted Hill, The (1999)      1.831576   \n",
            "\n",
            "                   model  \n",
            "0    Hybrid (UBCF + CBF)  \n",
            "192  Hybrid (UBCF + CBF)  \n",
            "155  Hybrid (UBCF + CBF)  \n",
            "215  Hybrid (UBCF + CBF)  \n",
            "37   Hybrid (UBCF + CBF)  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model-Based Filtering:**\n",
        "\n",
        "  * *SVD (Surprise)*: Learns latent features from the rating matrix.\n",
        "  * *ALS (PySpark)*: Scalable factorization method for large datasets.\n"
      ],
      "metadata": {
        "id": "R6d3SP0OolH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 9: Model-Based Collaborative Filtering (SVD using Surprise)**\n",
        "\n",
        "**Purpose:**\n",
        "Use matrix factorization (SVD) to learn latent user/item features from the rating matrix.\n",
        "\n",
        "**Application:**\n",
        "- Accurate, scalable recommendations for sparse datasets using user/item embeddings.\n",
        "- Suitable for small to medium datasets.\n",
        "- Optimized via `GridSearchCV` for hyperparameter tuning.\n",
        "- Good interpretability of latent factors per user and item.\n",
        "\n"
      ],
      "metadata": {
        "id": "4dCPxrNMnQkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 9: Model-Based Collaborative Filtering (SVD using Surprise)\n",
        "# ==============================\n",
        "\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split, GridSearchCV\n",
        "from surprise.accuracy import rmse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================\n",
        "# Prepare Surprise Dataset\n",
        "# ==============================\n",
        "\n",
        "def prepare_surprise_data(ratings):\n",
        "    reader = Reader(rating_scale=(0.5, 5.0))\n",
        "    return Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# ==============================\n",
        "# Tune SVD Model with Grid Search\n",
        "# ==============================\n",
        "\n",
        "def tune_svd_model(data):\n",
        "    param_grid = {\n",
        "        'n_factors': [50, 100],\n",
        "        'lr_all': [0.005, 0.01],\n",
        "        'reg_all': [0.02, 0.1]\n",
        "    }\n",
        "    print(\"Tuning SVD model with GridSearchCV...\")\n",
        "    gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, joblib_verbose=0)\n",
        "\n",
        "    with tqdm(total=1, desc=\"GridSearchCV\") as pbar:\n",
        "        gs.fit(data)\n",
        "        pbar.update(1)\n",
        "\n",
        "    print(f\"Best RMSE: {gs.best_score['rmse']} with params: {gs.best_params['rmse']}\")\n",
        "    return gs.best_estimator['rmse']\n",
        "\n",
        "# ==============================\n",
        "# Train and Evaluate SVD\n",
        "# ==============================\n",
        "\n",
        "def evaluate_svd(model, data, model_label='SVD (Surprise)'):\n",
        "    trainset, testset = train_test_split(data, test_size=0.2)\n",
        "    model.fit(trainset)\n",
        "\n",
        "    print(\"Making predictions...\")\n",
        "    predictions = [model.predict(item[0], item[1], r_ui=item[2]) for item in tqdm(testset, desc=\"Predicting\")]\n",
        "\n",
        "    score = rmse(predictions)\n",
        "\n",
        "    pred_df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
        "    pred_df = pred_df.rename(columns={'uid': 'userId', 'iid': 'movieId', 'rui': 'true_rating', 'est': 'pred_rating'})\n",
        "    pred_df['model'] = model_label\n",
        "    return pred_df[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']], score\n",
        "\n",
        "# ==============================\n",
        "# Main Execution\n",
        "# ==============================\n",
        "\n",
        "# Step 1: Load ratings\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# Step 2: Prepare Surprise data\n",
        "data = prepare_surprise_data(ratings)\n",
        "\n",
        "# Step 3: Tune model\n",
        "best_svd_model = tune_svd_model(data)\n",
        "\n",
        "# Step 4: Evaluate model\n",
        "pred_df, rmse_score = evaluate_svd(best_svd_model, data)\n",
        "\n",
        "# Step 5: Output\n",
        "print(pred_df.head())\n",
        "print(f\"Final RMSE: {rmse_score:.4f}\")\n",
        "\n",
        "# Step 6: Top-50 Predictions for User 5549\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = [mid for mid in all_movie_ids if mid not in rated_movie_ids]\n",
        "\n",
        "print(f\"\\nGenerating predictions for User {target_user}...\")\n",
        "top_preds = [(movie_id, best_svd_model.predict(target_user, movie_id).est)\n",
        "             for movie_id in tqdm(unrated_movie_ids, desc=\"Predicting for user\")]\n",
        "\n",
        "top_50_df = pd.DataFrame(top_preds, columns=['movieId', 'pred_rating'])\n",
        "top_50_df = top_50_df.sort_values(by='pred_rating', ascending=False).head(50)\n",
        "top_50_df['userId'] = target_user\n",
        "top_50_df['model'] = 'SVD (Surprise)'\n",
        "top_50_df = top_50_df[['userId', 'movieId', 'pred_rating', 'model']]\n",
        "\n",
        "# Step 7: Merge with movie titles only\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_df = top_50_df.merge(movies, on='movieId', how='left')\n",
        "\n",
        "# Step 8: Final Output\n",
        "print(\"\\nTop 50 Recommendations for User 5549:\")\n",
        "print(top_50_df[['movieId', 'title', 'pred_rating']].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0JR9B0YokAB",
        "outputId": "f31dbe34-42f8-4e19-b989-bd5ede2e7e29"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning SVD model with GridSearchCV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GridSearchCV: 100%|██████████| 1/1 [06:38<00:00, 398.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE: 0.8820577922491172 with params: {'n_factors': 50, 'lr_all': 0.005, 'reg_all': 0.02}\n",
            "Making predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 200042/200042 [00:02<00:00, 95750.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.8701\n",
            "   userId  movieId  true_rating  pred_rating           model\n",
            "0    1470     2873          1.0     2.022161  SVD (Surprise)\n",
            "1    1974     3201          4.0     4.217697  SVD (Surprise)\n",
            "2    2825     2384          5.0     3.837053  SVD (Surprise)\n",
            "3     462     2640          3.0     3.146119  SVD (Surprise)\n",
            "4    1937      858          5.0     4.142713  SVD (Surprise)\n",
            "Final RMSE: 0.8701\n",
            "\n",
            "Generating predictions for User 5549...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting for user: 100%|██████████| 3673/3673 [00:00<00:00, 173473.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 50 Recommendations for User 5549:\n",
            "   movieId  \\\n",
            "0      911   \n",
            "1     2905   \n",
            "2     1262   \n",
            "3     1207   \n",
            "4     2019   \n",
            "5      920   \n",
            "6     1272   \n",
            "7      913   \n",
            "8     3338   \n",
            "9      318   \n",
            "\n",
            "                                                                 title  \\\n",
            "0                                                       Charade (1963)   \n",
            "1                                                       Sanjuro (1962)   \n",
            "2                                             Great Escape, The (1963)   \n",
            "3                                         To Kill a Mockingbird (1962)   \n",
            "4  Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)   \n",
            "5                                            Gone with the Wind (1939)   \n",
            "6                                                        Patton (1970)   \n",
            "7                                           Maltese Falcon, The (1941)   \n",
            "8                                               For All Mankind (1989)   \n",
            "9                                     Shawshank Redemption, The (1994)   \n",
            "\n",
            "   pred_rating  \n",
            "0     4.425615  \n",
            "1     4.385578  \n",
            "2     4.378872  \n",
            "3     4.369493  \n",
            "4     4.346556  \n",
            "5     4.294037  \n",
            "6     4.292043  \n",
            "7     4.281435  \n",
            "8     4.272779  \n",
            "9     4.260661  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model-Based Collaborative Filtering (ALS using PySpark)**\n",
        "\n",
        "**Purpose:**\n",
        "Use Alternating Least Squares (ALS) to learn latent user/item features at scale.\n",
        "\n",
        "**Application:**\n",
        "- Distributed recommendation system for large-scale datasets.\n",
        "- Runs on Apache Spark for horizontal scalability.\n",
        "- Handles sparsity well using factorization.\n",
        "- Suited for real-time, production-level systems with massive data.\n"
      ],
      "metadata": {
        "id": "DeCmPC_DnZip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 10: Model-Based Collaborative Filtering (ALS using PySpark)\n",
        "# ==============================\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import Row\n",
        "import pandas as pd\n",
        "\n",
        "# --- Start Spark Session ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ALSModel\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Load Ratings ---\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "ratings_df = spark.createDataFrame(ratings[['userId', 'movieId', 'rating']])\n",
        "\n",
        "# --- Train ALS Model ---\n",
        "als = ALS(\n",
        "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "    rank=10, maxIter=10, regParam=0.1,\n",
        "    coldStartStrategy=\"drop\", nonnegative=True\n",
        ")\n",
        "als_model = als.fit(ratings_df)\n",
        "\n",
        "# --- Evaluate ALS Model ---\n",
        "predictions = als_model.transform(ratings_df)\n",
        "pred_pd = predictions.select('userId', 'movieId', 'rating', 'prediction').toPandas()\n",
        "pred_pd = pred_pd.rename(columns={'rating': 'true_rating', 'prediction': 'pred_rating'})\n",
        "pred_pd['model'] = 'ALS (PySpark)'\n",
        "\n",
        "# --- Evaluate ALS RMSE ---\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName='rmse',\n",
        "    labelCol='rating',\n",
        "    predictionCol='prediction'\n",
        ")\n",
        "rmse_score = evaluator.evaluate(predictions)\n",
        "\n",
        "# --- Output Evaluation ---\n",
        "print(pred_pd[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']].head())\n",
        "print(f\"\\nFinal RMSE: {rmse_score:.4f}\")\n",
        "\n",
        "# ==============================\n",
        "# Step 6: Top-50 Predictions for User 5549\n",
        "# ==============================\n",
        "\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = list(set(all_movie_ids) - set(rated_movie_ids))\n",
        "\n",
        "# Create Spark DataFrame of userId + unrated movieId pairs\n",
        "user_unrated_pairs = spark.createDataFrame([Row(userId=target_user, movieId=int(mid)) for mid in unrated_movie_ids])\n",
        "\n",
        "# Predict ratings using ALS model\n",
        "print(f\"\\nGenerating Top-50 recommendations for User {target_user}...\")\n",
        "top_preds_df = als_model.transform(user_unrated_pairs).dropna()\n",
        "\n",
        "# Get top-50 highest predicted ratings\n",
        "top_50_preds = top_preds_df.orderBy(col(\"prediction\").desc()).limit(50)\n",
        "top_50_pd = top_50_preds.select(\"userId\", \"movieId\", \"prediction\").toPandas()\n",
        "top_50_pd['model'] = \"ALS (PySpark)\"\n",
        "top_50_pd = top_50_pd.rename(columns={'prediction': 'pred_rating'})\n",
        "\n",
        "# ==============================\n",
        "# Step 7: Merge with Movie Titles Only\n",
        "# ==============================\n",
        "\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_pd = top_50_pd.merge(movies, on='movieId', how='left')\n",
        "\n",
        "# ==============================\n",
        "# Step 8: Output Top-50\n",
        "# ==============================\n",
        "\n",
        "print(\"\\nTop 50 Recommendations for User 5549:\")\n",
        "print(top_50_pd[['movieId', 'title', 'pred_rating']].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLtk_Rsuom7R",
        "outputId": "5ae78569-65a9-4e28-ef1d-773016e71472"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   userId  movieId  true_rating  pred_rating          model\n",
            "0     148     2122            4     2.768286  ALS (PySpark)\n",
            "1     148     2142            4     3.385553  ALS (PySpark)\n",
            "2     148     2366            5     3.547921  ALS (PySpark)\n",
            "3     148     3175            5     3.873491  ALS (PySpark)\n",
            "4     148     1580            4     4.024727  ALS (PySpark)\n",
            "\n",
            "Final RMSE: 0.8357\n",
            "\n",
            "Generating Top-50 recommendations for User 5549...\n",
            "\n",
            "Top 50 Recommendations for User 5549:\n",
            "   movieId                            title  pred_rating\n",
            "0      572           Foreign Student (1994)     5.054198\n",
            "1     1471               Boys Life 2 (1997)     4.919178\n",
            "2     2760  Gambler, The (A Játékos) (1997)     4.464029\n",
            "3      953     It's a Wonderful Life (1946)     4.359691\n",
            "4     1519            Broken English (1996)     4.351626\n",
            "5     2503          Apple, The (Sib) (1998)     4.329711\n",
            "6     2129     Saltmen of Tibet, The (1997)     4.324127\n",
            "7      912                Casablanca (1942)     4.317692\n",
            "8      751                   Careful (1992)     4.312503\n",
            "9     3365            Searchers, The (1956)     4.302980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ls6RqljaolUG"
      }
    }
  ]
}