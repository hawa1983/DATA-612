{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-612/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose**\n",
        "\n",
        "The purpose of this script is to enrich the MovieLens movie dataset (`movies.dat`) with detailed movie metadata from The Movie Database (TMDB) API. This metadata includes movie overviews, genres, poster and backdrop image URLs, cast and director information, keywords, user ratings, and trailer links. The enriched dataset will serve as the foundation for building content-based, collaborative, and hybrid recommender systems.\n",
        "\n",
        "### **Methodology**\n",
        "\n",
        "1. **Load MovieLens Movie Data**\n",
        "   The script loads the `movies.dat` file, which contains basic movie information including `movieId`, `title`, and `genres`.\n",
        "\n",
        "2. **Clean Titles and Extract Years**\n",
        "   It processes the movie titles to remove the year from the title string and separately extracts the release year to improve search accuracy when querying TMDB.\n",
        "\n",
        "3. **Query TMDB API**\n",
        "   For each movie, it sends a search request to TMDB using the cleaned title and release year. If a match is found, it retrieves the movie’s TMDB ID.\n",
        "\n",
        "4. **Retrieve Detailed Metadata**\n",
        "   Using the TMDB ID, the script fetches:\n",
        "\n",
        "   * Overview (plot summary)\n",
        "   * Poster and backdrop image paths\n",
        "   * Genre IDs, which are then mapped to readable genre names\n",
        "   * Top 3 cast members\n",
        "   * Director(s)\n",
        "   * Associated keywords\n",
        "   * YouTube trailer link (if available)\n",
        "\n",
        "5. **Construct and Save Enriched Dataset**\n",
        "   All metadata is compiled into a structured format and merged with the original MovieLens data. The final dataset is saved as `movies_enriched_full.csv` for downstream use in recommendation models.\n"
      ],
      "metadata": {
        "id": "OptAJ12iBc8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "7qJ0maYGaxtJ",
        "outputId": "2cab814c-7502-488a-f415-fd6922272d72"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'movies.dat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-494969615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Load movies.dat - format: MovieID::Title::Genres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmovies_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"movies.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"genres\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# ---------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movies.dat'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ---------------------------------------\n",
        "# CONFIG\n",
        "# ---------------------------------------\n",
        "BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "IMAGE_BASE = \"https://image.tmdb.org/t/p/w500\"\n",
        "\n",
        "# Use your TMDB Bearer Token (v4)\n",
        "HEADERS = {\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyZGZlNjMwMGMzYjIzMjc2NzExNjQ0N2JhNzhiMjM5MyIsIm5iZiI6MTc1MTkyMjA3Ni4xMzUsInN1YiI6IjY4NmMzNTljMzc4NjllOGEyNDUxZTM0OSIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.S773ddH3FiIHtokPW4sYpJog0mXWS1o4OPov1KZneUw\"\n",
        "}\n",
        "\n",
        "# TMDB genre ID to name mapping\n",
        "GENRE_ID_TO_NAME = {\n",
        "    28: \"Action\", 12: \"Adventure\", 16: \"Animation\", 35: \"Comedy\", 80: \"Crime\",\n",
        "    99: \"Documentary\", 18: \"Drama\", 10751: \"Family\", 14: \"Fantasy\", 36: \"History\",\n",
        "    27: \"Horror\", 10402: \"Music\", 9648: \"Mystery\", 10749: \"Romance\", 878: \"Science Fiction\",\n",
        "    10770: \"TV Movie\", 53: \"Thriller\", 10752: \"War\", 37: \"Western\"\n",
        "}\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 1: Load MovieLens .dat Files\n",
        "# ---------------------------------------\n",
        "\n",
        "# Load movies.dat - format: MovieID::Title::Genres\n",
        "movies_df = pd.read_csv(\"movies.dat\", sep=\"::\", engine='python', header=None, names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 2: Clean Movie Titles and Extract Year\n",
        "# ---------------------------------------\n",
        "\n",
        "def extract_year(title):\n",
        "    if \"(\" in title:\n",
        "        try:\n",
        "            return int(title.strip()[-5:-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_title(title):\n",
        "    if \"(\" in title:\n",
        "        return title[:title.rfind(\"(\")].strip()\n",
        "    return title.strip()\n",
        "\n",
        "movies_df[\"year\"] = movies_df[\"title\"].apply(extract_year)\n",
        "movies_df[\"clean_title\"] = movies_df[\"title\"].apply(clean_title)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 3: TMDB Metadata Functions\n",
        "# ---------------------------------------\n",
        "\n",
        "# Search for movie in TMDB\n",
        "def search_tmdb(title, year):\n",
        "    url = f\"{BASE_URL}/search/movie\"\n",
        "    params = {\"query\": title, \"year\": year}\n",
        "    response = requests.get(url, headers=HEADERS, params=params)\n",
        "    r = response.json()\n",
        "    if r.get(\"results\"):\n",
        "        return r[\"results\"][0]\n",
        "    return None\n",
        "\n",
        "# Get full metadata from TMDB\n",
        "def get_full_tmdb_metadata(tmdb_id):\n",
        "    metadata = {}\n",
        "\n",
        "    # Credits (cast, crew)\n",
        "    credits = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/credits\", headers=HEADERS).json()\n",
        "    cast = [c[\"name\"] for c in credits.get(\"cast\", [])[:3]]\n",
        "    directors = [c[\"name\"] for c in credits.get(\"crew\", []) if c.get(\"job\") == \"Director\"]\n",
        "\n",
        "    # Keywords\n",
        "    keywords = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/keywords\", headers=HEADERS).json()\n",
        "    keyword_list = [k[\"name\"] for k in keywords.get(\"keywords\", [])]\n",
        "\n",
        "    # Videos (trailers)\n",
        "    videos = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/videos\", headers=HEADERS).json()\n",
        "    trailer_links = [\n",
        "        f\"https://www.youtube.com/watch?v={v['key']}\"\n",
        "        for v in videos.get(\"results\", [])\n",
        "        if v[\"site\"] == \"YouTube\" and v[\"type\"] == \"Trailer\"\n",
        "    ]\n",
        "\n",
        "    # Final metadata dictionary\n",
        "    metadata[\"top_3_cast\"] = \", \".join(cast)\n",
        "    metadata[\"directors\"] = \", \".join(directors)\n",
        "    metadata[\"keywords\"] = \", \".join(keyword_list)\n",
        "    metadata[\"trailer_link\"] = trailer_links[0] if trailer_links else None\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 4: Enrich Movie Data\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched = []\n",
        "\n",
        "for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
        "    movie_data = search_tmdb(row[\"clean_title\"], row[\"year\"])\n",
        "\n",
        "    if movie_data:\n",
        "        tmdb_id = movie_data[\"id\"]\n",
        "        extra = get_full_tmdb_metadata(tmdb_id)\n",
        "\n",
        "        genre_ids = movie_data.get(\"genre_ids\", [])\n",
        "        genre_names = [GENRE_ID_TO_NAME.get(gid, str(gid)) for gid in genre_ids]\n",
        "\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": tmdb_id,\n",
        "            \"overview\": movie_data.get(\"overview\", \"\"),\n",
        "            \"poster_path\": IMAGE_BASE + movie_data.get(\"poster_path\", \"\") if movie_data.get(\"poster_path\") else None,\n",
        "            \"backdrop_path\": IMAGE_BASE + movie_data.get(\"backdrop_path\", \"\") if movie_data.get(\"backdrop_path\") else None,\n",
        "            \"vote_average\": movie_data.get(\"vote_average\", None),\n",
        "            \"vote_count\": movie_data.get(\"vote_count\", None),\n",
        "            \"tmdb_genres\": \", \".join(genre_names),\n",
        "            **extra\n",
        "        })\n",
        "    else:\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": None,\n",
        "            \"overview\": None,\n",
        "            \"poster_path\": None,\n",
        "            \"backdrop_path\": None,\n",
        "            \"vote_average\": None,\n",
        "            \"vote_count\": None,\n",
        "            \"tmdb_genres\": None,\n",
        "            \"top_3_cast\": None,\n",
        "            \"directors\": None,\n",
        "            \"keywords\": None,\n",
        "            \"trailer_link\": None\n",
        "        })\n",
        "\n",
        "    time.sleep(0.25)  # Respect TMDB API rate limits\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 5: Save Final Dataset\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched)\n",
        "final_df = pd.concat([movies_df, enriched_df], axis=1)\n",
        "final_df.to_csv(\"movies_enriched_full.csv\", index=False)\n",
        "\n",
        "print(\"DONE: Saved as 'movies_enriched_full.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Personalized Content-Based Movie Recommendation System**\n",
        "\n",
        "This Python script implements a **Content-Based Filtering (CBF)** system enhanced with **personalized recommendations** using user-specific rating profiles. Built using the MovieLens 1M dataset and enriched metadata, the pipeline performs vectorization, similarity computation, and profile-based predictions.\n",
        "\n",
        "**What This Script Does**\n",
        "\n",
        "* **Module 1–2**: Load essential libraries and enriched movie data.\n",
        "* **Module 3**: Load user ratings and demographics.\n",
        "* **Module 4**: Engineer features combining genres, cast, crew, keywords, and movie overviews.\n",
        "* **Module 5**: Transform content into TF-IDF, Count, or Binary vectors, and compute pairwise similarities using Cosine or Jaccard metrics.\n",
        "* **Module 6**: Construct a weighted content profile per user based on past ratings.\n",
        "* **Module 7**: Recommend top-N movies similar to the user profile, excluding already seen titles.\n",
        "\n",
        "**Techniques Used**\n",
        "\n",
        "* **Text Vectorization**: TF-IDF, CountVectorizer, Binary Count\n",
        "* **Similarity Metrics**: Cosine Similarity, Jaccard Similarity\n",
        "* **Personalization**: Weighted vector averaging based on each user’s rated items\n",
        "* **Parallelization**: Speeds up Jaccard similarity computation using joblib\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "* Personalized recommendations for new users with a few ratings (cold-start)\n",
        "* Improving diversity and relevance in suggested movies\n",
        "* Generating fallback content suggestions in hybrid recommender systems"
      ],
      "metadata": {
        "id": "c7KNZkOThu7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9GeB2GWMyfPL",
        "outputId": "2a72c22b-308d-49f6-a9d9-72a1f9bbc94d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Collecting scikit-surprise==1.1.4 (from -r requirements.txt (line 3))\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==1.4.2 (from -r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting matplotlib==3.8.4 (from -r requirements.txt (line 5))\n",
            "  Downloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
            "Collecting tqdm==4.66.4 (from -r requirements.txt (line 7))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise==1.1.4->-r requirements.txt (line 3)) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 2)) (1.17.0)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m128.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2469552 sha256=61d62eddc4bc42c705b1b96d4005c633168dd318df721aa056c9a844a3bebebf\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: tqdm, numpy, scikit-surprise, scikit-learn, matplotlib\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "dataproc-spark-connect 0.8.2 requires tqdm>=4.67, but you have tqdm 4.66.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.8.4 numpy-1.26.4 scikit-learn-1.4.2 scikit-surprise-1.1.4 tqdm-4.66.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "sklearn",
                  "tqdm"
                ]
              },
              "id": "b048df46552d4fde93038bb3cd17cd8c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 1: Imports & Configuration\n",
        "# ==============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# ==============================\n",
        "# Module 2: Load Movie Data\n",
        "# ==============================\n",
        "def load_movie_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    print(f\"Loaded {len(df)} movies.\")\n",
        "    return df\n",
        "\n",
        "# ==============================\n",
        "# Module 3: Load User Ratings and Demographics\n",
        "# ==============================\n",
        "def load_user_data(ratings_path, users_path):\n",
        "    ratings = pd.read_csv(ratings_path, sep=\"::\", engine=\"python\",\n",
        "                          names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "    users = pd.read_csv(users_path, sep=\"::\", engine=\"python\",\n",
        "                        names=[\"userId\", \"gender\", \"age\", \"occupation\", \"zip\"])\n",
        "    print(f\"Loaded {len(ratings)} ratings and {len(users)} users.\")\n",
        "    return ratings, users\n",
        "\n",
        "# ==============================\n",
        "# Module 4: Feature Engineering\n",
        "# ==============================\n",
        "def create_feature_string(df):\n",
        "    def split_and_clean(col, delimiter='|'):\n",
        "        return col.fillna('').str.replace(r'\\s+', '', regex=True).str.split(delimiter)\n",
        "\n",
        "    genre_list_1 = split_and_clean(df['genres'], delimiter='|')\n",
        "    genre_list_2 = split_and_clean(df['tmdb_genres'], delimiter=',')\n",
        "    merged_genres = [\n",
        "        ' '.join(sorted(set(g1 or []) | set(g2 or [])))\n",
        "        for g1, g2 in zip(genre_list_1, genre_list_2)\n",
        "    ]\n",
        "\n",
        "    def clean_text(col):\n",
        "        return col.fillna('').str.replace(r'\\s+', '', regex=True).str.replace(',', ' ')\n",
        "\n",
        "    overview_clean = df['overview'].fillna('').str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
        "    year_str = df['year'].astype(str).fillna('')\n",
        "\n",
        "    df['cbf_features'] = (\n",
        "        pd.Series(merged_genres) + ' ' +\n",
        "        clean_text(df['keywords']) + ' ' +\n",
        "        clean_text(df['top_3_cast']) + ' ' +\n",
        "        clean_text(df['directors']) + ' ' +\n",
        "        overview_clean + ' ' +\n",
        "        year_str\n",
        "    )\n",
        "\n",
        "    return df[['movieId', 'title', 'cbf_features']]\n",
        "\n",
        "# ==============================\n",
        "# Module 5: Vectorization & Similarity\n",
        "# ==============================\n",
        "def vectorize_features(text_series, method='tfidf'):\n",
        "    if method == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    elif method == 'count':\n",
        "        vectorizer = CountVectorizer(stop_words='english')\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'tfidf' or 'count'\")\n",
        "    matrix = vectorizer.fit_transform(text_series)\n",
        "    print(f\"{method.upper()} vectorization complete. Shape: {matrix.shape}\")\n",
        "    return matrix, vectorizer\n",
        "\n",
        "def binary_vectorize(text_series):\n",
        "    vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
        "    matrix = vectorizer.fit_transform(text_series)\n",
        "    print(f\"Binary Count vectorization complete. Shape: {matrix.shape}\")\n",
        "    return matrix.toarray(), vectorizer\n",
        "\n",
        "def compute_cosine_similarity(matrix):\n",
        "    sim = cosine_similarity(matrix)\n",
        "    print(\"Cosine similarity computed.\")\n",
        "    return sim\n",
        "\n",
        "# def jaccard_pairwise_parallel(matrix):\n",
        "#     n = matrix.shape[0]\n",
        "#     sim_matrix = np.zeros((n, n))\n",
        "\n",
        "#     def jaccard_row(i):\n",
        "#         a = matrix[i]\n",
        "#         row_sim = np.zeros(n)\n",
        "#         for j in range(i, n):\n",
        "#             b = matrix[j]\n",
        "#             intersection = np.logical_and(a, b).sum()\n",
        "#             union = np.logical_or(a, b).sum()\n",
        "#             score = intersection / union if union > 0 else 0.0\n",
        "#             row_sim[j] = score\n",
        "#         return i, row_sim\n",
        "\n",
        "#     results = Parallel(n_jobs=-1)(\n",
        "#         delayed(jaccard_row)(i) for i in tqdm(range(n), desc=\"Jaccard Similarity\")\n",
        "#     )\n",
        "\n",
        "#     for i, row in results:\n",
        "#         sim_matrix[i, i:] = row[i:]\n",
        "#         sim_matrix[i:, i] = row[i:]\n",
        "\n",
        "#     print(\"Jaccard similarity matrix built.\")\n",
        "#     return sim_matrix\n",
        "\n",
        "# def jaccard_pairwise_parallel(A):\n",
        "#     A = A.astype(bool).astype(int)  # Ensure binary\n",
        "#     intersection = A @ A.T\n",
        "#     row_sums = A.sum(axis=1).A1  # Convert to 1D array\n",
        "#     union = row_sums[:, None] + row_sums[None, :] - intersection\n",
        "#     jaccard = intersection / np.maximum(union, 1e-10)  # Prevent divide by zero\n",
        "#     return jaccard\n",
        "\n",
        "def jaccard_pairwise_parallel(A):\n",
        "    A = A.astype(bool).astype(int)  # Ensure binary\n",
        "    intersection = A @ A.T\n",
        "    row_sums = A.sum(axis=1)  # This is already a 1D NumPy array\n",
        "    union = row_sums[:, None] + row_sums[None, :] - intersection\n",
        "    jaccard = intersection / np.maximum(union, 1e-10)\n",
        "    return jaccard\n",
        "\n",
        "\n",
        "def save_matrix(matrix, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(matrix, f)\n",
        "    print(f\"Saved similarity matrix to: {filename}\")\n",
        "\n",
        "# ==============================\n",
        "# Module 6: Build User Profile\n",
        "# ==============================\n",
        "def build_user_profile(user_id, ratings, tfidf_matrix, movie_df):\n",
        "    user_ratings = ratings[ratings['userId'] == user_id]\n",
        "    rated_movies = movie_df[movie_df['movieId'].isin(user_ratings['movieId'])]\n",
        "    indices = rated_movies.index.tolist()\n",
        "    weights = user_ratings.set_index('movieId').loc[rated_movies['movieId']]['rating'].values\n",
        "    profile = np.average(tfidf_matrix[indices].toarray(), axis=0, weights=weights)\n",
        "    return profile.reshape(1, -1)\n",
        "\n",
        "def recommend_from_profile(user_id, ratings, feature_matrix, movie_df, model_label, top_n=50):\n",
        "    user_profile = build_user_profile(user_id, ratings, feature_matrix, movie_df)\n",
        "    sims = cosine_similarity(user_profile, feature_matrix).flatten()\n",
        "\n",
        "    user_seen = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(user_seen)].index\n",
        "    top_indices = unseen_indices[np.argsort(sims[unseen_indices])[-top_n:][::-1]]\n",
        "\n",
        "    recs = movie_df.iloc[top_indices][['movieId', 'title']].copy()\n",
        "    recs['score'] = sims[top_indices]\n",
        "    return recs\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Module 7: Personalized Recommendation\n",
        "# ==============================\n",
        "def recommend_movies(user_id, ratings, tfidf_matrix, movie_df, top_n=50):\n",
        "    user_profile = build_user_profile(user_id, ratings, tfidf_matrix, movie_df)\n",
        "    sims = cosine_similarity(user_profile, tfidf_matrix).flatten()\n",
        "    user_seen = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(user_seen)].index\n",
        "    top_indices = unseen_indices[np.argsort(sims[unseen_indices])[-top_n:][::-1]]\n",
        "    return movie_df.iloc[top_indices][['movieId', 'title', 'year']], sims[top_indices]\n"
      ],
      "metadata": {
        "id": "3T-K1RtG-IUk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Content-Based Similarity Recommendations***\n",
        "\n",
        "Purpose:\n",
        "Generate item recommendations using multiple content-based similarity strategies. Each set of recommendations is labeled by model type for downstream evaluation and comparison.\n",
        "\n",
        "Methodology:\n",
        "1. Load enriched movie metadata and user ratings.\n",
        "2. Create combined feature strings using genres, keywords, cast, directors, and overview.\n",
        "3. Vectorize the features using three methods: TF-IDF, Count, and Binary.\n",
        "4. Compute pairwise similarity:\n",
        "   - Cosine similarity for TF-IDF and Count vectors\n",
        "   - Jaccard similarity for binary vectors\n",
        "5. For a given user, identify previously seen movies and score unseen ones based on average similarity to the seen set.\n",
        "6. Return top-N recommendations as labeled DataFrames including: movieId, title, predicted score, and model name.\n",
        "\n"
      ],
      "metadata": {
        "id": "vj82RTA_ldyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# ==============================\n",
        "# Load Data\n",
        "# ==============================\n",
        "movie_df = load_movie_data(\"movies_enriched_full.csv\")\n",
        "ratings, users = load_user_data(\"ratings.dat\", \"users.dat\")\n",
        "\n",
        "movie_df.drop(columns=['trailer_link_y', 'backdrop_path_y', 'poster_path_y'], errors='ignore', inplace=True)\n",
        "\n",
        "# Now safely create feature string\n",
        "movie_df = create_feature_string(movie_df)\n",
        "\n",
        "# ==============================\n",
        "# Split Data\n",
        "# ==============================\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# ==============================\n",
        "# Vectorize\n",
        "# ==============================\n",
        "tfidf_matrix_tfidf, _ = vectorize_features(movie_df['cbf_features'], method='tfidf')\n",
        "count_matrix_count, _ = vectorize_features(movie_df['cbf_features'], method='count')\n",
        "binary_matrix_binary, _ = binary_vectorize(movie_df['cbf_features'])\n",
        "sim_matrix_binary_jaccard = jaccard_pairwise_parallel(binary_matrix_binary)\n",
        "\n",
        "# ==============================\n",
        "# Recommender Functions\n",
        "# ==============================\n",
        "\n",
        "def recommend_from_similarity_matrix(user_id, ratings, sim_matrix, movie_df, model_label, top_n=50):\n",
        "    user_rated = ratings[ratings['userId'] == user_id]\n",
        "    user_seen_ids = user_rated['movieId'].tolist()\n",
        "\n",
        "    movie_id_to_idx = {mid: idx for idx, mid in enumerate(movie_df['movieId'])}\n",
        "    idx_to_movie_id = {idx: mid for idx, mid in enumerate(movie_df['movieId'])}\n",
        "\n",
        "    seen_indices = [movie_id_to_idx[mid] for mid in user_seen_ids if mid in movie_id_to_idx]\n",
        "    if not seen_indices:\n",
        "        return pd.DataFrame(columns=['movieId', 'title', 'score'])\n",
        "\n",
        "    similarity_scores = sim_matrix[seen_indices].mean(axis=0)\n",
        "    similarity_scores[seen_indices] = -1\n",
        "\n",
        "    top_indices = np.argsort(similarity_scores)[-top_n:][::-1]\n",
        "    top_movie_ids = [idx_to_movie_id[i] for i in top_indices]\n",
        "\n",
        "    recs = movie_df[movie_df['movieId'].isin(top_movie_ids)][['movieId', 'title']].copy()\n",
        "    recs['score'] = similarity_scores[top_indices]\n",
        "    return recs\n",
        "\n",
        "# ==============================\n",
        "# Predict + Evaluate\n",
        "# ==============================\n",
        "def predict_and_evaluate(model_label, user_id, sim_matrix=None, feature_matrix=None):\n",
        "    if sim_matrix is not None:\n",
        "        recs = recommend_from_similarity_matrix(user_id, train_ratings, sim_matrix, movie_df, model_label, top_n=50)\n",
        "    else:\n",
        "        recs = recommend_from_profile(user_id, train_ratings, feature_matrix, movie_df, model_label, top_n=50)\n",
        "\n",
        "    merged = pd.merge(recs, test_ratings[test_ratings['userId'] == user_id][['movieId', 'rating']],\n",
        "                      on='movieId', how='inner')\n",
        "    merged['userId'] = user_id\n",
        "    merged.rename(columns={'rating': 'true_rating', 'score': 'pred_rating'}, inplace=True)\n",
        "\n",
        "    if not merged.empty:\n",
        "        rmse = np.sqrt(mean_squared_error(merged['true_rating'], merged['pred_rating']))\n",
        "        mae = mean_absolute_error(merged['true_rating'], merged['pred_rating'])\n",
        "    else:\n",
        "        rmse = np.nan\n",
        "        mae = np.nan\n",
        "\n",
        "    merged['model'] = model_label\n",
        "    return merged[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']], rmse, mae\n",
        "\n",
        "# ==============================\n",
        "# Unified Evaluation with Single Progress Bar\n",
        "# ==============================\n",
        "def evaluate_all_models(test_ratings, train_ratings, movie_df, tfidf_matrix, count_matrix, jaccard_matrix, n_jobs=-1):\n",
        "    user_ids = test_ratings['userId'].unique()\n",
        "    models = [\n",
        "        (\"TF-IDF + Cosine\", tfidf_matrix, None),\n",
        "        (\"Count + Cosine\", count_matrix, None),\n",
        "        (\"Binary + Jaccard\", None, jaccard_matrix)\n",
        "    ]\n",
        "\n",
        "    tasks = []\n",
        "    for user_id in user_ids:\n",
        "        for model_label, feature_matrix, sim_matrix in models:\n",
        "            tasks.append((user_id, model_label, feature_matrix, sim_matrix))\n",
        "\n",
        "    def evaluate_user(user_id, model_label, feature_matrix, sim_matrix):\n",
        "        try:\n",
        "            df, rmse, mae = predict_and_evaluate(\n",
        "                model_label=model_label,\n",
        "                user_id=user_id,\n",
        "                sim_matrix=sim_matrix,\n",
        "                feature_matrix=feature_matrix\n",
        "            )\n",
        "            return df, rmse, mae, model_label\n",
        "        except:\n",
        "            return None, np.nan, np.nan, model_label\n",
        "\n",
        "    results = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(evaluate_user)(uid, model_label, feature_matrix, sim_matrix)\n",
        "        for uid, model_label, feature_matrix, sim_matrix in tqdm(tasks, desc=\"Evaluating all models\")\n",
        "    )\n",
        "\n",
        "    summary = {}\n",
        "    for df, rmse, mae, model_label in results:\n",
        "        if model_label not in summary:\n",
        "            summary[model_label] = {'dfs': [], 'rmses': [], 'maes': []}\n",
        "        if df is not None:\n",
        "            summary[model_label]['dfs'].append(df)\n",
        "        if not np.isnan(rmse):\n",
        "            summary[model_label]['rmses'].append(rmse)\n",
        "            summary[model_label]['maes'].append(mae)\n",
        "\n",
        "    final_results = {}\n",
        "    for model_label, content in summary.items():\n",
        "        all_df = pd.concat(content['dfs'], ignore_index=True) if content['dfs'] else pd.DataFrame()\n",
        "        avg_rmse = np.mean(content['rmses']) if content['rmses'] else np.nan\n",
        "        avg_mae = np.mean(content['maes']) if content['maes'] else np.nan\n",
        "        final_results[model_label] = (all_df, avg_rmse, avg_mae)\n",
        "\n",
        "    return final_results\n",
        "\n",
        "# ==============================\n",
        "# Run All Evaluations\n",
        "# ==============================\n",
        "results = evaluate_all_models(\n",
        "    test_ratings=test_ratings,\n",
        "    train_ratings=train_ratings,\n",
        "    movie_df=movie_df,\n",
        "    tfidf_matrix=tfidf_matrix_tfidf,\n",
        "    count_matrix=count_matrix_count,\n",
        "    jaccard_matrix=sim_matrix_binary_jaccard\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# Save Outputs\n",
        "# ==============================\n",
        "output_dir = \"cbf_outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for model_name, (df, rmse, mae) in results.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"  RMSE: {rmse:.4f}\" if not np.isnan(rmse) else \"  RMSE: N/A\")\n",
        "    print(f\"  MAE : {mae:.4f}\" if not np.isnan(mae) else \"  MAE : N/A\")\n",
        "\n",
        "    filename = model_name.lower().replace(\" + \", \"_\").replace(\" \", \"_\") + \"_predictions.csv\"\n",
        "    df.to_csv(os.path.join(output_dir, filename), index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KBKA0t2-F9X",
        "outputId": "7ce42845-2106-47e6-9390-8b378f48ea0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3883 movies.\n",
            "Loaded 1000209 ratings and 6040 users.\n",
            "TFIDF vectorization complete. Shape: (3883, 33433)\n",
            "COUNT vectorization complete. Shape: (3883, 33433)\n",
            "Binary Count vectorization complete. Shape: (3883, 33433)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating all models: 100%|██████████| 18114/18114 [09:03<00:00, 33.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: TF-IDF + Cosine\n",
            "  RMSE: 3.7476\n",
            "  MAE : 3.6947\n",
            "\n",
            "Model: Count + Cosine\n",
            "  RMSE: 3.4250\n",
            "  MAE : 3.3762\n",
            "\n",
            "Model: Binary + Jaccard\n",
            "  RMSE: 3.6354\n",
            "  MAE : 3.5767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(movie_df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "q2zAPpkFY3Is",
        "outputId": "d40b21f3-8fc5-4a11-f70d-a61b8f9c7849",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movieId', 'title', 'genres', 'year', 'clean_title', 'tmdb_id', 'overview', 'poster_path', 'backdrop_path', 'vote_average', 'vote_count', 'tmdb_genres', 'cast', 'director', 'keywords', 'trailer_link']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 8: Content-Based Similarity Recommendations (Multi-Model)\n",
        "# ==============================\n",
        "\n",
        "# ==============================\n",
        "# Step 1: Load Movie & User Data\n",
        "# ==============================\n",
        "\n",
        "movie_df = load_movie_data(\"movies_enriched_full.csv\")\n",
        "ratings, users = load_user_data(\"ratings.dat\", \"users.dat\")\n",
        "\n",
        "# Remove redundant columns if they exist\n",
        "movie_df.drop(columns=['trailer_link_y', 'backdrop_path_y', 'poster_path_y'], errors='ignore', inplace=True)\n",
        "\n",
        "# Recreate CBF Features\n",
        "movie_df = create_feature_string(movie_df)\n",
        "\n",
        "# ==============================\n",
        "# Step 2: Vectorize & Compute Similarities\n",
        "# ==============================\n",
        "\n",
        "# --- TF-IDF + Cosine ---\n",
        "tfidf_matrix_tfidf, vectorizer_tfidf = vectorize_features(movie_df['cbf_features'], method='tfidf')\n",
        "sim_matrix_tfidf_cosine = compute_cosine_similarity(tfidf_matrix_tfidf)\n",
        "\n",
        "# --- Count + Cosine ---\n",
        "count_matrix_count, vectorizer_count = vectorize_features(movie_df['cbf_features'], method='count')\n",
        "sim_matrix_count_cosine = compute_cosine_similarity(count_matrix_count)\n",
        "\n",
        "# --- Binary + Jaccard ---\n",
        "binary_matrix_binary, vectorizer_binary = binary_vectorize(movie_df['cbf_features'])\n",
        "sim_matrix_binary_jaccard = jaccard_pairwise_parallel(binary_matrix_binary)\n",
        "\n",
        "# ==============================\n",
        "# Step 3: Recommendation Functions\n",
        "# ==============================\n",
        "\n",
        "def recommend_from_similarity_matrix(user_id, ratings, sim_matrix, movie_df, model_label, top_n=50):\n",
        "    seen_movie_ids = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    seen_indices = movie_df[movie_df['movieId'].isin(seen_movie_ids)].index.tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(seen_movie_ids)].index.tolist()\n",
        "\n",
        "    if not seen_indices:\n",
        "        print(f\"No ratings found for user {user_id}.\")\n",
        "        return pd.DataFrame(columns=['movieId', 'title', 'score', 'model'])\n",
        "\n",
        "    mean_sims = sim_matrix[unseen_indices][:, seen_indices].mean(axis=1)\n",
        "    top_indices = np.argsort(mean_sims)[-top_n:][::-1]\n",
        "    top_movie_indices = np.array(unseen_indices)[top_indices]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'movieId': movie_df.iloc[top_movie_indices]['movieId'].values,\n",
        "        'title': movie_df.iloc[top_movie_indices]['title'].values,\n",
        "        'score': mean_sims[top_indices],\n",
        "        'model': model_label\n",
        "    })\n",
        "\n",
        "def recommend_from_profile(user_id, ratings, tfidf_matrix, movie_df, model_label, top_n=50):\n",
        "    user_profile = build_user_profile(user_id, ratings, tfidf_matrix, movie_df)\n",
        "    sims = cosine_similarity(user_profile, tfidf_matrix).flatten()\n",
        "    user_seen = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(user_seen)].index\n",
        "    top_indices = unseen_indices[np.argsort(sims[unseen_indices])[-top_n:][::-1]]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'movieId': movie_df.iloc[top_indices]['movieId'].values,\n",
        "        'title': movie_df.iloc[top_indices]['title'].values,\n",
        "        'score': sims[top_indices],\n",
        "        'model': model_label\n",
        "    })\n",
        "\n",
        "# ==============================\n",
        "# Step 4: Generate Recommendations (Labeled Outputs)\n",
        "# ==============================\n",
        "\n",
        "user_id = 5549\n",
        "\n",
        "df_tfidf_cosine = recommend_from_profile(\n",
        "    user_id, ratings, tfidf_matrix_tfidf, movie_df,\n",
        "    model_label='TF-IDF + Cosine', top_n=50\n",
        ")\n",
        "\n",
        "df_count_cosine = recommend_from_profile(\n",
        "    user_id, ratings, count_matrix_count, movie_df,\n",
        "    model_label='Count + Cosine', top_n=50\n",
        ")\n",
        "\n",
        "df_binary_jaccard = recommend_from_similarity_matrix(\n",
        "    user_id, ratings, sim_matrix_binary_jaccard, movie_df,\n",
        "    model_label='Binary + Jaccard', top_n=50\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# Step 5: Combine All Model Outputs\n",
        "# ==============================\n",
        "\n",
        "all_recommendations_combined = pd.concat([\n",
        "    df_tfidf_cosine,\n",
        "    df_count_cosine,\n",
        "    df_binary_jaccard\n",
        "], ignore_index=True)\n",
        "\n",
        "print(\"Recommendation generation complete. Combined shape:\", all_recommendations_combined.shape)\n",
        "print(\"\\nRecommendation generation complete. Combined shape:\\n\", all_recommendations_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "ClwOydOIOtnH",
        "outputId": "1375be99-b273-4d34-ee8f-c0569f2a72af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3883 movies.\n",
            "Loaded 1000209 ratings and 6040 users.\n",
            "TFIDF vectorization complete. Shape: (3883, 33433)\n",
            "Cosine similarity computed.\n",
            "COUNT vectorization complete. Shape: (3883, 33433)\n",
            "Cosine similarity computed.\n",
            "Binary Count vectorization complete. Shape: (3883, 33433)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-458685730.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# --- Binary + Jaccard ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mbinary_matrix_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_binary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cbf_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0msim_matrix_binary_jaccard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaccard_pairwise_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_matrix_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# ==============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-779470500.py\u001b[0m in \u001b[0;36mjaccard_pairwise_parallel\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mrow_sums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# This is already a 1D NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_sums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrow_sums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mintersection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mjaccard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersection\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0m\u001b[1;32m     48\u001b[0m          initial=_NoValue, where=True):\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 8: Content-Based Similarity Recommendations (Multi-Model)\n",
        "# ==============================\n",
        "\n",
        "# ==============================\n",
        "# Step 1: Load Movie & User Data\n",
        "# ==============================\n",
        "movie_df = load_movie_data(\"movies_enriched_full.csv\")\n",
        "ratings, users = load_user_data(\"ratings.dat\", \"users.dat\")\n",
        "\n",
        "movie_df.drop(columns=['trailer_link_y', 'backdrop_path_y', 'poster_path_y'], errors='ignore', inplace=True)\n",
        "movie_df = create_feature_string(movie_df)\n",
        "\n",
        "# ==============================\n",
        "# Step 2: Assume Precomputed Models Exist (Do NOT re-vectorize here)\n",
        "# ==============================\n",
        "# These must be preloaded or passed from a previous cell/module:\n",
        "# tfidf_matrix_tfidf, count_matrix_count, sim_matrix_binary_jaccard\n",
        "\n",
        "# ==============================\n",
        "# Step 3: Recommendation Functions\n",
        "# ==============================\n",
        "\n",
        "def recommend_from_similarity_matrix(user_id, ratings, sim_matrix, movie_df, model_label, top_n=50):\n",
        "    user_rated = ratings[ratings['userId'] == user_id]\n",
        "    seen_ids = user_rated['movieId'].tolist()\n",
        "\n",
        "    movie_id_to_idx = {mid: idx for idx, mid in enumerate(movie_df['movieId'])}\n",
        "    idx_to_movie_id = {idx: mid for idx, mid in enumerate(movie_df['movieId'])}\n",
        "\n",
        "    seen_indices = [movie_id_to_idx[mid] for mid in seen_ids if mid in movie_id_to_idx]\n",
        "    unseen_indices = [i for i in range(len(movie_df)) if i not in seen_indices]\n",
        "\n",
        "    if not seen_indices:\n",
        "        print(f\"No ratings found for user {user_id}\")\n",
        "        return pd.DataFrame(columns=['movieId', 'title', 'score', 'model'])\n",
        "\n",
        "    mean_sims = sim_matrix[unseen_indices][:, seen_indices].mean(axis=1)\n",
        "    top_indices = np.argsort(mean_sims)[-top_n:][::-1]\n",
        "    top_movie_indices = np.array(unseen_indices)[top_indices]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'movieId': movie_df.iloc[top_movie_indices]['movieId'].values,\n",
        "        'title': movie_df.iloc[top_movie_indices]['title'].values,\n",
        "        'score': mean_sims[top_indices],\n",
        "        'model': model_label\n",
        "    })\n",
        "\n",
        "def recommend_from_profile(user_id, ratings, feature_matrix, movie_df, model_label, top_n=50):\n",
        "    user_profile = build_user_profile(user_id, ratings, feature_matrix, movie_df)\n",
        "    sims = cosine_similarity(user_profile, feature_matrix).flatten()\n",
        "    seen_ids = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(seen_ids)].index\n",
        "\n",
        "    top_indices = unseen_indices[np.argsort(sims[unseen_indices])[-top_n:][::-1]]\n",
        "    return pd.DataFrame({\n",
        "        'movieId': movie_df.iloc[top_indices]['movieId'].values,\n",
        "        'title': movie_df.iloc[top_indices]['title'].values,\n",
        "        'score': sims[top_indices],\n",
        "        'model': model_label\n",
        "    })\n",
        "\n",
        "# ==============================\n",
        "# Step 4: Generate Recommendations (Use Pretrained Models)\n",
        "# ==============================\n",
        "\n",
        "user_id = 5549\n",
        "\n",
        "df_tfidf_cosine = recommend_from_profile(\n",
        "    user_id=user_id,\n",
        "    ratings=ratings,\n",
        "    feature_matrix=tfidf_matrix_tfidf,\n",
        "    movie_df=movie_df,\n",
        "    model_label='TF-IDF + Cosine',\n",
        "    top_n=50\n",
        ")\n",
        "\n",
        "df_count_cosine = recommend_from_profile(\n",
        "    user_id=user_id,\n",
        "    ratings=ratings,\n",
        "    feature_matrix=count_matrix_count,\n",
        "    movie_df=movie_df,\n",
        "    model_label='Count + Cosine',\n",
        "    top_n=50\n",
        ")\n",
        "\n",
        "df_binary_jaccard = recommend_from_similarity_matrix(\n",
        "    user_id=user_id,\n",
        "    ratings=ratings,\n",
        "    sim_matrix=sim_matrix_binary_jaccard,\n",
        "    movie_df=movie_df,\n",
        "    model_label='Binary + Jaccard',\n",
        "    top_n=50\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# Step 5: Combine and Output\n",
        "# ==============================\n",
        "all_recommendations_combined = pd.concat([\n",
        "    df_tfidf_cosine,\n",
        "    df_count_cosine,\n",
        "    df_binary_jaccard\n",
        "], ignore_index=True)\n",
        "\n",
        "print(\"Recommendation generation complete. Combined shape:\", all_recommendations_combined.shape)\n",
        "print(all_recommendations_combined.head(10))\n"
      ],
      "metadata": {
        "id": "F5oT-8zYdAAF",
        "outputId": "f686a779-eeea-4c24-cddb-67bbe73c5095",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3883 movies.\n",
            "Loaded 1000209 ratings and 6040 users.\n",
            "Recommendation generation complete. Combined shape: (150, 4)\n",
            "   movieId                                                     title  \\\n",
            "0      293  Professional, The (a.k.a. Leon: The Professional) (1994)   \n",
            "1     3540                                    Passion of Mind (1999)   \n",
            "2     2845                                         White Boys (1999)   \n",
            "3     3902                  Goya in Bordeaux (Goya en Bodeos) (1999)   \n",
            "4     3568                      Smiling Fish and Goat on Fire (1999)   \n",
            "5     2564                                  Empty Mirror, The (1999)   \n",
            "6     3457                                    Waking the Dead (1999)   \n",
            "7     3867                All the Rage (a.k.a. It's the Rage) (1999)   \n",
            "8     3907                        Prince of Central Park, The (1999)   \n",
            "9     1221                            Godfather: Part II, The (1974)   \n",
            "\n",
            "      score            model  \n",
            "0  0.217338  TF-IDF + Cosine  \n",
            "1  0.215956  TF-IDF + Cosine  \n",
            "2  0.214411  TF-IDF + Cosine  \n",
            "3  0.214411  TF-IDF + Cosine  \n",
            "4  0.214411  TF-IDF + Cosine  \n",
            "5  0.214411  TF-IDF + Cosine  \n",
            "6  0.214411  TF-IDF + Cosine  \n",
            "7  0.214411  TF-IDF + Cosine  \n",
            "8  0.214411  TF-IDF + Cosine  \n",
            "9  0.188468  TF-IDF + Cosine  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Memory-based collaborative filtering module (UBCF, IBCF)**\n",
        "\n",
        "***Purpose:***\n",
        "\n",
        "This module implements **memory-based collaborative filtering** using **user-user** or **item-item** similarity. It addresses **user bias** by normalizing ratings through mean-centering and optionally **rescaling predictions** to the original rating scale for interpretability.\n",
        "\n",
        "***Methodology:***\n",
        "\n",
        "1. **Rating Matrix Construction**:\n",
        "\n",
        "   * A user-item matrix is built from raw MovieLens-style ratings data.\n",
        "   * For `kind='user'`, ratings are mean-centered per user to reduce bias from lenient or strict raters.\n",
        "   * For `kind='item'`, raw ratings are used directly (no normalization), as the algorithm focuses on item similarities based on a single user's input.\n",
        "\n",
        "2. **Similarity Computation**:\n",
        "\n",
        "   * Cosine similarity is computed either:\n",
        "\n",
        "     * **Across users** for user-based CF (`kind='user'`)\n",
        "     * **Across items** for item-based CF (`kind='item'`)\n",
        "   * `sklearn.metrics.pairwise_distances` is used to derive similarity as `1 - cosine_distance`.\n",
        "\n",
        "3. **Prediction Generation**:\n",
        "\n",
        "   * For **user-based CF**:\n",
        "\n",
        "     * Ratings from similar users are weighted by similarity and averaged.\n",
        "     * The user’s mean rating is **added back** to restore predictions to the original scale (e.g., 1–5).\n",
        "   * For **item-based CF**:\n",
        "\n",
        "     * A user’s own ratings are used to compute scores for similar items.\n",
        "     * No mean is added back, since predictions are already on the correct scale.\n",
        "\n",
        "4. **Top-N Recommendations**:\n",
        "\n",
        "   * The system filters out movies the user has already rated.\n",
        "   * It ranks unseen movies by predicted score and returns the top-N recommendations.\n",
        "   * Each recommendation is labeled with the model type (`User-Based CF` or `Item-Based CF`) for downstream tracking."
      ],
      "metadata": {
        "id": "omg4Y6k5XmSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==============================\n",
        "# Step 1: Load & Split Ratings\n",
        "# ==============================\n",
        "\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# ==============================\n",
        "# Step 2: Create User-Item Matrix from TRAIN only\n",
        "# ==============================\n",
        "\n",
        "def create_normalized_user_item_matrix(ratings_df):\n",
        "    user_item = ratings_df.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
        "    user_means = user_item.replace(0, np.NaN).mean(axis=1)\n",
        "    normalized = user_item.sub(user_means, axis=0).fillna(0)\n",
        "    return normalized, user_means\n",
        "\n",
        "user_item_matrix, user_means = create_normalized_user_item_matrix(train_ratings)\n",
        "\n",
        "# ==============================\n",
        "# Step 3: Similarity Computation\n",
        "# ==============================\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_similarity(matrix, kind='user'):\n",
        "    if kind == 'user':\n",
        "        return cosine_similarity(matrix)\n",
        "    elif kind == 'item':\n",
        "        return cosine_similarity(matrix.T)\n",
        "    else:\n",
        "        raise ValueError(\"kind must be 'user' or 'item'\")\n",
        "\n",
        "user_sim_matrix = compute_similarity(user_item_matrix, kind='user')\n",
        "item_sim_matrix = compute_similarity(user_item_matrix, kind='item')\n",
        "\n",
        "# ==============================\n",
        "# Step 4: Memory-Based Prediction Function\n",
        "# ==============================\n",
        "\n",
        "def recommend_memory_based(user_id, user_item_matrix, user_means, sim_matrix, kind='user', top_n=50):\n",
        "    if kind == 'user':\n",
        "        sim_scores = sim_matrix[user_item_matrix.index.get_loc(user_id)]\n",
        "        weighted_sum = sim_scores @ user_item_matrix.values\n",
        "        sim_sums = np.abs(sim_scores).sum()\n",
        "        pred_ratings = weighted_sum / sim_sums if sim_sums > 0 else weighted_sum\n",
        "        pred_ratings = pred_ratings + user_means[user_id]\n",
        "        pred_series = pd.Series(pred_ratings, index=user_item_matrix.columns)\n",
        "    else:\n",
        "        user_ratings = user_item_matrix.loc[user_id].values\n",
        "        weighted_sum = user_ratings @ sim_matrix\n",
        "        sim_sums = np.abs(sim_matrix).sum(axis=0)\n",
        "        pred_ratings = weighted_sum / sim_sums\n",
        "        pred_ratings = pred_ratings + user_means[user_id]\n",
        "        pred_series = pd.Series(pred_ratings, index=user_item_matrix.columns)\n",
        "\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    pred_series = pred_series.drop(labels=seen, errors='ignore')\n",
        "    top_preds = pred_series.sort_values(ascending=False).head(top_n)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'userId': user_id,\n",
        "        'movieId': top_preds.index,\n",
        "        'score': top_preds.values\n",
        "    })\n",
        "\n",
        "# ==============================\n",
        "# Step 5: Evaluate on TEST Ratings Only\n",
        "# ==============================\n",
        "\n",
        "def evaluate_cf_model(test_ratings, user_item_matrix, user_means, sim_matrix, kind, label):\n",
        "    test_users = test_ratings['userId'].unique()\n",
        "    predictions = []\n",
        "\n",
        "    for uid in test_users:\n",
        "        if uid not in user_item_matrix.index:\n",
        "            continue\n",
        "        recs = recommend_memory_based(uid, user_item_matrix, user_means, sim_matrix, kind=kind, top_n=1000)\n",
        "        merged = pd.merge(recs, test_ratings[test_ratings['userId'] == uid], on='movieId')\n",
        "        predictions.append(merged)\n",
        "\n",
        "    all_preds = pd.concat(predictions, ignore_index=True)\n",
        "    if all_preds.empty:\n",
        "        rmse, mae = np.nan, np.nan\n",
        "    else:\n",
        "        rmse = np.sqrt(mean_squared_error(all_preds['rating'], all_preds['score']))\n",
        "        mae = mean_absolute_error(all_preds['rating'], all_preds['score'])\n",
        "\n",
        "    print(f\"\\nModel: {label}\")\n",
        "    print(f\"  RMSE: {rmse:.4f}\" if not np.isnan(rmse) else \"  RMSE: N/A\")\n",
        "    print(f\"  MAE : {mae:.4f}\" if not np.isnan(mae) else \"  MAE : N/A\")\n",
        "\n",
        "    return all_preds, rmse, mae\n",
        "\n",
        "# ==============================\n",
        "# Step 6: Run Evaluation\n",
        "# ==============================\n",
        "\n",
        "user_cf_preds, user_cf_rmse, user_cf_mae = evaluate_cf_model(\n",
        "    test_ratings, user_item_matrix, user_means, user_sim_matrix,\n",
        "    kind='user', label='User-Based CF'\n",
        ")\n",
        "\n",
        "item_cf_preds, item_cf_rmse, item_cf_mae = evaluate_cf_model(\n",
        "    test_ratings, user_item_matrix, user_means, item_sim_matrix,\n",
        "    kind='item', label='Item-Based CF'\n",
        ")\n"
      ],
      "metadata": {
        "id": "iZIG6Zt3fgxk",
        "outputId": "050c2330-28a2-4ab6-af37-4672db80a0fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: User-Based CF\n",
            "  RMSE: 3.4663\n",
            "  MAE : 3.3226\n",
            "\n",
            "Model: Item-Based CF\n",
            "  RMSE: 3.5660\n",
            "  MAE : 3.3878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Application of UBCF and IBCF***"
      ],
      "metadata": {
        "id": "FFUDwqVQXqC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Step 1: Create Bias-Normalized Matrix\n",
        "# ==============================\n",
        "\n",
        "user_item_matrix, user_means = create_normalized_user_item_matrix(ratings)\n",
        "\n",
        "# ==============================\n",
        "# Step 2: Compute Similarity Matrices\n",
        "# ==============================\n",
        "\n",
        "user_sim_matrix = compute_similarity(user_item_matrix, kind='user')\n",
        "item_sim_matrix = compute_similarity(user_item_matrix, kind='item')\n",
        "\n",
        "# ==============================\n",
        "# Step 3: Generate Recommendations\n",
        "# ==============================\n",
        "\n",
        "user_cf_recs = recommend_memory_based(\n",
        "    5549,                    # user_id\n",
        "    user_item_matrix,\n",
        "    user_means,\n",
        "    user_sim_matrix,\n",
        "    kind='user',\n",
        "    top_n=50\n",
        ")\n",
        "\n",
        "item_cf_recs = recommend_memory_based(\n",
        "    5549,                    # user_id\n",
        "    user_item_matrix,\n",
        "    user_means,\n",
        "    item_sim_matrix,\n",
        "    kind='item',\n",
        "    top_n=50\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# Step 4: Merge with Movie Titles Only\n",
        "# ==============================\n",
        "\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]  # Retain only movieId and title\n",
        "user_cf_recs = user_cf_recs.merge(movies, on=\"movieId\", how=\"left\")\n",
        "item_cf_recs = item_cf_recs.merge(movies, on=\"movieId\", how=\"left\")\n",
        "\n",
        "# ==============================\n",
        "# Step 5: Display Output\n",
        "# ==============================\n",
        "\n",
        "print(\"\\nTop 50 User-Based CF Recommendations for User 5549:\")\n",
        "print(user_cf_recs[['movieId', 'title', 'score']].head())\n",
        "\n",
        "print(\"\\nTop 50 Item-Based CF Recommendations for User 5549:\")\n",
        "print(item_cf_recs[['movieId', 'title', 'score']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbI1OXxcXqfv",
        "outputId": "66fccf81-1d1e-4ced-a67e-72374783ccdb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 50 User-Based CF Recommendations for User 5549:\n",
            "   movieId                                                  title     score\n",
            "0      260              Star Wars: Episode IV - A New Hope (1977)  1.988588\n",
            "1     1196  Star Wars: Episode V - The Empire Strikes Back (1980)  1.906232\n",
            "2     1210      Star Wars: Episode VI - Return of the Jedi (1983)  1.706735\n",
            "3     2028                             Saving Private Ryan (1998)  1.692017\n",
            "4     1198                         Raiders of the Lost Ark (1981)  1.645312\n",
            "\n",
            "Top 50 Item-Based CF Recommendations for User 5549:\n",
            "   movieId                                                  title     score\n",
            "0      858                                  Godfather, The (1972)  0.031831\n",
            "1     1198                         Raiders of the Lost Ark (1981)  0.031699\n",
            "2      260              Star Wars: Episode IV - A New Hope (1977)  0.031651\n",
            "3     1196  Star Wars: Episode V - The Empire Strikes Back (1980)  0.031443\n",
            "4      608                                           Fargo (1996)  0.030868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid CBF and UBCF Model"
      ],
      "metadata": {
        "id": "zPb1m8iNIdAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Hybrid Recommender: UBCF + CBF\n",
        "# ==============================\n",
        "\n",
        "def hybrid_ubcf_cbf(user_id, user_item_matrix, user_means, user_sim_matrix,\n",
        "                    tfidf_matrix, ratings, movie_df,\n",
        "                    w_cf=0.2, w_cbf=0.8, top_n=50):\n",
        "    \"\"\"\n",
        "    Combine UBCF and CBF scores via weighted average.\n",
        "\n",
        "    Parameters:\n",
        "    - user_id: int\n",
        "    - user_item_matrix: pd.DataFrame (mean-centered matrix)\n",
        "    - user_means: pd.Series\n",
        "    - user_sim_matrix: np.array\n",
        "    - tfidf_matrix: sparse matrix from TF-IDF\n",
        "    - ratings: pd.DataFrame\n",
        "    - movie_df: pd.DataFrame with movieId, title\n",
        "    - w_cf: float, weight for UBCF\n",
        "    - w_cbf: float, weight for CBF\n",
        "    - top_n: int\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with movieId, title, hybrid_score, model\n",
        "    \"\"\"\n",
        "    # --- UBCF predictions ---\n",
        "    ubcf_df = recommend_memory_based(\n",
        "        user_id=user_id,\n",
        "        user_item_matrix=user_item_matrix,\n",
        "        user_means=user_means,\n",
        "        similarity_matrix=user_sim_matrix,\n",
        "        kind='user',\n",
        "        top_n=1000  # keep more to allow intersection\n",
        "    )\n",
        "\n",
        "    # --- CBF predictions ---\n",
        "    cbf_df = recommend_from_profile(\n",
        "        user_id=user_id,\n",
        "        ratings=ratings,\n",
        "        tfidf_matrix=tfidf_matrix,\n",
        "        movie_df=movie_df,\n",
        "        model_label='CBF',  # temporary label\n",
        "        top_n=1000\n",
        "    ).rename(columns={'score': 'cbf_score'})\n",
        "\n",
        "    # --- Merge ---\n",
        "    ubcf_df = ubcf_df.rename(columns={'score': 'ubcf_score'})\n",
        "    merged = pd.merge(ubcf_df, cbf_df, on='movieId')\n",
        "\n",
        "    # --- Combine Scores ---\n",
        "    merged['hybrid_score'] = w_cf * merged['ubcf_score'] + w_cbf * merged['cbf_score']\n",
        "    hybrid_df = merged[['movieId', 'title', 'hybrid_score']].copy()\n",
        "    hybrid_df['model'] = 'Hybrid (UBCF + CBF)'\n",
        "\n",
        "    return hybrid_df.sort_values(by='hybrid_score', ascending=False).head(top_n)[\n",
        "        ['movieId', 'title', 'hybrid_score', 'model']\n",
        "    ]\n",
        "\n",
        "# ==============================\n",
        "# Generate Hybrid Recommendations for User 5549\n",
        "# ==============================\n",
        "\n",
        "hybrid_recs = hybrid_ubcf_cbf(\n",
        "    user_id=5549,\n",
        "    user_item_matrix=user_item_matrix,\n",
        "    user_means=user_means,\n",
        "    user_sim_matrix=user_sim_matrix,\n",
        "    tfidf_matrix=tfidf_matrix_tfidf,\n",
        "    ratings=ratings,\n",
        "    movie_df=movie_df,\n",
        "    w_cf=0.5,\n",
        "    w_cbf=0.5,\n",
        "    top_n=50\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# Display Output\n",
        "# ==============================\n",
        "\n",
        "print(\"\\nTop 50 Hybrid Recommendations for User 5549:\")\n",
        "print(hybrid_recs.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "rqR9UEVpIlUW",
        "outputId": "c35fce0d-c85a-458d-9f08-e7303578232e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "recommend_memory_based() got an unexpected keyword argument 'similarity_matrix'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-921147464.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# ==============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m hybrid_recs = hybrid_ubcf_cbf(\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5549\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0muser_item_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_item_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-8-921147464.py\u001b[0m in \u001b[0;36mhybrid_ubcf_cbf\u001b[0;34m(user_id, user_item_matrix, user_means, user_sim_matrix, tfidf_matrix, ratings, movie_df, w_cf, w_cbf, top_n)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# --- UBCF predictions ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     ubcf_df = recommend_memory_based(\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0muser_item_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_item_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: recommend_memory_based() got an unexpected keyword argument 'similarity_matrix'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model-Based Filtering:**\n",
        "\n",
        "  * *SVD (Surprise)*: Learns latent features from the rating matrix.\n",
        "  * *ALS (PySpark)*: Scalable factorization method for large datasets.\n"
      ],
      "metadata": {
        "id": "R6d3SP0OolH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 9: Model-Based Collaborative Filtering (SVD using Surprise)**\n",
        "\n",
        "**Purpose:**\n",
        "Use matrix factorization (SVD) to learn latent user/item features from the rating matrix.\n",
        "\n",
        "**Application:**\n",
        "- Accurate, scalable recommendations for sparse datasets using user/item embeddings.\n",
        "- Suitable for small to medium datasets.\n",
        "- Optimized via `GridSearchCV` for hyperparameter tuning.\n",
        "- Good interpretability of latent factors per user and item.\n",
        "\n"
      ],
      "metadata": {
        "id": "4dCPxrNMnQkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 9: Model-Based Collaborative Filtering (SVD using Surprise)\n",
        "# ==============================\n",
        "\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split, GridSearchCV\n",
        "from surprise.accuracy import rmse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================\n",
        "# Prepare Surprise Dataset\n",
        "# ==============================\n",
        "\n",
        "def prepare_surprise_data(ratings):\n",
        "    reader = Reader(rating_scale=(0.5, 5.0))\n",
        "    return Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# ==============================\n",
        "# Tune SVD Model with Grid Search\n",
        "# ==============================\n",
        "\n",
        "def tune_svd_model(data):\n",
        "    param_grid = {\n",
        "        'n_factors': [50, 100],\n",
        "        'lr_all': [0.005, 0.01],\n",
        "        'reg_all': [0.02, 0.1]\n",
        "    }\n",
        "    print(\"Tuning SVD model with GridSearchCV...\")\n",
        "    gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, joblib_verbose=0)\n",
        "\n",
        "    with tqdm(total=1, desc=\"GridSearchCV\") as pbar:\n",
        "        gs.fit(data)\n",
        "        pbar.update(1)\n",
        "\n",
        "    print(f\"Best RMSE: {gs.best_score['rmse']} with params: {gs.best_params['rmse']}\")\n",
        "    return gs.best_estimator['rmse']\n",
        "\n",
        "# ==============================\n",
        "# Train and Evaluate SVD\n",
        "# ==============================\n",
        "\n",
        "def evaluate_svd(model, data, model_label='SVD (Surprise)'):\n",
        "    trainset, testset = train_test_split(data, test_size=0.2)\n",
        "    model.fit(trainset)\n",
        "\n",
        "    print(\"Making predictions...\")\n",
        "    predictions = [model.predict(item[0], item[1], r_ui=item[2]) for item in tqdm(testset, desc=\"Predicting\")]\n",
        "\n",
        "    score = rmse(predictions)\n",
        "\n",
        "    pred_df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
        "    pred_df = pred_df.rename(columns={'uid': 'userId', 'iid': 'movieId', 'rui': 'true_rating', 'est': 'pred_rating'})\n",
        "    pred_df['model'] = model_label\n",
        "    return pred_df[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']], score\n",
        "\n",
        "# ==============================\n",
        "# Main Execution\n",
        "# ==============================\n",
        "\n",
        "# Step 1: Load ratings\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# Step 2: Prepare Surprise data\n",
        "data = prepare_surprise_data(ratings)\n",
        "\n",
        "# Step 3: Tune model\n",
        "best_svd_model = tune_svd_model(data)\n",
        "\n",
        "# Step 4: Evaluate model\n",
        "pred_df, rmse_score = evaluate_svd(best_svd_model, data)\n",
        "\n",
        "# Step 5: Output\n",
        "print(pred_df.head())\n",
        "print(f\"Final RMSE: {rmse_score:.4f}\")\n",
        "\n",
        "# Step 6: Top-50 Predictions for User 5549\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = [mid for mid in all_movie_ids if mid not in rated_movie_ids]\n",
        "\n",
        "print(f\"\\nGenerating predictions for User {target_user}...\")\n",
        "top_preds = [(movie_id, best_svd_model.predict(target_user, movie_id).est)\n",
        "             for movie_id in tqdm(unrated_movie_ids, desc=\"Predicting for user\")]\n",
        "\n",
        "top_50_df = pd.DataFrame(top_preds, columns=['movieId', 'pred_rating'])\n",
        "top_50_df = top_50_df.sort_values(by='pred_rating', ascending=False).head(50)\n",
        "top_50_df['userId'] = target_user\n",
        "top_50_df['model'] = 'SVD (Surprise)'\n",
        "top_50_df = top_50_df[['userId', 'movieId', 'pred_rating', 'model']]\n",
        "\n",
        "# Step 7: Merge with movie titles only\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_df = top_50_df.merge(movies, on='movieId', how='left')\n",
        "\n",
        "# Step 8: Final Output\n",
        "print(\"\\nTop 50 Recommendations for User 5549:\")\n",
        "print(top_50_df[['movieId', 'title', 'pred_rating']].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0JR9B0YokAB",
        "outputId": "5bb5afaf-13cd-4ce2-a414-4d1f04fff476"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning SVD model with GridSearchCV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GridSearchCV: 100%|██████████| 1/1 [04:39<00:00, 279.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE: 0.8828943502067386 with params: {'n_factors': 50, 'lr_all': 0.005, 'reg_all': 0.02}\n",
            "Making predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 200042/200042 [00:01<00:00, 128972.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.8721\n",
            "   userId  movieId  true_rating  pred_rating           model\n",
            "0    2238      247          5.0     3.960535  SVD (Surprise)\n",
            "1    3830     3071          4.0     4.024998  SVD (Surprise)\n",
            "2    4476     2916          1.0     3.121430  SVD (Surprise)\n",
            "3    5026       20          2.0     2.192730  SVD (Surprise)\n",
            "4    1671      357          4.0     4.027924  SVD (Surprise)\n",
            "Final RMSE: 0.8721\n",
            "\n",
            "Generating predictions for User 5549...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting for user: 100%|██████████| 3673/3673 [00:00<00:00, 210811.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 50 Recommendations for User 5549:\n",
            "   movieId  \\\n",
            "0     1148   \n",
            "1      905   \n",
            "2      922   \n",
            "3      912   \n",
            "4     1207   \n",
            "5     2019   \n",
            "6     3469   \n",
            "7     1900   \n",
            "8     3022   \n",
            "9      969   \n",
            "\n",
            "                                                                 title  \\\n",
            "0                                           Wrong Trousers, The (1993)   \n",
            "1                                         It Happened One Night (1934)   \n",
            "2                        Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)   \n",
            "3                                                    Casablanca (1942)   \n",
            "4                                         To Kill a Mockingbird (1962)   \n",
            "5  Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)   \n",
            "6                                              Inherit the Wind (1960)   \n",
            "7                   Children of Heaven, The (Bacheha-Ye Aseman) (1997)   \n",
            "8                                                  General, The (1927)   \n",
            "9                                            African Queen, The (1951)   \n",
            "\n",
            "   pred_rating  \n",
            "0     4.239833  \n",
            "1     4.163585  \n",
            "2     4.142996  \n",
            "3     4.118738  \n",
            "4     4.115441  \n",
            "5     4.105059  \n",
            "6     4.098444  \n",
            "7     4.093649  \n",
            "8     4.089820  \n",
            "9     4.085358  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model-Based Collaborative Filtering (ALS using PySpark)**\n",
        "\n",
        "**Purpose:**\n",
        "Use Alternating Least Squares (ALS) to learn latent user/item features at scale.\n",
        "\n",
        "**Application:**\n",
        "- Distributed recommendation system for large-scale datasets.\n",
        "- Runs on Apache Spark for horizontal scalability.\n",
        "- Handles sparsity well using factorization.\n",
        "- Suited for real-time, production-level systems with massive data.\n"
      ],
      "metadata": {
        "id": "DeCmPC_DnZip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 10: Model-Based Collaborative Filtering (ALS using PySpark)\n",
        "# ==============================\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import Row\n",
        "import pandas as pd\n",
        "\n",
        "# --- Start Spark Session ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ALSModel\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Load Ratings ---\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "ratings_df = spark.createDataFrame(ratings[['userId', 'movieId', 'rating']])\n",
        "\n",
        "# --- Train ALS Model ---\n",
        "als = ALS(\n",
        "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "    rank=10, maxIter=10, regParam=0.1,\n",
        "    coldStartStrategy=\"drop\", nonnegative=True\n",
        ")\n",
        "als_model = als.fit(ratings_df)\n",
        "\n",
        "# --- Evaluate ALS Model ---\n",
        "predictions = als_model.transform(ratings_df)\n",
        "pred_pd = predictions.select('userId', 'movieId', 'rating', 'prediction').toPandas()\n",
        "pred_pd = pred_pd.rename(columns={'rating': 'true_rating', 'prediction': 'pred_rating'})\n",
        "pred_pd['model'] = 'ALS (PySpark)'\n",
        "\n",
        "# --- Evaluate ALS RMSE ---\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName='rmse',\n",
        "    labelCol='rating',\n",
        "    predictionCol='prediction'\n",
        ")\n",
        "rmse_score = evaluator.evaluate(predictions)\n",
        "\n",
        "# --- Output Evaluation ---\n",
        "print(pred_pd[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']].head())\n",
        "print(f\"\\nFinal RMSE: {rmse_score:.4f}\")\n",
        "\n",
        "# ==============================\n",
        "# Step 6: Top-50 Predictions for User 5549\n",
        "# ==============================\n",
        "\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = list(set(all_movie_ids) - set(rated_movie_ids))\n",
        "\n",
        "# Create Spark DataFrame of userId + unrated movieId pairs\n",
        "user_unrated_pairs = spark.createDataFrame([Row(userId=target_user, movieId=int(mid)) for mid in unrated_movie_ids])\n",
        "\n",
        "# Predict ratings using ALS model\n",
        "print(f\"\\nGenerating Top-50 recommendations for User {target_user}...\")\n",
        "top_preds_df = als_model.transform(user_unrated_pairs).dropna()\n",
        "\n",
        "# Get top-50 highest predicted ratings\n",
        "top_50_preds = top_preds_df.orderBy(col(\"prediction\").desc()).limit(50)\n",
        "top_50_pd = top_50_preds.select(\"userId\", \"movieId\", \"prediction\").toPandas()\n",
        "top_50_pd['model'] = \"ALS (PySpark)\"\n",
        "top_50_pd = top_50_pd.rename(columns={'prediction': 'pred_rating'})\n",
        "\n",
        "# ==============================\n",
        "# Step 7: Merge with Movie Titles Only\n",
        "# ==============================\n",
        "\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_pd = top_50_pd.merge(movies, on='movieId', how='left')\n",
        "\n",
        "# ==============================\n",
        "# Step 8: Output Top-50\n",
        "# ==============================\n",
        "\n",
        "print(\"\\nTop 50 Recommendations for User 5549:\")\n",
        "print(top_50_pd[['movieId', 'title', 'pred_rating']].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLtk_Rsuom7R",
        "outputId": "5ae78569-65a9-4e28-ef1d-773016e71472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   userId  movieId  true_rating  pred_rating          model\n",
            "0     148     2122            4     2.768286  ALS (PySpark)\n",
            "1     148     2142            4     3.385553  ALS (PySpark)\n",
            "2     148     2366            5     3.547921  ALS (PySpark)\n",
            "3     148     3175            5     3.873491  ALS (PySpark)\n",
            "4     148     1580            4     4.024727  ALS (PySpark)\n",
            "\n",
            "Final RMSE: 0.8357\n",
            "\n",
            "Generating Top-50 recommendations for User 5549...\n",
            "\n",
            "Top 50 Recommendations for User 5549:\n",
            "   movieId                            title  pred_rating\n",
            "0      572           Foreign Student (1994)     5.054198\n",
            "1     1471               Boys Life 2 (1997)     4.919178\n",
            "2     2760  Gambler, The (A Játékos) (1997)     4.464029\n",
            "3      953     It's a Wonderful Life (1946)     4.359691\n",
            "4     1519            Broken English (1996)     4.351626\n",
            "5     2503          Apple, The (Sib) (1998)     4.329711\n",
            "6     2129     Saltmen of Tibet, The (1997)     4.324127\n",
            "7      912                Casablanca (1942)     4.317692\n",
            "8      751                   Careful (1992)     4.312503\n",
            "9     3365            Searchers, The (1956)     4.302980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ls6RqljaolUG"
      }
    }
  ]
}