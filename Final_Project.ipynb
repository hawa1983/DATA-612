{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-612/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose**\n",
        "\n",
        "The purpose of this script is to enrich the MovieLens movie dataset (`movies.dat`) with detailed movie metadata from The Movie Database (TMDB) API. This metadata includes movie overviews, genres, poster and backdrop image URLs, cast and director information, keywords, user ratings, and trailer links. The enriched dataset will serve as the foundation for building content-based, collaborative, and hybrid recommender systems.\n",
        "\n",
        "### **Methodology**\n",
        "\n",
        "1. **Load MovieLens Movie Data**\n",
        "   The script loads the `movies.dat` file, which contains basic movie information including `movieId`, `title`, and `genres`.\n",
        "\n",
        "2. **Clean Titles and Extract Years**\n",
        "   It processes the movie titles to remove the year from the title string and separately extracts the release year to improve search accuracy when querying TMDB.\n",
        "\n",
        "3. **Query TMDB API**\n",
        "   For each movie, it sends a search request to TMDB using the cleaned title and release year. If a match is found, it retrieves the movie’s TMDB ID.\n",
        "\n",
        "4. **Retrieve Detailed Metadata**\n",
        "   Using the TMDB ID, the script fetches:\n",
        "\n",
        "   * Overview (plot summary)\n",
        "   * Poster and backdrop image paths\n",
        "   * Genre IDs, which are then mapped to readable genre names\n",
        "   * Top 3 cast members\n",
        "   * Director(s)\n",
        "   * Associated keywords\n",
        "   * YouTube trailer link (if available)\n",
        "\n",
        "5. **Construct and Save Enriched Dataset**\n",
        "   All metadata is compiled into a structured format and merged with the original MovieLens data. The final dataset is saved as `movies_enriched_full.csv` for downstream use in recommendation models.\n"
      ],
      "metadata": {
        "id": "OptAJ12iBc8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qJ0maYGaxtJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ---------------------------------------\n",
        "# CONFIG\n",
        "# ---------------------------------------\n",
        "BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "IMAGE_BASE = \"https://image.tmdb.org/t/p/w500\"\n",
        "\n",
        "# Use your TMDB Bearer Token (v4)\n",
        "HEADERS = {\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyZGZlNjMwMGMzYjIzMjc2NzExNjQ0N2JhNzhiMjM5MyIsIm5iZiI6MTc1MTkyMjA3Ni4xMzUsInN1YiI6IjY4NmMzNTljMzc4NjllOGEyNDUxZTM0OSIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.S773ddH3FiIHtokPW4sYpJog0mXWS1o4OPov1KZneUw\"\n",
        "}\n",
        "\n",
        "# TMDB genre ID to name mapping\n",
        "GENRE_ID_TO_NAME = {\n",
        "    28: \"Action\", 12: \"Adventure\", 16: \"Animation\", 35: \"Comedy\", 80: \"Crime\",\n",
        "    99: \"Documentary\", 18: \"Drama\", 10751: \"Family\", 14: \"Fantasy\", 36: \"History\",\n",
        "    27: \"Horror\", 10402: \"Music\", 9648: \"Mystery\", 10749: \"Romance\", 878: \"Science Fiction\",\n",
        "    10770: \"TV Movie\", 53: \"Thriller\", 10752: \"War\", 37: \"Western\"\n",
        "}\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 1: Load MovieLens .dat Files\n",
        "# ---------------------------------------\n",
        "\n",
        "# Load movies.dat - format: MovieID::Title::Genres\n",
        "movies_df = pd.read_csv(\"movies.dat\", sep=\"::\", engine='python', header=None, names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 2: Clean Movie Titles and Extract Year\n",
        "# ---------------------------------------\n",
        "\n",
        "def extract_year(title):\n",
        "    if \"(\" in title:\n",
        "        try:\n",
        "            return int(title.strip()[-5:-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_title(title):\n",
        "    if \"(\" in title:\n",
        "        return title[:title.rfind(\"(\")].strip()\n",
        "    return title.strip()\n",
        "\n",
        "movies_df[\"year\"] = movies_df[\"title\"].apply(extract_year)\n",
        "movies_df[\"clean_title\"] = movies_df[\"title\"].apply(clean_title)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 3: TMDB Metadata Functions\n",
        "# ---------------------------------------\n",
        "\n",
        "# Search for movie in TMDB\n",
        "def search_tmdb(title, year):\n",
        "    url = f\"{BASE_URL}/search/movie\"\n",
        "    params = {\"query\": title, \"year\": year}\n",
        "    response = requests.get(url, headers=HEADERS, params=params)\n",
        "    r = response.json()\n",
        "    if r.get(\"results\"):\n",
        "        return r[\"results\"][0]\n",
        "    return None\n",
        "\n",
        "# Get full metadata from TMDB\n",
        "def get_full_tmdb_metadata(tmdb_id):\n",
        "    metadata = {}\n",
        "\n",
        "    # Credits (cast, crew)\n",
        "    credits = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/credits\", headers=HEADERS).json()\n",
        "    cast = [c[\"name\"] for c in credits.get(\"cast\", [])[:3]]\n",
        "    directors = [c[\"name\"] for c in credits.get(\"crew\", []) if c.get(\"job\") == \"Director\"]\n",
        "\n",
        "    # Keywords\n",
        "    keywords = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/keywords\", headers=HEADERS).json()\n",
        "    keyword_list = [k[\"name\"] for k in keywords.get(\"keywords\", [])]\n",
        "\n",
        "    # Videos (trailers)\n",
        "    videos = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/videos\", headers=HEADERS).json()\n",
        "    trailer_links = [\n",
        "        f\"https://www.youtube.com/watch?v={v['key']}\"\n",
        "        for v in videos.get(\"results\", [])\n",
        "        if v[\"site\"] == \"YouTube\" and v[\"type\"] == \"Trailer\"\n",
        "    ]\n",
        "\n",
        "    # Final metadata dictionary\n",
        "    metadata[\"top_3_cast\"] = \", \".join(cast)\n",
        "    metadata[\"directors\"] = \", \".join(directors)\n",
        "    metadata[\"keywords\"] = \", \".join(keyword_list)\n",
        "    metadata[\"trailer_link\"] = trailer_links[0] if trailer_links else None\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 4: Enrich Movie Data\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched = []\n",
        "\n",
        "for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
        "    movie_data = search_tmdb(row[\"clean_title\"], row[\"year\"])\n",
        "\n",
        "    if movie_data:\n",
        "        tmdb_id = movie_data[\"id\"]\n",
        "        extra = get_full_tmdb_metadata(tmdb_id)\n",
        "\n",
        "        genre_ids = movie_data.get(\"genre_ids\", [])\n",
        "        genre_names = [GENRE_ID_TO_NAME.get(gid, str(gid)) for gid in genre_ids]\n",
        "\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": tmdb_id,\n",
        "            \"overview\": movie_data.get(\"overview\", \"\"),\n",
        "            \"poster_path\": IMAGE_BASE + movie_data.get(\"poster_path\", \"\") if movie_data.get(\"poster_path\") else None,\n",
        "            \"backdrop_path\": IMAGE_BASE + movie_data.get(\"backdrop_path\", \"\") if movie_data.get(\"backdrop_path\") else None,\n",
        "            \"vote_average\": movie_data.get(\"vote_average\", None),\n",
        "            \"vote_count\": movie_data.get(\"vote_count\", None),\n",
        "            \"tmdb_genres\": \", \".join(genre_names),\n",
        "            **extra\n",
        "        })\n",
        "    else:\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": None,\n",
        "            \"overview\": None,\n",
        "            \"poster_path\": None,\n",
        "            \"backdrop_path\": None,\n",
        "            \"vote_average\": None,\n",
        "            \"vote_count\": None,\n",
        "            \"tmdb_genres\": None,\n",
        "            \"top_3_cast\": None,\n",
        "            \"directors\": None,\n",
        "            \"keywords\": None,\n",
        "            \"trailer_link\": None\n",
        "        })\n",
        "\n",
        "    time.sleep(0.25)  # Respect TMDB API rate limits\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 5: Save Final Dataset\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched)\n",
        "final_df = pd.concat([movies_df, enriched_df], axis=1)\n",
        "final_df.to_csv(\"movies_enriched_full.csv\", index=False)\n",
        "\n",
        "print(\"DONE: Saved as 'movies_enriched_full.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Personalized Content-Based Movie Recommendation System**\n",
        "\n",
        "This Python script implements a **Content-Based Filtering (CBF)** system enhanced with **personalized recommendations** using user-specific rating profiles. Built using the MovieLens 1M dataset and enriched metadata, the pipeline performs vectorization, similarity computation, and profile-based predictions.\n",
        "\n",
        "**What This Script Does**\n",
        "\n",
        "* **Module 1–2**: Load essential libraries and enriched movie data.\n",
        "* **Module 3**: Load user ratings and demographics.\n",
        "* **Module 4**: Engineer features combining genres, cast, crew, keywords, and movie overviews.\n",
        "* **Module 5**: Transform content into TF-IDF, Count, or Binary vectors, and compute pairwise similarities using Cosine or Jaccard metrics.\n",
        "* **Module 6**: Construct a weighted content profile per user based on past ratings.\n",
        "* **Module 7**: Recommend top-N movies similar to the user profile, excluding already seen titles.\n",
        "\n",
        "**Techniques Used**\n",
        "\n",
        "* **Text Vectorization**: TF-IDF, CountVectorizer, Binary Count\n",
        "* **Similarity Metrics**: Cosine Similarity, Jaccard Similarity\n",
        "* **Personalization**: Weighted vector averaging based on each user’s rated items\n",
        "* **Parallelization**: Speeds up Jaccard similarity computation using joblib\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "* Personalized recommendations for new users with a few ratings (cold-start)\n",
        "* Improving diversity and relevance in suggested movies\n",
        "* Generating fallback content suggestions in hybrid recommender systems"
      ],
      "metadata": {
        "id": "c7KNZkOThu7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GeB2GWMyfPL",
        "outputId": "8f828d14-e799-42e2-a43c-c43f0d395ec8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: scikit-surprise==1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.1.4)\n",
            "Requirement already satisfied: scikit-learn==1.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: matplotlib==3.8.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.8.4)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
            "Requirement already satisfied: tqdm==4.66.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.66.4)\n",
            "Requirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise==1.1.4->-r requirements.txt (line 3)) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 2)) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Personalized Content-Based Movie Recommendation System Using Hybrid Textual Metadata and Multiple Similarity Models**\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "The goal of this project is to build a personalized movie recommendation system that leverages content-based filtering techniques using enriched movie metadata. By incorporating user rating data and multiple text-based similarity strategies, the system aims to generate relevant and diverse movie suggestions tailored to individual user preferences—especially in cold-start or sparsely rated scenarios.\n",
        "\n",
        "**Methodology**\n",
        "\n",
        "1. **Data Loading & Preparation**\n",
        "\n",
        "   * Movie metadata is loaded from an enriched dataset containing genres, keywords, cast, director, overview, and release year.\n",
        "   * User ratings and demographic data are loaded and used to personalize recommendations.\n",
        "\n",
        "2. **Feature Engineering**\n",
        "\n",
        "   * A composite text field (`cbf_features`) is created for each movie by concatenating cleaned metadata fields: genres, keywords, cast, director, overview, and year.\n",
        "\n",
        "3. **Vectorization**\n",
        "\n",
        "   * Three representations of movie content are generated:\n",
        "\n",
        "     * **TF-IDF Vectors**: Capture term importance within documents.\n",
        "     * **Count Vectors**: Raw term frequencies without weighting.\n",
        "     * **Binary Genre-Like Vectors**: For Jaccard similarity (1 if feature exists).\n",
        "\n",
        "4. **Similarity Computation**\n",
        "\n",
        "   * Cosine similarity is computed for TF-IDF and Count vectors.\n",
        "   * Jaccard similarity is computed for binary vectors using pairwise intersection-over-union.\n",
        "\n",
        "5. **User Profiling & Recommendation**\n",
        "\n",
        "   * For **TF-IDF** and **Count** models:\n",
        "\n",
        "     * A personalized **user profile vector** is created using a weighted average of vectors from rated movies.\n",
        "     * Recommendations are generated by finding unseen movies most similar to the user’s profile.\n",
        "   * For the **Binary + Jaccard** model:\n",
        "\n",
        "     * The average Jaccard similarity is computed between each unseen movie and the user’s seen movies.\n",
        "\n",
        "6. **Result Generation & Tagging**\n",
        "\n",
        "   * Top 50 movie recommendations are produced per user for each model.\n",
        "   * Each output is tagged with the model name: `\"TF-IDF + Cosine\"`, `\"Count + Cosine\"`, or `\"Binary + Jaccard\"`.\n",
        "\n",
        "7. **Output Consolidation**\n",
        "\n",
        "   * All recommendation outputs are combined into one labeled DataFrame for comparative analysis and visualization."
      ],
      "metadata": {
        "id": "vj82RTA_ldyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Hybrid CBF Pipeline with RMSE, Top-N, and CSV Export\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load Data\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# Feature Engineering\n",
        "def create_feature_string(df):\n",
        "    def clean(col):\n",
        "        return df[col].fillna('').astype(str).str.replace(',', ' ').str.replace(r'\\s+', ' ', regex=True)\n",
        "    df['cbf_features'] = (\n",
        "        clean('tmdb_genres') + ' ' +\n",
        "        clean('keywords') + ' ' +\n",
        "        clean('top_3_cast') + ' ' +\n",
        "        clean('directors') + ' ' +\n",
        "        df['overview'].fillna('').str.lower().str.replace(r'[^\\w\\s]', '', regex=True) + ' ' +\n",
        "        df['year'].astype(str)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "movies = create_feature_string(movies)\n",
        "\n",
        "# Train-Test Split Per User\n",
        "def train_test_split_user(ratings, test_size=0.2):\n",
        "    train_rows, test_rows = [], []\n",
        "    for user_id, group in ratings.groupby('userId'):\n",
        "        if len(group) >= 5:\n",
        "            train, test = train_test_split(group, test_size=test_size, random_state=42)\n",
        "            train_rows.append(train)\n",
        "            test_rows.append(test)\n",
        "        else:\n",
        "            train_rows.append(group)\n",
        "    return pd.concat(train_rows), pd.concat(test_rows)\n",
        "\n",
        "train_ratings, test_ratings = train_test_split_user(ratings)\n",
        "\n",
        "# Bias Terms\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "user_bias = train_ratings.groupby('userId')['rating'].mean() - global_mean\n",
        "item_bias = train_ratings.groupby('movieId')['rating'].mean() - global_mean\n",
        "\n",
        "# Vectorizers\n",
        "tfidf_matrix = TfidfVectorizer(stop_words='english').fit_transform(movies['cbf_features'])\n",
        "count_matrix = CountVectorizer(stop_words='english').fit_transform(movies['cbf_features'])\n",
        "binary_matrix = CountVectorizer(binary=True).fit_transform(movies['cbf_features'])\n",
        "\n",
        "# Helper Functions\n",
        "def build_user_profile(user_id, train_ratings, matrix, movies):\n",
        "    user_train = train_ratings[train_ratings['userId'] == user_id]\n",
        "    indices = movies[movies['movieId'].isin(user_train['movieId'])].index\n",
        "\n",
        "    if len(indices) == 0:\n",
        "        return None\n",
        "\n",
        "    weights = user_train.set_index('movieId').loc[movies.iloc[indices]['movieId']]['rating'].values\n",
        "\n",
        "    # Check if matrix is sparse\n",
        "    row_vectors = matrix[indices].toarray() if hasattr(matrix, \"toarray\") else matrix[indices]\n",
        "\n",
        "    return np.average(row_vectors, axis=0, weights=weights).reshape(1, -1)\n",
        "\n",
        "def evaluate_rmse_for_user(user_id, train_ratings, test_ratings, matrix, movies, sim_fn):\n",
        "    profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "    if profile is None:\n",
        "        return None\n",
        "    user_test = test_ratings[test_ratings['userId'] == user_id]\n",
        "    test_movies = movies[movies['movieId'].isin(user_test['movieId'])]\n",
        "    test_indices = test_movies.index\n",
        "    if len(test_indices) == 0:\n",
        "        return None\n",
        "    sims = sim_fn(profile, matrix[test_indices]).flatten()\n",
        "    b_u = user_bias.get(user_id, 0)\n",
        "    b_i = item_bias.reindex(test_movies['movieId']).fillna(0).values\n",
        "    preds = np.clip(global_mean + b_u + b_i + sims * 1.5, 0.5, 5.0)\n",
        "    actual = user_test.set_index('movieId').loc[test_movies['movieId']]['rating'].values\n",
        "    return np.sqrt(mean_squared_error(actual, preds))\n",
        "\n",
        "def evaluate_rmse_all_users(train_ratings, test_ratings, matrix, movies, sim_fn):\n",
        "    user_ids = test_ratings['userId'].unique()\n",
        "    rmses = []\n",
        "    for user_id in tqdm(user_ids, desc=\"Evaluating users\"):\n",
        "        rmse = evaluate_rmse_for_user(user_id, train_ratings, test_ratings, matrix, movies, sim_fn)\n",
        "        if rmse is not None:\n",
        "            rmses.append(rmse)\n",
        "    return np.mean(rmses)\n",
        "\n",
        "def recommend_top_n(user_id, train_ratings, matrix, movies, sim_fn, top_n=50):\n",
        "    profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "    if profile is None:\n",
        "        return pd.DataFrame()\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId']\n",
        "    unseen = movies[~movies['movieId'].isin(seen)]\n",
        "    sims = sim_fn(profile, matrix[unseen.index]).flatten()\n",
        "    content_scores = sims * 1.5\n",
        "    b_u = user_bias.get(user_id, 0)\n",
        "    b_i = item_bias.reindex(unseen['movieId']).fillna(0).values\n",
        "    preds = np.clip(global_mean + b_u + b_i + content_scores, 0.5, 5.0)\n",
        "    top_idx = np.argsort(preds)[-top_n:][::-1]\n",
        "    return unseen.iloc[top_idx][['movieId', 'title']].assign(predicted_rating=preds[top_idx])\n",
        "\n",
        "# Save Predictions for Meta-Learner\n",
        "def save_predictions(user_ids, matrix, sim_fn, label):\n",
        "    dfs = []\n",
        "    for user_id in tqdm(user_ids, desc=f\"Scoring {label}\"):\n",
        "        profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "        if profile is None:\n",
        "            continue\n",
        "        user_test = test_ratings[test_ratings['userId'] == user_id]\n",
        "        test_movies = movies[movies['movieId'].isin(user_test['movieId'])]\n",
        "        test_indices = test_movies.index\n",
        "        if len(test_indices) == 0:\n",
        "            continue\n",
        "        sims = sim_fn(profile, matrix[test_indices]).flatten()\n",
        "        b_u = user_bias.get(user_id, 0)\n",
        "        b_i = item_bias.reindex(test_movies['movieId']).fillna(0).values\n",
        "        preds = np.clip(global_mean + b_u + b_i + sims * 1.5, 0.5, 5.0)\n",
        "        actual = user_test.set_index('movieId').loc[test_movies['movieId']]['rating'].values\n",
        "        df = pd.DataFrame({\n",
        "            'userId': user_id,\n",
        "            'movieId': test_movies['movieId'].values,\n",
        "            'true_rating': actual,\n",
        "            f'{label}_score': preds\n",
        "        })\n",
        "        dfs.append(df)\n",
        "    result = pd.concat(dfs)\n",
        "    result.to_csv(f'cbf_predictions_{label}.csv', index=False)\n",
        "\n",
        "# Run Evaluations and Save Predictions\n",
        "rmse_tfidf = evaluate_rmse_all_users(train_ratings, test_ratings, tfidf_matrix, movies, cosine_similarity)\n",
        "rmse_count = evaluate_rmse_all_users(train_ratings, test_ratings, count_matrix, movies, cosine_similarity)\n",
        "rmse_binary = evaluate_rmse_all_users(train_ratings, test_ratings, binary_matrix.toarray(), movies,\n",
        "                                      lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard'))\n",
        "\n",
        "print(f\"\\nTF-IDF + Cosine RMSE: {rmse_tfidf:.4f}\")\n",
        "print(f\"Count + Cosine RMSE: {rmse_count:.4f}\")\n",
        "print(f\"Binary + Jaccard RMSE: {rmse_binary:.4f}\")\n",
        "\n",
        "# Top-N Recommendations for User 5549\n",
        "print(\"\\nTop-N Recommendations for User 5549 — TF-IDF\")\n",
        "print(recommend_top_n(5549, train_ratings, tfidf_matrix, movies, cosine_similarity).head())\n",
        "\n",
        "print(\"\\nTop-N Recommendations for User 5549 — Count\")\n",
        "print(recommend_top_n(5549, train_ratings, count_matrix, movies, cosine_similarity).head())\n",
        "\n",
        "print(\"\\nTop-N Recommendations for User 5549 — Jaccard\")\n",
        "print(\"\\nTop-N Recommendations for User 5549 — Jaccard\")\n",
        "print(recommend_top_n(\n",
        "    5549,\n",
        "    train_ratings,\n",
        "    binary_matrix.toarray(),   # Convert to dense\n",
        "    movies,\n",
        "    lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard')\n",
        ").head())\n",
        "\n",
        "\n",
        "# Save Predictions\n",
        "save_predictions(test_ratings['userId'].unique(), tfidf_matrix, cosine_similarity, 'tfidf')\n",
        "save_predictions(test_ratings['userId'].unique(), count_matrix, cosine_similarity, 'count')\n",
        "save_predictions(test_ratings['userId'].unique(), binary_matrix.toarray(),\n",
        "                 lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard'), 'binary')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2U48IdMw0rJ",
        "outputId": "a0eb57a5-278c-4e37-9a07-518036bc443d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating users: 100%|██████████| 6040/6040 [03:57<00:00, 25.41it/s]\n",
            "Evaluating users: 100%|██████████| 6040/6040 [04:00<00:00, 25.08it/s]\n",
            "Evaluating users: 100%|██████████| 6040/6040 [04:13<00:00, 23.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF + Cosine RMSE: 0.9424\n",
            "Count + Cosine RMSE: 0.9693\n",
            "Binary + Jaccard RMSE: 0.9297\n",
            "\n",
            "Top-N Recommendations for User 5549 — TF-IDF\n",
            "      movieId                                              title  \\\n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1396     1420  Message to Love: The Isle of Wight Festival (1...   \n",
            "3313     3382                             Song of Freedom (1936)   \n",
            "3587     3656                                       Lured (1947)   \n",
            "777       787                 Gate of Heavenly Peace, The (1995)   \n",
            "\n",
            "      predicted_rating  \n",
            "3164               5.0  \n",
            "1396               5.0  \n",
            "3313               5.0  \n",
            "3587               5.0  \n",
            "777                5.0  \n",
            "\n",
            "Top-N Recommendations for User 5549 — Count\n",
            "      movieId                                              title  \\\n",
            "1762     1830                            Follow the Bitch (1998)   \n",
            "3811     3881                           Bittersweet Motel (2000)   \n",
            "977       989          Schlafes Bruder (Brother of Sleep) (1995)   \n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1339     1360  Identification of a Woman (Identificazione di ...   \n",
            "\n",
            "      predicted_rating  \n",
            "1762               5.0  \n",
            "3811               5.0  \n",
            "977                5.0  \n",
            "3164               5.0  \n",
            "1339               5.0  \n",
            "\n",
            "Top-N Recommendations for User 5549 — Jaccard\n",
            "\n",
            "Top-N Recommendations for User 5549 — Jaccard\n",
            "      movieId                                              title  \\\n",
            "3313     3382                             Song of Freedom (1936)   \n",
            "1762     1830                            Follow the Bitch (1998)   \n",
            "777       787                 Gate of Heavenly Peace, The (1995)   \n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1396     1420  Message to Love: The Isle of Wight Festival (1...   \n",
            "\n",
            "      predicted_rating  \n",
            "3313               5.0  \n",
            "1762               5.0  \n",
            "777                5.0  \n",
            "3164               5.0  \n",
            "1396               5.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring tfidf: 100%|██████████| 6040/6040 [03:59<00:00, 25.18it/s]\n",
            "Scoring count: 100%|██████████| 6040/6040 [03:56<00:00, 25.52it/s]\n",
            "Scoring binary: 100%|██████████| 6040/6040 [04:13<00:00, 23.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Memory-based collaborative filtering module (UBCF, IBCF)**\n",
        "\n",
        "***Purpose:***\n",
        "\n",
        "This module implements **memory-based collaborative filtering** using **user-user** or **item-item** similarity. It addresses **user bias** by normalizing ratings through mean-centering and optionally **rescaling predictions** to the original rating scale for interpretability.\n",
        "\n",
        "***Methodology:***\n",
        "\n",
        "1. **Rating Matrix Construction**:\n",
        "\n",
        "   * A user-item matrix is built from raw MovieLens-style ratings data.\n",
        "   * For `kind='user'`, ratings are mean-centered per user to reduce bias from lenient or strict raters.\n",
        "   * For `kind='item'`, raw ratings are used directly (no normalization), as the algorithm focuses on item similarities based on a single user's input.\n",
        "\n",
        "2. **Similarity Computation**:\n",
        "\n",
        "   * Cosine similarity is computed either:\n",
        "\n",
        "     * **Across users** for user-based CF (`kind='user'`)\n",
        "     * **Across items** for item-based CF (`kind='item'`)\n",
        "   * `sklearn.metrics.pairwise_distances` is used to derive similarity as `1 - cosine_distance`.\n",
        "\n",
        "3. **Prediction Generation**:\n",
        "\n",
        "   * For **user-based CF**:\n",
        "\n",
        "     * Ratings from similar users are weighted by similarity and averaged.\n",
        "     * The user’s mean rating is **added back** to restore predictions to the original scale (e.g., 1–5).\n",
        "   * For **item-based CF**:\n",
        "\n",
        "     * A user’s own ratings are used to compute scores for similar items.\n",
        "     * No mean is added back, since predictions are already on the correct scale.\n",
        "\n",
        "4. **Top-N Recommendations**:\n",
        "\n",
        "   * The system filters out movies the user has already rated.\n",
        "   * It ranks unseen movies by predicted score and returns the top-N recommendations.\n",
        "   * Each recommendation is labeled with the model type (`User-Based CF` or `Item-Based CF`) for downstream tracking."
      ],
      "metadata": {
        "id": "omg4Y6k5XmSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === Step 1: Load Data ===\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# === Step 2: Create Bias-Adjusted Matrix ===\n",
        "def create_bias_adjusted_matrix(ratings_df):\n",
        "    matrix = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
        "    global_mean = ratings_df['rating'].mean()\n",
        "    user_bias = matrix.sub(global_mean, axis=0).mean(axis=1)\n",
        "    item_bias = matrix.sub(global_mean, axis=0).sub(user_bias, axis=0).mean(axis=0)\n",
        "    adjusted = matrix.sub(global_mean).sub(user_bias, axis=0).sub(item_bias, axis=1)\n",
        "    return adjusted.fillna(0), global_mean, user_bias, item_bias\n",
        "\n",
        "user_item_matrix, global_mean, user_bias, item_bias = create_bias_adjusted_matrix(train_ratings)\n",
        "\n",
        "# === Step 3: Similarity Matrices ===\n",
        "user_sim_matrix = cosine_similarity(user_item_matrix)\n",
        "item_sim_matrix = cosine_similarity(user_item_matrix.T)\n",
        "\n",
        "# === Step 4: Recommender Function ===\n",
        "def recommend_memory_based(user_id, matrix, global_mean, user_bias, item_bias, sim_matrix, kind='user', top_n=50, return_full=False):\n",
        "    if kind == 'user':\n",
        "        sim_scores = sim_matrix[matrix.index.get_loc(user_id)]\n",
        "        weighted = sim_scores @ matrix.values\n",
        "        norm = np.abs(sim_scores).sum()\n",
        "        preds = weighted / norm if norm != 0 else np.zeros_like(weighted)\n",
        "        preds += global_mean + user_bias.loc[user_id]\n",
        "    else:\n",
        "        user_vector = matrix.loc[user_id]\n",
        "        weighted = user_vector @ sim_matrix\n",
        "        norm = (user_vector != 0) @ np.abs(sim_matrix)\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            preds = np.true_divide(weighted, norm)\n",
        "            preds[norm == 0] = 0\n",
        "        preds += global_mean + user_bias.loc[user_id] + item_bias.values\n",
        "\n",
        "    preds = np.clip(preds, 1.0, 5.0)\n",
        "    pred_series = pd.Series(preds, index=matrix.columns)\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    pred_series = pred_series.drop(labels=seen, errors='ignore')\n",
        "\n",
        "    if return_full:\n",
        "        return pred_series\n",
        "    else:\n",
        "        top_preds = pred_series.sort_values(ascending=False).head(top_n)\n",
        "        return pd.DataFrame({\n",
        "            'userId': user_id,\n",
        "            'movieId': top_preds.index,\n",
        "            'score': top_preds.values\n",
        "        })\n",
        "\n",
        "# === Step 5: Evaluation Function ===\n",
        "def evaluate_model_and_save(test_df, matrix, global_mean, user_bias, item_bias, sim_matrix, kind='user', output_file=None):\n",
        "    all_preds = []\n",
        "    for uid in test_df['userId'].unique():\n",
        "        if uid not in matrix.index:\n",
        "            continue\n",
        "        recs = recommend_memory_based(uid, matrix, global_mean, user_bias, item_bias, sim_matrix, kind, top_n=1000, return_full=True)\n",
        "        actual = test_df[test_df['userId'] == uid]\n",
        "        merged = pd.merge(actual, recs.rename(\"score\"), on=\"movieId\")\n",
        "        all_preds.append(merged)\n",
        "\n",
        "    all_preds_df = pd.concat(all_preds, ignore_index=True)\n",
        "    if output_file:\n",
        "        all_preds_df.to_csv(output_file, index=False)\n",
        "    rmse = np.sqrt(mean_squared_error(all_preds_df['rating'], all_preds_df['score'])) if not all_preds_df.empty else np.nan\n",
        "    return rmse\n",
        "\n",
        "# === Step 6: Run Evaluation and Save Predictions ===\n",
        "user_rmse = evaluate_model_and_save(test_ratings, user_item_matrix, global_mean, user_bias, item_bias, user_sim_matrix, 'user', \"ubcf_predictions.csv\")\n",
        "item_rmse = evaluate_model_and_save(test_ratings, user_item_matrix, global_mean, user_bias, item_bias, item_sim_matrix, 'item', \"ibcf_predictions.csv\")\n",
        "dummy_rmse = np.sqrt(mean_squared_error(test_ratings['rating'], [global_mean] * len(test_ratings)))\n",
        "\n",
        "print(f\"User-Based CF RMSE: {user_rmse:.4f}\")\n",
        "print(f\"Item-Based CF RMSE: {item_rmse:.4f}\")\n",
        "print(f\"Dummy Predictor RMSE: {dummy_rmse:.4f}\")\n",
        "\n",
        "# === Step 7: Top-N for One User ===\n",
        "user_id = 5549\n",
        "user_recs = recommend_memory_based(user_id, user_item_matrix, global_mean, user_bias, item_bias, user_sim_matrix, 'user', top_n=50)\n",
        "item_recs = recommend_memory_based(user_id, user_item_matrix, global_mean, user_bias, item_bias, item_sim_matrix, 'item', top_n=50)\n",
        "\n",
        "user_recs = user_recs.merge(movies, on='movieId', how='left')\n",
        "item_recs = item_recs.merge(movies, on='movieId', how='left')\n",
        "\n",
        "print(\"\\nTop 10 User-Based CF Recommendations:\")\n",
        "print(user_recs[['movieId', 'title', 'score']].head(10))\n",
        "\n",
        "print(\"\\nTop 10 Item-Based CF Recommendations:\")\n",
        "print(item_recs[['movieId', 'title', 'score']].head(10))\n"
      ],
      "metadata": {
        "id": "LDxfs5nRH3hv",
        "outputId": "edda872d-4ae6-413d-a604-06eb3b6ad14a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-Based CF RMSE: 1.0336\n",
            "Item-Based CF RMSE: 0.8796\n",
            "Dummy Predictor RMSE: 1.1197\n",
            "\n",
            "Top 10 User-Based CF Recommendations:\n",
            "   movieId                          title     score\n",
            "0     2701          Wild Wild West (1999)  3.608871\n",
            "1     1917              Armageddon (1998)  3.608796\n",
            "2     1721                 Titanic (1997)  3.607812\n",
            "3     3753            Patriot, The (2000)  3.605514\n",
            "4     2881         Double Jeopardy (1999)  3.604429\n",
            "5     2722           Deep Blue Sea (1999)  3.602775\n",
            "6      736                 Twister (1996)  3.602065\n",
            "7     3113             End of Days (1999)  3.601182\n",
            "8      780  Independence Day (ID4) (1996)  3.600492\n",
            "9     1101                 Top Gun (1986)  3.600417\n",
            "\n",
            "Top 10 Item-Based CF Recommendations:\n",
            "   movieId                                              title     score\n",
            "0      557                                  Mamma Roma (1962)  5.000000\n",
            "1      657                                 Yankee Zulu (1994)  5.000000\n",
            "2     3601                        Castaway Cowboy, The (1974)  5.000000\n",
            "3      989          Schlafes Bruder (Brother of Sleep) (1995)  5.000000\n",
            "4     3517                                  Bells, The (1926)  5.000000\n",
            "5      729  Institute Benjamenta, or This Dream People Cal...  5.000000\n",
            "6     2591  Jeanne and the Perfect Guy (Jeanne et le garço...  5.000000\n",
            "7     1787  Paralyzing Fear: The Story of Polio in America...  4.865259\n",
            "8     3323                              Chain of Fools (2000)  4.825758\n",
            "9      787                 Gate of Heavenly Peace, The (1995)  4.823364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model-Based Filtering:**\n",
        "\n",
        "  * *SVD (Surprise)*: Learns latent features from the rating matrix.\n",
        "  * *ALS (PySpark)*: Scalable factorization method for large datasets.\n"
      ],
      "metadata": {
        "id": "R6d3SP0OolH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 9: Model-Based Collaborative Filtering (SVD using Surprise)**\n",
        "\n",
        "**Purpose:**\n",
        "Use matrix factorization (SVD) to learn latent user/item features from the rating matrix.\n",
        "\n",
        "**Application:**\n",
        "- Accurate, scalable recommendations for sparse datasets using user/item embeddings.\n",
        "- Suitable for small to medium datasets.\n",
        "- Optimized via `GridSearchCV` for hyperparameter tuning.\n",
        "- Good interpretability of latent factors per user and item.\n",
        "\n"
      ],
      "metadata": {
        "id": "4dCPxrNMnQkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split, GridSearchCV\n",
        "from surprise.accuracy import rmse as surprise_rmse\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Step 1: Load ratings\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# === Step 2: Prepare Surprise data\n",
        "reader = Reader(rating_scale=(0.5, 5.0))\n",
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# === Step 3: Tune SVD Model\n",
        "param_grid = {\n",
        "    'n_factors': [50, 100],\n",
        "    'lr_all': [0.005, 0.01],\n",
        "    'reg_all': [0.02, 0.1]\n",
        "}\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
        "gs.fit(data)\n",
        "best_svd_model = gs.best_estimator['rmse']\n",
        "print(f\"Best RMSE: {gs.best_score['rmse']:.4f}, Params: {gs.best_params['rmse']}\")\n",
        "\n",
        "# === Step 4: Train/Test Split and Evaluate\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "best_svd_model.fit(trainset)\n",
        "\n",
        "predictions = [best_svd_model.predict(uid, iid, r_ui=rui) for uid, iid, rui in tqdm(testset)]\n",
        "svd_rmse = surprise_rmse(predictions)\n",
        "\n",
        "# === Save full predictions with tag\n",
        "pred_df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
        "pred_df = pred_df.rename(columns={'uid': 'userId', 'iid': 'movieId', 'rui': 'true_rating', 'est': 'pred_rating'})\n",
        "pred_df['model'] = 'svd_surprise'\n",
        "pred_df.to_csv(\"svd_surprise_predictions.csv\", index=False)\n",
        "print(\"Saved: svd_surprise_predictions.csv\")\n",
        "\n",
        "# === Step 5: Top-N for User 5549\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = [mid for mid in all_movie_ids if mid not in rated_movie_ids]\n",
        "\n",
        "print(f\"\\nGenerating Top-N for User {target_user}...\")\n",
        "top_preds = [(mid, best_svd_model.predict(target_user, mid).est) for mid in tqdm(unrated_movie_ids)]\n",
        "top_50_df = pd.DataFrame(top_preds, columns=['movieId', 'pred_rating'])\n",
        "top_50_df = top_50_df.sort_values(by='pred_rating', ascending=False).head(50)\n",
        "top_50_df['userId'] = target_user\n",
        "top_50_df['model'] = 'svd_surprise'\n",
        "top_50_df = top_50_df[['userId', 'movieId', 'pred_rating', 'model']]\n",
        "\n",
        "# Optional: Merge with movie titles\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_df = top_50_df.merge(movies, on='movieId', how='left')\n",
        "\n",
        "print(\"\\nTop 10 Recommendations:\")\n",
        "print(top_50_df[['movieId', 'title', 'pred_rating']].head(10))\n",
        "\n",
        "# === Save Top-N recommendations\n",
        "top_50_df.to_csv(\"top50_svd_surprise_user_5549.csv\", index=False)\n",
        "print(\"Saved: top50_svd_surprise_user_5549.csv\")\n"
      ],
      "metadata": {
        "id": "vQVXyqXBMFes",
        "outputId": "5e3f7fc6-8582-4fbe-b7d3-4fbcd6f6cbbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE: 0.8823, Params: {'n_factors': 50, 'lr_all': 0.005, 'reg_all': 0.02}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200042/200042 [00:02<00:00, 82198.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.8713\n",
            "Saved: svd_surprise_predictions.csv\n",
            "\n",
            "Generating Top-N for User 5549...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3673/3673 [00:00<00:00, 148870.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 Recommendations:\n",
            "   movieId                                        title  pred_rating\n",
            "0     2905                               Sanjuro (1962)     4.640094\n",
            "1      527                      Schindler's List (1993)     4.626092\n",
            "2      318             Shawshank Redemption, The (1994)     4.617420\n",
            "3     2324   Life Is Beautiful (La Vita è bella) (1997)     4.511809\n",
            "4     3134  Grand Illusion (Grande illusion, La) (1937)     4.424373\n",
            "5      905                 It Happened One Night (1934)     4.419785\n",
            "6     1949                Man for All Seasons, A (1966)     4.402241\n",
            "7     1250         Bridge on the River Kwai, The (1957)     4.395232\n",
            "8     2925      Conformist, The (Il Conformista) (1970)     4.379199\n",
            "9      670       World of Apu, The (Apur Sansar) (1959)     4.367088\n",
            "Saved: top50_svd_surprise_user_5549.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model-Based Collaborative Filtering (ALS using PySpark)**\n",
        "\n",
        "**Purpose:**\n",
        "Use Alternating Least Squares (ALS) to learn latent user/item features at scale.\n",
        "\n",
        "**Application:**\n",
        "- Distributed recommendation system for large-scale datasets.\n",
        "- Runs on Apache Spark for horizontal scalability.\n",
        "- Handles sparsity well using factorization.\n",
        "- Suited for real-time, production-level systems with massive data.\n"
      ],
      "metadata": {
        "id": "DeCmPC_DnZip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# ALS with Train/Test Split and Evaluation\n",
        "# ==============================\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import Row\n",
        "import pandas as pd\n",
        "\n",
        "# --- Start Spark Session ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ALSModel\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Load Ratings ---\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "ratings_df = spark.createDataFrame(ratings[['userId', 'movieId', 'rating']])\n",
        "\n",
        "# --- Train/Test Split ---\n",
        "(training_df, test_df) = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# --- Train ALS Model ---\n",
        "als = ALS(\n",
        "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "    rank=10, maxIter=10, regParam=0.1,\n",
        "    coldStartStrategy=\"drop\", nonnegative=True\n",
        ")\n",
        "als_model = als.fit(training_df)\n",
        "\n",
        "# --- Predict on Test Set ---\n",
        "test_predictions = als_model.transform(test_df)\n",
        "\n",
        "# --- Evaluate RMSE ---\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName='rmse',\n",
        "    labelCol='rating',\n",
        "    predictionCol='prediction'\n",
        ")\n",
        "rmse_score = evaluator.evaluate(test_predictions)\n",
        "\n",
        "# --- Save Prediction Data ---\n",
        "pred_pd = test_predictions.select('userId', 'movieId', 'rating', 'prediction').toPandas()\n",
        "pred_pd = pred_pd.rename(columns={'rating': 'true_rating', 'prediction': 'pred_rating'})\n",
        "pred_pd['model'] = 'ALS (PySpark)'\n",
        "pred_pd.to_csv(\"als_predictions_test.csv\", index=False)\n",
        "\n",
        "print(pred_pd[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']].head())\n",
        "print(f\"\\nFinal RMSE on Test Set: {rmse_score:.4f}\")\n",
        "\n",
        "# ==============================\n",
        "# Top-50 Predictions for User 5549\n",
        "# ==============================\n",
        "\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = list(set(all_movie_ids) - set(rated_movie_ids))\n",
        "\n",
        "# Create Spark DataFrame of userId + unrated movieId pairs\n",
        "user_unrated_pairs = spark.createDataFrame([Row(userId=target_user, movieId=int(mid)) for mid in unrated_movie_ids])\n",
        "\n",
        "# Predict ratings for unrated movies\n",
        "print(f\"\\nGenerating Top-50 recommendations for User {target_user}...\")\n",
        "top_preds_df = als_model.transform(user_unrated_pairs).dropna()\n",
        "\n",
        "# Get top-50 highest predicted ratings\n",
        "top_50_preds = top_preds_df.orderBy(col(\"prediction\").desc()).limit(50)\n",
        "top_50_pd = top_50_preds.select(\"userId\", \"movieId\", \"prediction\").toPandas()\n",
        "top_50_pd['model'] = \"ALS (PySpark)\"\n",
        "top_50_pd = top_50_pd.rename(columns={'prediction': 'pred_rating'})\n",
        "\n",
        "# Merge with Movie Titles\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_pd = top_50_pd.merge(movies, on='movieId', how='left')\n",
        "\n",
        "# Save Top-N\n",
        "top_50_pd.to_csv(\"als_top_50_user_5549.csv\", index=False)\n",
        "\n",
        "# Print Top-N\n",
        "print(\"\\nTop 10 ALS Recommendations for User 5549:\")\n",
        "print(top_50_pd[['movieId', 'title', 'pred_rating']].head(10))\n"
      ],
      "metadata": {
        "id": "42y8vTW1SmGY",
        "outputId": "d1cb99fc-1e92-4afd-8293-31f6fb899cdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   userId  movieId  true_rating  pred_rating          model\n",
            "0     148       11            5     4.048533  ALS (PySpark)\n",
            "1     148       17            4     3.681547  ALS (PySpark)\n",
            "2     148      107            4     3.395952  ALS (PySpark)\n",
            "3     148      165            3     3.825573  ALS (PySpark)\n",
            "4     148      185            3     3.480136  ALS (PySpark)\n",
            "\n",
            "Final RMSE on Test Set: 0.8718\n",
            "\n",
            "Generating Top-50 recommendations for User 5549...\n",
            "\n",
            "Top 10 ALS Recommendations for User 5549:\n",
            "   movieId                                           title  pred_rating\n",
            "0      572                          Foreign Student (1994)     4.957121\n",
            "1     2197                                Firelight (1997)     4.368938\n",
            "2     3172                         Ulysses (Ulisse) (1954)     4.116779\n",
            "3     1035                      Sound of Music, The (1965)     4.106802\n",
            "4      953                    It's a Wonderful Life (1946)     4.084922\n",
            "5     3585              Great Locomotive Chase, The (1956)     4.069288\n",
            "6      920                       Gone with the Wind (1939)     4.068130\n",
            "7      985                            Small Wonders (1996)     4.024732\n",
            "8      919                        Wizard of Oz, The (1939)     4.024457\n",
            "9      615  Bread and Chocolate (Pane e cioccolata) (1973)     4.017833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ls6RqljaolUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(movies.head())"
      ],
      "metadata": {
        "id": "kTr9F_PueaKj",
        "outputId": "8c5538ec-2433-43b6-abbc-b8d0381f82be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   movieId                               title\n",
            "0        1                    Toy Story (1995)\n",
            "1        2                      Jumanji (1995)\n",
            "2        3             Grumpier Old Men (1995)\n",
            "3        4            Waiting to Exhale (1995)\n",
            "4        5  Father of the Bride Part II (1995)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eOPkUwmWieTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load movie metadata\n",
        "movie_df = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "\n",
        "# List of prediction files with desired output name\n",
        "prediction_files = {\n",
        "    \"ubcf\": \"ubcf_predictions.csv\",\n",
        "    \"svd\": \"svd_surprise_predictions.csv\",\n",
        "    \"ibcf\": \"ibcf_predictions.csv\",\n",
        "    \"cbf_tfidf\": \"cbf_predictions_tfidf.csv\",\n",
        "    \"cbf_count\": \"cbf_predictions_count.csv\",\n",
        "    \"cbf_binary\": \"cbf_predictions_binary.csv\",\n",
        "    \"als\": \"als_pyspark_predictions.csv\"\n",
        "}\n",
        "\n",
        "# Merge and save each separately\n",
        "for model_name, file_path in prediction_files.items():\n",
        "    df = pd.read_csv(file_path)\n",
        "    merged = pd.merge(df, movie_df, on=\"movieId\", how=\"left\")\n",
        "    output_path = f\"/content/{model_name}_merged.csv\"\n",
        "    merged.to_csv(output_path, index=False)\n",
        "    print(f\"Saved: {output_path} with shape {merged.shape}\")\n"
      ],
      "metadata": {
        "id": "rMDYya4Qie_u",
        "outputId": "c79223bf-6301-408e-f85c-97cc376d1090",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/ubcf_merged.csv with shape (200016, 20)\n",
            "Saved: /content/svd_merged.csv with shape (200042, 21)\n",
            "Saved: /content/ibcf_merged.csv with shape (200016, 20)\n",
            "Saved: /content/cbf_tfidf_merged.csv with shape (202451, 19)\n",
            "Saved: /content/cbf_count_merged.csv with shape (202451, 19)\n",
            "Saved: /content/cbf_binary_merged.csv with shape (202451, 19)\n",
            "Saved: /content/als_merged.csv with shape (1000209, 20)\n"
          ]
        }
      ]
    }
  ]
}