{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-612/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose**\n",
        "\n",
        "The purpose of this script is to enrich the MovieLens movie dataset (`movies.dat`) with detailed movie metadata from The Movie Database (TMDB) API. This metadata includes movie overviews, genres, poster and backdrop image URLs, cast and director information, keywords, user ratings, and trailer links. The enriched dataset will serve as the foundation for building content-based, collaborative, and hybrid recommender systems.\n",
        "\n",
        "### **Methodology**\n",
        "\n",
        "1. **Load MovieLens Movie Data**\n",
        "   The script loads the `movies.dat` file, which contains basic movie information including `movieId`, `title`, and `genres`.\n",
        "\n",
        "2. **Clean Titles and Extract Years**\n",
        "   It processes the movie titles to remove the year from the title string and separately extracts the release year to improve search accuracy when querying TMDB.\n",
        "\n",
        "3. **Query TMDB API**\n",
        "   For each movie, it sends a search request to TMDB using the cleaned title and release year. If a match is found, it retrieves the movie’s TMDB ID.\n",
        "\n",
        "4. **Retrieve Detailed Metadata**\n",
        "   Using the TMDB ID, the script fetches:\n",
        "\n",
        "   * Overview (plot summary)\n",
        "   * Poster and backdrop image paths\n",
        "   * Genre IDs, which are then mapped to readable genre names\n",
        "   * Top 3 cast members\n",
        "   * Director(s)\n",
        "   * Associated keywords\n",
        "   * YouTube trailer link (if available)\n",
        "\n",
        "5. **Construct and Save Enriched Dataset**\n",
        "   All metadata is compiled into a structured format and merged with the original MovieLens data. The final dataset is saved as `movies_enriched_full.csv` for downstream use in recommendation models.\n"
      ],
      "metadata": {
        "id": "OptAJ12iBc8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "7qJ0maYGaxtJ",
        "outputId": "2cab814c-7502-488a-f415-fd6922272d72"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'movies.dat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-494969615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Load movies.dat - format: MovieID::Title::Genres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmovies_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"movies.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"genres\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# ---------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movies.dat'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ---------------------------------------\n",
        "# CONFIG\n",
        "# ---------------------------------------\n",
        "BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "IMAGE_BASE = \"https://image.tmdb.org/t/p/w500\"\n",
        "\n",
        "# Use your TMDB Bearer Token (v4)\n",
        "HEADERS = {\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyZGZlNjMwMGMzYjIzMjc2NzExNjQ0N2JhNzhiMjM5MyIsIm5iZiI6MTc1MTkyMjA3Ni4xMzUsInN1YiI6IjY4NmMzNTljMzc4NjllOGEyNDUxZTM0OSIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.S773ddH3FiIHtokPW4sYpJog0mXWS1o4OPov1KZneUw\"\n",
        "}\n",
        "\n",
        "# TMDB genre ID to name mapping\n",
        "GENRE_ID_TO_NAME = {\n",
        "    28: \"Action\", 12: \"Adventure\", 16: \"Animation\", 35: \"Comedy\", 80: \"Crime\",\n",
        "    99: \"Documentary\", 18: \"Drama\", 10751: \"Family\", 14: \"Fantasy\", 36: \"History\",\n",
        "    27: \"Horror\", 10402: \"Music\", 9648: \"Mystery\", 10749: \"Romance\", 878: \"Science Fiction\",\n",
        "    10770: \"TV Movie\", 53: \"Thriller\", 10752: \"War\", 37: \"Western\"\n",
        "}\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 1: Load MovieLens .dat Files\n",
        "# ---------------------------------------\n",
        "\n",
        "# Load movies.dat - format: MovieID::Title::Genres\n",
        "movies_df = pd.read_csv(\"movies.dat\", sep=\"::\", engine='python', header=None, names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 2: Clean Movie Titles and Extract Year\n",
        "# ---------------------------------------\n",
        "\n",
        "def extract_year(title):\n",
        "    if \"(\" in title:\n",
        "        try:\n",
        "            return int(title.strip()[-5:-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_title(title):\n",
        "    if \"(\" in title:\n",
        "        return title[:title.rfind(\"(\")].strip()\n",
        "    return title.strip()\n",
        "\n",
        "movies_df[\"year\"] = movies_df[\"title\"].apply(extract_year)\n",
        "movies_df[\"clean_title\"] = movies_df[\"title\"].apply(clean_title)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 3: TMDB Metadata Functions\n",
        "# ---------------------------------------\n",
        "\n",
        "# Search for movie in TMDB\n",
        "def search_tmdb(title, year):\n",
        "    url = f\"{BASE_URL}/search/movie\"\n",
        "    params = {\"query\": title, \"year\": year}\n",
        "    response = requests.get(url, headers=HEADERS, params=params)\n",
        "    r = response.json()\n",
        "    if r.get(\"results\"):\n",
        "        return r[\"results\"][0]\n",
        "    return None\n",
        "\n",
        "# Get full metadata from TMDB\n",
        "def get_full_tmdb_metadata(tmdb_id):\n",
        "    metadata = {}\n",
        "\n",
        "    # Credits (cast, crew)\n",
        "    credits = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/credits\", headers=HEADERS).json()\n",
        "    cast = [c[\"name\"] for c in credits.get(\"cast\", [])[:3]]\n",
        "    directors = [c[\"name\"] for c in credits.get(\"crew\", []) if c.get(\"job\") == \"Director\"]\n",
        "\n",
        "    # Keywords\n",
        "    keywords = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/keywords\", headers=HEADERS).json()\n",
        "    keyword_list = [k[\"name\"] for k in keywords.get(\"keywords\", [])]\n",
        "\n",
        "    # Videos (trailers)\n",
        "    videos = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/videos\", headers=HEADERS).json()\n",
        "    trailer_links = [\n",
        "        f\"https://www.youtube.com/watch?v={v['key']}\"\n",
        "        for v in videos.get(\"results\", [])\n",
        "        if v[\"site\"] == \"YouTube\" and v[\"type\"] == \"Trailer\"\n",
        "    ]\n",
        "\n",
        "    # Final metadata dictionary\n",
        "    metadata[\"top_3_cast\"] = \", \".join(cast)\n",
        "    metadata[\"directors\"] = \", \".join(directors)\n",
        "    metadata[\"keywords\"] = \", \".join(keyword_list)\n",
        "    metadata[\"trailer_link\"] = trailer_links[0] if trailer_links else None\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 4: Enrich Movie Data\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched = []\n",
        "\n",
        "for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
        "    movie_data = search_tmdb(row[\"clean_title\"], row[\"year\"])\n",
        "\n",
        "    if movie_data:\n",
        "        tmdb_id = movie_data[\"id\"]\n",
        "        extra = get_full_tmdb_metadata(tmdb_id)\n",
        "\n",
        "        genre_ids = movie_data.get(\"genre_ids\", [])\n",
        "        genre_names = [GENRE_ID_TO_NAME.get(gid, str(gid)) for gid in genre_ids]\n",
        "\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": tmdb_id,\n",
        "            \"overview\": movie_data.get(\"overview\", \"\"),\n",
        "            \"poster_path\": IMAGE_BASE + movie_data.get(\"poster_path\", \"\") if movie_data.get(\"poster_path\") else None,\n",
        "            \"backdrop_path\": IMAGE_BASE + movie_data.get(\"backdrop_path\", \"\") if movie_data.get(\"backdrop_path\") else None,\n",
        "            \"vote_average\": movie_data.get(\"vote_average\", None),\n",
        "            \"vote_count\": movie_data.get(\"vote_count\", None),\n",
        "            \"tmdb_genres\": \", \".join(genre_names),\n",
        "            **extra\n",
        "        })\n",
        "    else:\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": None,\n",
        "            \"overview\": None,\n",
        "            \"poster_path\": None,\n",
        "            \"backdrop_path\": None,\n",
        "            \"vote_average\": None,\n",
        "            \"vote_count\": None,\n",
        "            \"tmdb_genres\": None,\n",
        "            \"top_3_cast\": None,\n",
        "            \"directors\": None,\n",
        "            \"keywords\": None,\n",
        "            \"trailer_link\": None\n",
        "        })\n",
        "\n",
        "    time.sleep(0.25)  # Respect TMDB API rate limits\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 5: Save Final Dataset\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched)\n",
        "final_df = pd.concat([movies_df, enriched_df], axis=1)\n",
        "final_df.to_csv(\"movies_enriched_full.csv\", index=False)\n",
        "\n",
        "print(\"DONE: Saved as 'movies_enriched_full.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Personalized Content-Based Movie Recommendation System**\n",
        "\n",
        "This Python script implements a **Content-Based Filtering (CBF)** system enhanced with **personalized recommendations** using user-specific rating profiles. Built using the MovieLens 1M dataset and enriched metadata, the pipeline performs vectorization, similarity computation, and profile-based predictions.\n",
        "\n",
        "**What This Script Does**\n",
        "\n",
        "* **Module 1–2**: Load essential libraries and enriched movie data.\n",
        "* **Module 3**: Load user ratings and demographics.\n",
        "* **Module 4**: Engineer features combining genres, cast, crew, keywords, and movie overviews.\n",
        "* **Module 5**: Transform content into TF-IDF, Count, or Binary vectors, and compute pairwise similarities using Cosine or Jaccard metrics.\n",
        "* **Module 6**: Construct a weighted content profile per user based on past ratings.\n",
        "* **Module 7**: Recommend top-N movies similar to the user profile, excluding already seen titles.\n",
        "\n",
        "**Techniques Used**\n",
        "\n",
        "* **Text Vectorization**: TF-IDF, CountVectorizer, Binary Count\n",
        "* **Similarity Metrics**: Cosine Similarity, Jaccard Similarity\n",
        "* **Personalization**: Weighted vector averaging based on each user’s rated items\n",
        "* **Parallelization**: Speeds up Jaccard similarity computation using joblib\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "* Personalized recommendations for new users with a few ratings (cold-start)\n",
        "* Improving diversity and relevance in suggested movies\n",
        "* Generating fallback content suggestions in hybrid recommender systems"
      ],
      "metadata": {
        "id": "c7KNZkOThu7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GeB2GWMyfPL",
        "outputId": "412ffdc4-3769-449c-c3fb-0420fee0d1d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: scikit-surprise==1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.1.4)\n",
            "Requirement already satisfied: scikit-learn==1.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: matplotlib==3.8.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.8.4)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
            "Requirement already satisfied: tqdm==4.66.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.66.4)\n",
            "Requirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise==1.1.4->-r requirements.txt (line 3)) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 2)) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Personalized Content-Based Movie Recommendation System Using Hybrid Textual Metadata and Multiple Similarity Models**\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "The goal of this project is to build a personalized movie recommendation system that leverages content-based filtering techniques using enriched movie metadata. By incorporating user rating data and multiple text-based similarity strategies, the system aims to generate relevant and diverse movie suggestions tailored to individual user preferences—especially in cold-start or sparsely rated scenarios.\n",
        "\n",
        "**Methodology**\n",
        "\n",
        "1. **Data Loading & Preparation**\n",
        "\n",
        "   * Movie metadata is loaded from an enriched dataset containing genres, keywords, cast, director, overview, and release year.\n",
        "   * User ratings and demographic data are loaded and used to personalize recommendations.\n",
        "\n",
        "2. **Feature Engineering**\n",
        "\n",
        "   * A composite text field (`cbf_features`) is created for each movie by concatenating cleaned metadata fields: genres, keywords, cast, director, overview, and year.\n",
        "\n",
        "3. **Vectorization**\n",
        "\n",
        "   * Three representations of movie content are generated:\n",
        "\n",
        "     * **TF-IDF Vectors**: Capture term importance within documents.\n",
        "     * **Count Vectors**: Raw term frequencies without weighting.\n",
        "     * **Binary Genre-Like Vectors**: For Jaccard similarity (1 if feature exists).\n",
        "\n",
        "4. **Similarity Computation**\n",
        "\n",
        "   * Cosine similarity is computed for TF-IDF and Count vectors.\n",
        "   * Jaccard similarity is computed for binary vectors using pairwise intersection-over-union.\n",
        "\n",
        "5. **User Profiling & Recommendation**\n",
        "\n",
        "   * For **TF-IDF** and **Count** models:\n",
        "\n",
        "     * A personalized **user profile vector** is created using a weighted average of vectors from rated movies.\n",
        "     * Recommendations are generated by finding unseen movies most similar to the user’s profile.\n",
        "   * For the **Binary + Jaccard** model:\n",
        "\n",
        "     * The average Jaccard similarity is computed between each unseen movie and the user’s seen movies.\n",
        "\n",
        "6. **Result Generation & Tagging**\n",
        "\n",
        "   * Top 50 movie recommendations are produced per user for each model.\n",
        "   * Each output is tagged with the model name: `\"TF-IDF + Cosine\"`, `\"Count + Cosine\"`, or `\"Binary + Jaccard\"`.\n",
        "\n",
        "7. **Output Consolidation**\n",
        "\n",
        "   * All recommendation outputs are combined into one labeled DataFrame for comparative analysis and visualization."
      ],
      "metadata": {
        "id": "vj82RTA_ldyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# STEP 1: Imports and Configuration\n",
        "# ==============================\n",
        "# Purpose: Import all necessary libraries required for data manipulation, text vectorization, similarity computation,\n",
        "# evaluation, and parallel processing. These are core components for implementing a content-based recommender system.\n",
        "\n",
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================\n",
        "# STEP 2: Load Movie and User Data\n",
        "# ==============================\n",
        "# Purpose: Load movie metadata (e.g., genres, overview, cast, etc.) and user rating data from local CSV/Dat files.\n",
        "# These datasets are essential for both feature creation (movie side) and personalization (user side).\n",
        "\n",
        "def load_movie_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    print(f\"Loaded {len(df)} movies.\")\n",
        "    return df\n",
        "\n",
        "def load_user_data(ratings_path, users_path):\n",
        "    ratings = pd.read_csv(ratings_path, sep=\"::\", engine=\"python\",\n",
        "                          names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "    users = pd.read_csv(users_path, sep=\"::\", engine=\"python\",\n",
        "                        names=[\"userId\", \"gender\", \"age\", \"occupation\", \"zip\"])\n",
        "    print(f\"Loaded {len(ratings)} ratings and {len(users)} users.\")\n",
        "    return ratings, users\n",
        "\n",
        "# ==============================\n",
        "# STEP 3: Feature Engineering\n",
        "# ==============================\n",
        "# Purpose: Create a single text string (cbf_features) per movie by combining genres, keywords, cast, director,\n",
        "# overview, and year. Deduplicate genres and clean other fields to improve vectorization quality.\n",
        "\n",
        "def create_feature_string(df):\n",
        "    # Extract and clean genre strings from both columns\n",
        "    genre_list_1 = df['genres'].fillna('').str.replace(r'\\s+', '', regex=True).str.split('|')\n",
        "    genre_list_2 = df['tmdb_genres'].fillna('').str.replace(r'\\s+', '', regex=True).str.split(',')\n",
        "\n",
        "    # Merge and deduplicate genres\n",
        "    merged_genres = [\n",
        "        ' '.join(sorted(set((g1 or []) + (g2 or []))))\n",
        "        for g1, g2 in zip(genre_list_1, genre_list_2)\n",
        "    ]\n",
        "\n",
        "    # Clean other textual fields\n",
        "    def clean(col):\n",
        "        return df[col].fillna('').astype(str).str.replace(',', ' ').str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "    keywords = clean('keywords')\n",
        "    cast = clean('top_3_cast') if 'top_3_cast' in df else ''\n",
        "    directors = clean('directors') if 'directors' in df else ''\n",
        "    overview = df['overview'].fillna('').astype(str).str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "    year = df['year'].astype(str).fillna('')\n",
        "\n",
        "    # Concatenate all fields into one text string per movie\n",
        "    df['cbf_features'] = [\n",
        "        f\"{genres} {kw} {c} {d} {o} {y}\".strip()\n",
        "        for genres, kw, c, d, o, y in zip(merged_genres, keywords, cast, directors, overview, year)\n",
        "    ]\n",
        "    return df\n",
        "\n",
        "# ==============================\n",
        "# STEP 4: Vectorization and Similarity Computation\n",
        "# ==============================\n",
        "# Purpose: Convert text features into numeric vectors using TF-IDF, CountVectorizer, and Binary encoding.\n",
        "# Then compute cosine and Jaccard similarity matrices to compare movies.\n",
        "\n",
        "def vectorize_features(feature_series, method='tfidf'):\n",
        "    if method == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    elif method == 'count':\n",
        "        vectorizer = CountVectorizer(stop_words='english')\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'tfidf' or 'count'\")\n",
        "    matrix = vectorizer.fit_transform(feature_series)\n",
        "    return matrix, vectorizer\n",
        "\n",
        "def binary_vectorize(feature_series):\n",
        "    token_lists = feature_series.apply(lambda x: x.split())\n",
        "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "    matrix = mlb.fit_transform(token_lists)\n",
        "    return matrix, mlb\n",
        "\n",
        "def compute_cosine_similarity(matrix):\n",
        "    return cosine_similarity(matrix)\n",
        "\n",
        "def jaccard_pairwise_parallel(matrix):\n",
        "    \"\"\"\n",
        "    Compute the Jaccard similarity matrix in parallel using matrix operations.\n",
        "    Args:\n",
        "        matrix (csr_matrix): A binary sparse matrix (e.g., multi-hot genre vectors)\n",
        "    Returns:\n",
        "        ndarray: Pairwise Jaccard similarity matrix\n",
        "    \"\"\"\n",
        "    A = matrix.astype(bool).astype(int)\n",
        "    intersection = A @ A.T\n",
        "    row_sums = np.asarray(A.sum(axis=1)).ravel()  # Fix: convert to ndarray\n",
        "    union = row_sums[:, None] + row_sums[None, :] - intersection.toarray()\n",
        "    jaccard = intersection.toarray() / np.maximum(union, 1e-10)  # Avoid division by zero\n",
        "    return jaccard\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# STEP 5: Build User Profile\n",
        "# ==============================\n",
        "# Purpose: Construct a personalized user profile vector by averaging the vectors of movies they've rated,\n",
        "# weighted by the user's actual ratings.\n",
        "\n",
        "def build_user_profile(user_id, ratings, tfidf_matrix, movie_df):\n",
        "    user_ratings = ratings[ratings['userId'] == user_id]\n",
        "    rated_movies = movie_df[movie_df['movieId'].isin(user_ratings['movieId'])]\n",
        "    indices = rated_movies.index.tolist()\n",
        "    weights = user_ratings.set_index('movieId').loc[rated_movies['movieId']]['rating'].values\n",
        "    profile = np.average(tfidf_matrix[indices].toarray(), axis=0, weights=weights)\n",
        "    return profile.reshape(1, -1)\n",
        "\n",
        "# ==============================\n",
        "# STEP 6: Recommendation Functions\n",
        "# ==============================\n",
        "# Purpose: Recommend movies to users by comparing either user profile to movie features,\n",
        "# or movies-to-movies using similarity matrices.\n",
        "\n",
        "def recommend_from_profile(user_id, ratings, tfidf_matrix, movie_df, model_label, top_n=50):\n",
        "    user_profile = build_user_profile(user_id, ratings, tfidf_matrix, movie_df)\n",
        "    sims = cosine_similarity(user_profile, tfidf_matrix).flatten()\n",
        "    user_seen = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(user_seen)].index\n",
        "    top_indices = unseen_indices[np.argsort(sims[unseen_indices])[-top_n:][::-1]]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'movieId': movie_df.iloc[top_indices]['movieId'].values,\n",
        "        'title': movie_df.iloc[top_indices]['title'].values,\n",
        "        'score': sims[top_indices],\n",
        "        'model': model_label\n",
        "    })\n",
        "\n",
        "def recommend_from_similarity_matrix(user_id, ratings, sim_matrix, movie_df, model_label, top_n=50):\n",
        "    seen_movie_ids = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    seen_indices = movie_df[movie_df['movieId'].isin(seen_movie_ids)].index.tolist()\n",
        "    unseen_indices = movie_df[~movie_df['movieId'].isin(seen_movie_ids)].index.tolist()\n",
        "\n",
        "    if not seen_indices:\n",
        "        print(f\"No ratings found for user {user_id}.\")\n",
        "        return pd.DataFrame(columns=['movieId', 'title', 'score', 'model'])\n",
        "\n",
        "    mean_sims = sim_matrix[unseen_indices][:, seen_indices].mean(axis=1)\n",
        "    top_indices = np.argsort(mean_sims)[-top_n:][::-1]\n",
        "    top_movie_indices = np.array(unseen_indices)[top_indices]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'movieId': movie_df.iloc[top_movie_indices]['movieId'].values,\n",
        "        'title': movie_df.iloc[top_movie_indices]['title'].values,\n",
        "        'score': mean_sims[top_indices],\n",
        "        'model': model_label\n",
        "    })\n",
        "\n",
        "# ==============================\n",
        "# STEP 7: Run Full Pipeline\n",
        "# ==============================\n",
        "# Purpose: Execute the end-to-end CBF pipeline: Load data, engineer features, vectorize them,\n",
        "# compute similarity matrices, and generate personalized movie recommendations.\n",
        "\n",
        "movie_df = load_movie_data(\"movies_enriched_full.csv\")\n",
        "ratings, users = load_user_data(\"ratings.dat\", \"users.dat\")\n",
        "movie_df.drop(columns=['trailer_link_y', 'backdrop_path_y', 'poster_path_y'], errors='ignore', inplace=True)\n",
        "\n",
        "# Create the final feature string used for vectorization\n",
        "movie_df = create_feature_string(movie_df)\n",
        "\n",
        "# Vectorize features using three different techniques\n",
        "tfidf_matrix, _ = vectorize_features(movie_df['cbf_features'], method='tfidf')\n",
        "count_matrix, _ = vectorize_features(movie_df['cbf_features'], method='count')\n",
        "binary_matrix, _ = binary_vectorize(movie_df['cbf_features'])\n",
        "\n",
        "# Compute similarity matrices\n",
        "sim_matrix_tfidf_cosine = compute_cosine_similarity(tfidf_matrix)\n",
        "sim_matrix_count_cosine = compute_cosine_similarity(count_matrix)\n",
        "sim_matrix_binary_jaccard = jaccard_pairwise_parallel(binary_matrix)\n",
        "\n",
        "# Generate recommendations for a sample user\n",
        "user_id = 5549\n",
        "df_tfidf_cosine = recommend_from_profile(user_id, ratings, tfidf_matrix, movie_df, \"TF-IDF + Cosine\")\n",
        "df_count_cosine = recommend_from_profile(user_id, ratings, count_matrix, movie_df, \"Count + Cosine\")\n",
        "df_binary_jaccard = recommend_from_similarity_matrix(user_id, ratings, sim_matrix_binary_jaccard, movie_df, \"Binary + Jaccard\")\n",
        "\n",
        "# Combine all model recommendations for analysis or output\n",
        "all_recommendations = pd.concat([df_tfidf_cosine, df_count_cosine, df_binary_jaccard], ignore_index=True)\n",
        "\n",
        "# Preview top recommendations\n",
        "print(\"Final Combined Recommendations:\")\n",
        "print(all_recommendations.head())\n"
      ],
      "metadata": {
        "id": "Zoh56bk5hyRT",
        "outputId": "9f1daa38-fa7f-45a4-d544-5dce1ddb35ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3883 movies.\n",
            "Loaded 1000209 ratings and 6040 users.\n",
            "Final Combined Recommendations:\n",
            "   movieId                                                     title  \\\n",
            "0     1221                            Godfather: Part II, The (1974)   \n",
            "1      293  Professional, The (a.k.a. Leon: The Professional) (1994)   \n",
            "2     3540                                    Passion of Mind (1999)   \n",
            "3     3568                      Smiling Fish and Goat on Fire (1999)   \n",
            "4     3907                        Prince of Central Park, The (1999)   \n",
            "\n",
            "      score            model  \n",
            "0  0.246621  TF-IDF + Cosine  \n",
            "1  0.226811  TF-IDF + Cosine  \n",
            "2  0.203520  TF-IDF + Cosine  \n",
            "3  0.191359  TF-IDF + Cosine  \n",
            "4  0.191359  TF-IDF + Cosine  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Memory-based collaborative filtering module (UBCF, IBCF)**\n",
        "\n",
        "***Purpose:***\n",
        "\n",
        "This module implements **memory-based collaborative filtering** using **user-user** or **item-item** similarity. It addresses **user bias** by normalizing ratings through mean-centering and optionally **rescaling predictions** to the original rating scale for interpretability.\n",
        "\n",
        "***Methodology:***\n",
        "\n",
        "1. **Rating Matrix Construction**:\n",
        "\n",
        "   * A user-item matrix is built from raw MovieLens-style ratings data.\n",
        "   * For `kind='user'`, ratings are mean-centered per user to reduce bias from lenient or strict raters.\n",
        "   * For `kind='item'`, raw ratings are used directly (no normalization), as the algorithm focuses on item similarities based on a single user's input.\n",
        "\n",
        "2. **Similarity Computation**:\n",
        "\n",
        "   * Cosine similarity is computed either:\n",
        "\n",
        "     * **Across users** for user-based CF (`kind='user'`)\n",
        "     * **Across items** for item-based CF (`kind='item'`)\n",
        "   * `sklearn.metrics.pairwise_distances` is used to derive similarity as `1 - cosine_distance`.\n",
        "\n",
        "3. **Prediction Generation**:\n",
        "\n",
        "   * For **user-based CF**:\n",
        "\n",
        "     * Ratings from similar users are weighted by similarity and averaged.\n",
        "     * The user’s mean rating is **added back** to restore predictions to the original scale (e.g., 1–5).\n",
        "   * For **item-based CF**:\n",
        "\n",
        "     * A user’s own ratings are used to compute scores for similar items.\n",
        "     * No mean is added back, since predictions are already on the correct scale.\n",
        "\n",
        "4. **Top-N Recommendations**:\n",
        "\n",
        "   * The system filters out movies the user has already rated.\n",
        "   * It ranks unseen movies by predicted score and returns the top-N recommendations.\n",
        "   * Each recommendation is labeled with the model type (`User-Based CF` or `Item-Based CF`) for downstream tracking."
      ],
      "metadata": {
        "id": "omg4Y6k5XmSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === STEP 1: Load Ratings ===\n",
        "# Purpose: Import user-movie ratings dataset and split it into training and test sets.\n",
        "# This allows us to train the model on one portion and evaluate on unseen data.\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# === STEP 2: Create Bias-Adjusted User-Item Matrix ===\n",
        "# Purpose: Create a user-item ratings matrix and adjust for biases by removing the global mean,\n",
        "# user bias (tendency to rate high/low), and item bias (popularity effects).\n",
        "def create_bias_adjusted_matrix(ratings_df):\n",
        "    matrix = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
        "    global_mean = ratings_df['rating'].mean()\n",
        "    user_bias = matrix.sub(global_mean, axis=0).mean(axis=1)\n",
        "    item_bias = matrix.sub(global_mean, axis=0).sub(user_bias, axis=0).mean(axis=0)\n",
        "    adjusted = matrix.sub(global_mean).sub(user_bias, axis=0).sub(item_bias, axis=1)\n",
        "    return adjusted.fillna(0), global_mean, user_bias, item_bias\n",
        "\n",
        "user_item_matrix, global_mean, user_bias, item_bias = create_bias_adjusted_matrix(train_ratings)\n",
        "\n",
        "# === STEP 3: Compute Similarity Matrices ===\n",
        "# Purpose: Measure similarity between users or items using cosine similarity on the bias-adjusted matrix.\n",
        "# These similarity scores will later help generate personalized recommendations.\n",
        "user_sim_matrix = cosine_similarity(user_item_matrix)\n",
        "item_sim_matrix = cosine_similarity(user_item_matrix.T)\n",
        "\n",
        "# === STEP 4: Recommender Function (Top-N or Full Scores) ===\n",
        "# Purpose: Generate movie recommendations using memory-based collaborative filtering.\n",
        "# Predict scores for unseen items using either user-based or item-based similarity and adjust with biases.\n",
        "def recommend_memory_based(user_id, matrix, global_mean, user_bias, item_bias, sim_matrix, kind='user', top_n=50, return_full=False):\n",
        "    if kind == 'user':\n",
        "        sim_scores = sim_matrix[matrix.index.get_loc(user_id)]\n",
        "        weighted = sim_scores @ matrix.values\n",
        "        norm = np.abs(sim_scores).sum()\n",
        "        preds = weighted / norm if norm != 0 else np.zeros_like(weighted)\n",
        "        preds += global_mean + user_bias.loc[user_id]\n",
        "    else:\n",
        "        user_vector = matrix.loc[user_id]\n",
        "        weighted = user_vector @ sim_matrix\n",
        "        norm = (user_vector != 0) @ np.abs(sim_matrix)\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            preds = np.true_divide(weighted, norm)\n",
        "            preds[norm == 0] = 0\n",
        "        preds += global_mean + user_bias.loc[user_id] + item_bias.values\n",
        "\n",
        "    preds = np.clip(preds, 1.0, 5.0)\n",
        "    pred_series = pd.Series(preds, index=matrix.columns)\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    pred_series = pred_series.drop(labels=seen, errors='ignore')\n",
        "\n",
        "    if return_full:\n",
        "        return pred_series\n",
        "    else:\n",
        "        top_preds = pred_series.sort_values(ascending=False).head(top_n)\n",
        "        return pd.DataFrame({\n",
        "            'userId': user_id,\n",
        "            'movieId': top_preds.index,\n",
        "            'score': top_preds.values\n",
        "        })\n",
        "\n",
        "# === STEP 5: Evaluation Function ===\n",
        "# Purpose: Evaluate model performance using RMSE by comparing predicted scores to actual ratings\n",
        "# in the test set for multiple users. Measures how accurate the recommender is overall.\n",
        "def evaluate_model(test_df, matrix, global_mean, user_bias, item_bias, sim_matrix, kind='user'):\n",
        "    all_preds = []\n",
        "    for uid in test_df['userId'].unique():\n",
        "        if uid not in matrix.index:\n",
        "            continue\n",
        "        recs = recommend_memory_based(uid, matrix, global_mean, user_bias, item_bias, sim_matrix, kind, top_n=1000, return_full=True)\n",
        "        actual = test_df[test_df['userId'] == uid]\n",
        "        merged = pd.merge(actual, recs.rename(\"score\"), on=\"movieId\")\n",
        "        all_preds.append(merged)\n",
        "\n",
        "    all_preds_df = pd.concat(all_preds, ignore_index=True)\n",
        "    rmse = np.sqrt(mean_squared_error(all_preds_df['rating'], all_preds_df['score'])) if not all_preds_df.empty else np.nan\n",
        "    return rmse\n",
        "\n",
        "# === STEP 6: Run Evaluation ===\n",
        "# Purpose: Calculate RMSE for user-based CF, item-based CF, and a dummy predictor\n",
        "# that always predicts the global mean rating.\n",
        "user_rmse = evaluate_model(test_ratings, user_item_matrix, global_mean, user_bias, item_bias, user_sim_matrix, 'user')\n",
        "item_rmse = evaluate_model(test_ratings, user_item_matrix, global_mean, user_bias, item_bias, item_sim_matrix, 'item')\n",
        "dummy_rmse = np.sqrt(mean_squared_error(test_ratings['rating'], [global_mean] * len(test_ratings)))\n",
        "\n",
        "print(f\"User-Based CF RMSE: {user_rmse:.4f}\")\n",
        "print(f\"Item-Based CF RMSE: {item_rmse:.4f}\")\n",
        "print(f\"Dummy Predictor RMSE: {dummy_rmse:.4f}\")\n",
        "\n",
        "# === STEP 7: Get Recommendations for a Specific User (Optional) ===\n",
        "# Purpose: Generate and display the top-N recommended movies for a target user using both models.\n",
        "# This is useful for presenting personalized suggestions.\n",
        "user_id = 5549\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "user_recs = recommend_memory_based(user_id, user_item_matrix, global_mean, user_bias, item_bias, user_sim_matrix, 'user', top_n=50)\n",
        "item_recs = recommend_memory_based(user_id, user_item_matrix, global_mean, user_bias, item_bias, item_sim_matrix, 'item', top_n=50)\n",
        "\n",
        "user_recs = user_recs.merge(movies, on='movieId', how='left')\n",
        "item_recs = item_recs.merge(movies, on='movieId', how='left')\n",
        "\n",
        "print(\"\\nTop 10 User-Based CF Recommendations:\")\n",
        "print(user_recs[['movieId', 'title', 'score']])\n",
        "\n",
        "print(\"\\nTop 10 Item-Based CF Recommendations:\")\n",
        "print(item_recs[['movieId', 'title', 'score']])\n"
      ],
      "metadata": {
        "id": "BmjSwcUPB0wm",
        "outputId": "e698818e-8fbe-44ff-a15f-54f327285d84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-Based CF RMSE: 1.0336\n",
            "Item-Based CF RMSE: 0.8796\n",
            "Dummy Predictor RMSE: 1.1197\n",
            "\n",
            "Top 10 User-Based CF Recommendations:\n",
            "    movieId                                          title     score\n",
            "0      2701                          Wild Wild West (1999)  3.608871\n",
            "1      1917                              Armageddon (1998)  3.608796\n",
            "2      1721                                 Titanic (1997)  3.607812\n",
            "3      3753                            Patriot, The (2000)  3.605514\n",
            "4      2881                         Double Jeopardy (1999)  3.604429\n",
            "5      2722                           Deep Blue Sea (1999)  3.602775\n",
            "6       736                                 Twister (1996)  3.602065\n",
            "7      3113                             End of Days (1999)  3.601182\n",
            "8       780                  Independence Day (ID4) (1996)  3.600492\n",
            "9      1101                                 Top Gun (1986)  3.600417\n",
            "10     2724                           Runaway Bride (1999)  3.600414\n",
            "11     2605                              Entrapment (1999)  3.600344\n",
            "12     2393                 Star Trek: Insurrection (1998)  3.600233\n",
            "13      500                          Mrs. Doubtfire (1993)  3.599968\n",
            "14     3354                         Mission to Mars (2000)  3.599791\n",
            "15     1562                          Batman & Robin (1997)  3.599633\n",
            "16     1088                           Dirty Dancing (1987)  3.599000\n",
            "17      317                       Santa Clause, The (1994)  3.598909\n",
            "18     2694                               Big Daddy (1999)  3.598171\n",
            "19     1882                                Godzilla (1998)  3.598069\n",
            "20     1918                         Lethal Weapon 4 (1998)  3.597729\n",
            "21       95                            Broken Arrow (1996)  3.597312\n",
            "22      329                  Star Trek: Generations (1994)  3.597004\n",
            "23      587                                   Ghost (1990)  3.596801\n",
            "24     1372  Star Trek VI: The Undiscovered Country (1991)  3.596783\n",
            "25     2861                    For Love of the Game (1999)  3.596702\n",
            "26     1380                                  Grease (1978)  3.596600\n",
            "27      653                             Dragonheart (1996)  3.596559\n",
            "28     1370                              Die Hard 2 (1990)  3.596436\n",
            "29     2424                         You've Got Mail (1998)  3.596149\n",
            "30     1385                             Under Siege (1992)  3.595732\n",
            "31     1876                             Deep Impact (1998)  3.595710\n",
            "32     1833                          Mercury Rising (1998)  3.595533\n",
            "33      648                     Mission: Impossible (1996)  3.595404\n",
            "34      204           Under Siege 2: Dark Territory (1995)  3.595376\n",
            "35     1608                           Air Force One (1997)  3.595333\n",
            "36     1544          Lost World: Jurassic Park, The (1997)  3.595304\n",
            "37      920                      Gone with the Wind (1939)  3.595274\n",
            "38     1792                          U.S. Marshalls (1998)  3.595214\n",
            "39     2082                       Mighty Ducks, The (1992)  3.595168\n",
            "40     3441                                Red Dawn (1984)  3.595108\n",
            "41      786                                  Eraser (1996)  3.595083\n",
            "42     3257                          Bodyguard, The (1992)  3.594908\n",
            "43     1911                         Doctor Dolittle (1998)  3.594795\n",
            "44     2002                         Lethal Weapon 3 (1992)  3.594773\n",
            "45     2431                             Patch Adams (1998)  3.594655\n",
            "46      434                             Cliffhanger (1993)  3.594628\n",
            "47      350                             Client, The (1994)  3.594498\n",
            "48     3484                             Skulls, The (2000)  3.594466\n",
            "49     3157                           Stuart Little (1999)  3.594267\n",
            "\n",
            "Top 10 Item-Based CF Recommendations:\n",
            "    movieId                                              title     score\n",
            "0       557                                  Mamma Roma (1962)  5.000000\n",
            "1       657                                 Yankee Zulu (1994)  5.000000\n",
            "2      3601                        Castaway Cowboy, The (1974)  5.000000\n",
            "3       989          Schlafes Bruder (Brother of Sleep) (1995)  5.000000\n",
            "4      3517                                  Bells, The (1926)  5.000000\n",
            "5       729  Institute Benjamenta, or This Dream People Cal...  5.000000\n",
            "6      2591  Jeanne and the Perfect Guy (Jeanne et le garço...  5.000000\n",
            "7      1787  Paralyzing Fear: The Story of Polio in America...  4.865259\n",
            "8      3323                              Chain of Fools (2000)  4.825758\n",
            "9       787                 Gate of Heavenly Peace, The (1995)  4.823364\n",
            "10      846                                       Flirt (1995)  4.810221\n",
            "11     2930                           Return with Honor (1998)  4.737577\n",
            "12     2503                            Apple, The (Sib) (1998)  4.720328\n",
            "13     1423                            Hearts and Minds (1996)  4.696143\n",
            "14     3077                                       42 Up (1998)  4.692268\n",
            "15      649                 Cold Fever (Á köldum klaka) (1994)  4.682977\n",
            "16      758                          Jar, The (Khomreh) (1992)  4.680499\n",
            "17     3675                             White Christmas (1954)  4.677749\n",
            "18     1830                            Follow the Bitch (1998)  4.670290\n",
            "19      914                                My Fair Lady (1964)  4.658313\n",
            "20     1780                   Ayn Rand: A Sense of Life (1997)  4.656146\n",
            "21      789      I, Worst of All (Yo, la peor de todas) (1990)  4.653965\n",
            "22     2114                              Outsiders, The (1983)  4.642850\n",
            "23      578                        Hour of the Pig, The (1993)  4.637901\n",
            "24     1936                                Mrs. Miniver (1942)  4.632838\n",
            "25     1664                             Nénette et Boni (1996)  4.628931\n",
            "26     2501                                 October Sky (1999)  4.623253\n",
            "27     1546                                 Schizopolis (1996)  4.621047\n",
            "28      980                       In the Line of Duty 2 (1987)  4.618551\n",
            "29     1636                                        Stag (1997)  4.613790\n",
            "30     3338                             For All Mankind (1989)  4.606766\n",
            "31     2206                                   Suspicion (1941)  4.602309\n",
            "32     1226                              Quiet Man, The (1952)  4.597518\n",
            "33     3410                                  Soft Fruit (1999)  4.591307\n",
            "34     2342                              Hard Core Logo (1996)  4.585817\n",
            "35     3233                               Smashing Time (1967)  4.583333\n",
            "36     3607                           One Little Indian (1973)  4.575459\n",
            "37     3090                                     Matewan (1987)  4.565653\n",
            "38     3811                              Breaker Morant (1980)  4.562671\n",
            "39     2745                                Mission, The (1986)  4.560557\n",
            "40     2175                                     Déjà Vu (1997)  4.560102\n",
            "41     3849                       Spiral Staircase, The (1946)  4.553369\n",
            "42       22                                     Copycat (1995)  4.552960\n",
            "43      134                               Sonic Outlaws (1995)  4.542083\n",
            "44      920                          Gone with the Wind (1939)  4.539308\n",
            "45     1944                       From Here to Eternity (1953)  4.534718\n",
            "46      668                             Pather Panchali (1955)  4.533705\n",
            "47     3360                                    Hoosiers (1986)  4.532066\n",
            "48     3135                          Great Santini, The (1979)  4.528566\n",
            "49     3880                Ballad of Ramblin' Jack, The (2000)  4.527764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid CBF and UBCF Model"
      ],
      "metadata": {
        "id": "zPb1m8iNIdAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Hybrid Recommender: UBCF + CBF\n",
        "# ==============================\n",
        "\n",
        "def hybrid_ubcf_cbf(user_id, user_item_matrix, user_means, user_sim_matrix,\n",
        "                    tfidf_matrix, ratings, movie_df,\n",
        "                    w_cf=0.2, w_cbf=0.8, top_n=50):\n",
        "    \"\"\"\n",
        "    Combine UBCF and CBF scores via weighted average.\n",
        "\n",
        "    Parameters:\n",
        "    - user_id: int\n",
        "    - user_item_matrix: pd.DataFrame (mean-centered matrix)\n",
        "    - user_means: pd.Series\n",
        "    - user_sim_matrix: np.array\n",
        "    - tfidf_matrix: sparse matrix from TF-IDF\n",
        "    - ratings: pd.DataFrame\n",
        "    - movie_df: pd.DataFrame with movieId, title\n",
        "    - w_cf: float, weight for UBCF\n",
        "    - w_cbf: float, weight for CBF\n",
        "    - top_n: int\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with movieId, title, hybrid_score, model\n",
        "    \"\"\"\n",
        "    # --- UBCF predictions ---\n",
        "    ubcf_df = recommend_memory_based(\n",
        "        user_id=user_id,\n",
        "        user_item_matrix=user_item_matrix,\n",
        "        user_means=user_means,\n",
        "        similarity_matrix=user_sim_matrix,\n",
        "        kind='user',\n",
        "        top_n=1000  # keep more to allow intersection\n",
        "    )\n",
        "\n",
        "    # --- CBF predictions ---\n",
        "    cbf_df = recommend_from_profile(\n",
        "        user_id=user_id,\n",
        "        ratings=ratings,\n",
        "        tfidf_matrix=tfidf_matrix,\n",
        "        movie_df=movie_df,\n",
        "        model_label='CBF',  # temporary label\n",
        "        top_n=1000\n",
        "    ).rename(columns={'score': 'cbf_score'})\n",
        "\n",
        "    # --- Merge ---\n",
        "    ubcf_df = ubcf_df.rename(columns={'score': 'ubcf_score'})\n",
        "    merged = pd.merge(ubcf_df, cbf_df, on='movieId')\n",
        "\n",
        "    # --- Combine Scores ---\n",
        "    merged['hybrid_score'] = w_cf * merged['ubcf_score'] + w_cbf * merged['cbf_score']\n",
        "    hybrid_df = merged[['movieId', 'title', 'hybrid_score']].copy()\n",
        "    hybrid_df['model'] = 'Hybrid (UBCF + CBF)'\n",
        "\n",
        "    return hybrid_df.sort_values(by='hybrid_score', ascending=False).head(top_n)[\n",
        "        ['movieId', 'title', 'hybrid_score', 'model']\n",
        "    ]\n",
        "\n",
        "# ==============================\n",
        "# Generate Hybrid Recommendations for User 5549\n",
        "# ==============================\n",
        "\n",
        "hybrid_recs = hybrid_ubcf_cbf(\n",
        "    user_id=5549,\n",
        "    user_item_matrix=user_item_matrix,\n",
        "    user_means=user_means,\n",
        "    user_sim_matrix=user_sim_matrix,\n",
        "    tfidf_matrix=tfidf_matrix_tfidf,\n",
        "    ratings=ratings,\n",
        "    movie_df=movie_df,\n",
        "    w_cf=0.5,\n",
        "    w_cbf=0.5,\n",
        "    top_n=50\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# Display Output\n",
        "# ==============================\n",
        "\n",
        "print(\"\\nTop 50 Hybrid Recommendations for User 5549:\")\n",
        "print(hybrid_recs.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqR9UEVpIlUW",
        "outputId": "b4c5877a-110b-4964-a33b-6af3b59bb71f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 50 Hybrid Recommendations for User 5549:\n",
            "     movieId                              title  hybrid_score  \\\n",
            "0       1221     Godfather: Part II, The (1974)      1.894670   \n",
            "192     3457             Waking the Dead (1999)      1.866184   \n",
            "155     3177                 Next Friday (1999)      1.845677   \n",
            "215     3721                      Trixie (1999)      1.844950   \n",
            "37      2995  House on Haunted Hill, The (1999)      1.831576   \n",
            "\n",
            "                   model  \n",
            "0    Hybrid (UBCF + CBF)  \n",
            "192  Hybrid (UBCF + CBF)  \n",
            "155  Hybrid (UBCF + CBF)  \n",
            "215  Hybrid (UBCF + CBF)  \n",
            "37   Hybrid (UBCF + CBF)  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model-Based Filtering:**\n",
        "\n",
        "  * *SVD (Surprise)*: Learns latent features from the rating matrix.\n",
        "  * *ALS (PySpark)*: Scalable factorization method for large datasets.\n"
      ],
      "metadata": {
        "id": "R6d3SP0OolH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 9: Model-Based Collaborative Filtering (SVD using Surprise)**\n",
        "\n",
        "**Purpose:**\n",
        "Use matrix factorization (SVD) to learn latent user/item features from the rating matrix.\n",
        "\n",
        "**Application:**\n",
        "- Accurate, scalable recommendations for sparse datasets using user/item embeddings.\n",
        "- Suitable for small to medium datasets.\n",
        "- Optimized via `GridSearchCV` for hyperparameter tuning.\n",
        "- Good interpretability of latent factors per user and item.\n",
        "\n"
      ],
      "metadata": {
        "id": "4dCPxrNMnQkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 9: Model-Based Collaborative Filtering (SVD using Surprise)\n",
        "# ==============================\n",
        "\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split, GridSearchCV\n",
        "from surprise.accuracy import rmse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================\n",
        "# Prepare Surprise Dataset\n",
        "# ==============================\n",
        "\n",
        "def prepare_surprise_data(ratings):\n",
        "    reader = Reader(rating_scale=(0.5, 5.0))\n",
        "    return Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# ==============================\n",
        "# Tune SVD Model with Grid Search\n",
        "# ==============================\n",
        "\n",
        "def tune_svd_model(data):\n",
        "    param_grid = {\n",
        "        'n_factors': [50, 100],\n",
        "        'lr_all': [0.005, 0.01],\n",
        "        'reg_all': [0.02, 0.1]\n",
        "    }\n",
        "    print(\"Tuning SVD model with GridSearchCV...\")\n",
        "    gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, joblib_verbose=0)\n",
        "\n",
        "    with tqdm(total=1, desc=\"GridSearchCV\") as pbar:\n",
        "        gs.fit(data)\n",
        "        pbar.update(1)\n",
        "\n",
        "    print(f\"Best RMSE: {gs.best_score['rmse']} with params: {gs.best_params['rmse']}\")\n",
        "    return gs.best_estimator['rmse']\n",
        "\n",
        "# ==============================\n",
        "# Train and Evaluate SVD\n",
        "# ==============================\n",
        "\n",
        "def evaluate_svd(model, data, model_label='SVD (Surprise)'):\n",
        "    trainset, testset = train_test_split(data, test_size=0.2)\n",
        "    model.fit(trainset)\n",
        "\n",
        "    print(\"Making predictions...\")\n",
        "    predictions = [model.predict(item[0], item[1], r_ui=item[2]) for item in tqdm(testset, desc=\"Predicting\")]\n",
        "\n",
        "    score = rmse(predictions)\n",
        "\n",
        "    pred_df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
        "    pred_df = pred_df.rename(columns={'uid': 'userId', 'iid': 'movieId', 'rui': 'true_rating', 'est': 'pred_rating'})\n",
        "    pred_df['model'] = model_label\n",
        "    return pred_df[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']], score\n",
        "\n",
        "# ==============================\n",
        "# Main Execution\n",
        "# ==============================\n",
        "\n",
        "# Step 1: Load ratings\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# Step 2: Prepare Surprise data\n",
        "data = prepare_surprise_data(ratings)\n",
        "\n",
        "# Step 3: Tune model\n",
        "best_svd_model = tune_svd_model(data)\n",
        "\n",
        "# Step 4: Evaluate model\n",
        "pred_df, rmse_score = evaluate_svd(best_svd_model, data)\n",
        "\n",
        "# Step 5: Output\n",
        "print(pred_df.head())\n",
        "print(f\"Final RMSE: {rmse_score:.4f}\")\n",
        "\n",
        "# Step 6: Top-50 Predictions for User 5549\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = [mid for mid in all_movie_ids if mid not in rated_movie_ids]\n",
        "\n",
        "print(f\"\\nGenerating predictions for User {target_user}...\")\n",
        "top_preds = [(movie_id, best_svd_model.predict(target_user, movie_id).est)\n",
        "             for movie_id in tqdm(unrated_movie_ids, desc=\"Predicting for user\")]\n",
        "\n",
        "top_50_df = pd.DataFrame(top_preds, columns=['movieId', 'pred_rating'])\n",
        "top_50_df = top_50_df.sort_values(by='pred_rating', ascending=False).head(50)\n",
        "top_50_df['userId'] = target_user\n",
        "top_50_df['model'] = 'SVD (Surprise)'\n",
        "top_50_df = top_50_df[['userId', 'movieId', 'pred_rating', 'model']]\n",
        "\n",
        "# Step 7: Merge with movie titles only\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_df = top_50_df.merge(movies, on='movieId', how='left')\n",
        "\n",
        "# Step 8: Final Output\n",
        "print(\"\\nTop 50 Recommendations for User 5549:\")\n",
        "print(top_50_df[['movieId', 'title', 'pred_rating']].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0JR9B0YokAB",
        "outputId": "f31dbe34-42f8-4e19-b989-bd5ede2e7e29"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning SVD model with GridSearchCV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GridSearchCV: 100%|██████████| 1/1 [06:38<00:00, 398.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE: 0.8820577922491172 with params: {'n_factors': 50, 'lr_all': 0.005, 'reg_all': 0.02}\n",
            "Making predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 200042/200042 [00:02<00:00, 95750.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.8701\n",
            "   userId  movieId  true_rating  pred_rating           model\n",
            "0    1470     2873          1.0     2.022161  SVD (Surprise)\n",
            "1    1974     3201          4.0     4.217697  SVD (Surprise)\n",
            "2    2825     2384          5.0     3.837053  SVD (Surprise)\n",
            "3     462     2640          3.0     3.146119  SVD (Surprise)\n",
            "4    1937      858          5.0     4.142713  SVD (Surprise)\n",
            "Final RMSE: 0.8701\n",
            "\n",
            "Generating predictions for User 5549...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting for user: 100%|██████████| 3673/3673 [00:00<00:00, 173473.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 50 Recommendations for User 5549:\n",
            "   movieId  \\\n",
            "0      911   \n",
            "1     2905   \n",
            "2     1262   \n",
            "3     1207   \n",
            "4     2019   \n",
            "5      920   \n",
            "6     1272   \n",
            "7      913   \n",
            "8     3338   \n",
            "9      318   \n",
            "\n",
            "                                                                 title  \\\n",
            "0                                                       Charade (1963)   \n",
            "1                                                       Sanjuro (1962)   \n",
            "2                                             Great Escape, The (1963)   \n",
            "3                                         To Kill a Mockingbird (1962)   \n",
            "4  Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)   \n",
            "5                                            Gone with the Wind (1939)   \n",
            "6                                                        Patton (1970)   \n",
            "7                                           Maltese Falcon, The (1941)   \n",
            "8                                               For All Mankind (1989)   \n",
            "9                                     Shawshank Redemption, The (1994)   \n",
            "\n",
            "   pred_rating  \n",
            "0     4.425615  \n",
            "1     4.385578  \n",
            "2     4.378872  \n",
            "3     4.369493  \n",
            "4     4.346556  \n",
            "5     4.294037  \n",
            "6     4.292043  \n",
            "7     4.281435  \n",
            "8     4.272779  \n",
            "9     4.260661  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model-Based Collaborative Filtering (ALS using PySpark)**\n",
        "\n",
        "**Purpose:**\n",
        "Use Alternating Least Squares (ALS) to learn latent user/item features at scale.\n",
        "\n",
        "**Application:**\n",
        "- Distributed recommendation system for large-scale datasets.\n",
        "- Runs on Apache Spark for horizontal scalability.\n",
        "- Handles sparsity well using factorization.\n",
        "- Suited for real-time, production-level systems with massive data.\n"
      ],
      "metadata": {
        "id": "DeCmPC_DnZip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 10: Model-Based Collaborative Filtering (ALS using PySpark)\n",
        "# ==============================\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import Row\n",
        "import pandas as pd\n",
        "\n",
        "# --- Start Spark Session ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ALSModel\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Load Ratings ---\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "ratings_df = spark.createDataFrame(ratings[['userId', 'movieId', 'rating']])\n",
        "\n",
        "# --- Train ALS Model ---\n",
        "als = ALS(\n",
        "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "    rank=10, maxIter=10, regParam=0.1,\n",
        "    coldStartStrategy=\"drop\", nonnegative=True\n",
        ")\n",
        "als_model = als.fit(ratings_df)\n",
        "\n",
        "# --- Evaluate ALS Model ---\n",
        "predictions = als_model.transform(ratings_df)\n",
        "pred_pd = predictions.select('userId', 'movieId', 'rating', 'prediction').toPandas()\n",
        "pred_pd = pred_pd.rename(columns={'rating': 'true_rating', 'prediction': 'pred_rating'})\n",
        "pred_pd['model'] = 'ALS (PySpark)'\n",
        "\n",
        "# --- Evaluate ALS RMSE ---\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName='rmse',\n",
        "    labelCol='rating',\n",
        "    predictionCol='prediction'\n",
        ")\n",
        "rmse_score = evaluator.evaluate(predictions)\n",
        "\n",
        "# --- Output Evaluation ---\n",
        "print(pred_pd[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']].head())\n",
        "print(f\"\\nFinal RMSE: {rmse_score:.4f}\")\n",
        "\n",
        "# ==============================\n",
        "# Step 6: Top-50 Predictions for User 5549\n",
        "# ==============================\n",
        "\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = list(set(all_movie_ids) - set(rated_movie_ids))\n",
        "\n",
        "# Create Spark DataFrame of userId + unrated movieId pairs\n",
        "user_unrated_pairs = spark.createDataFrame([Row(userId=target_user, movieId=int(mid)) for mid in unrated_movie_ids])\n",
        "\n",
        "# Predict ratings using ALS model\n",
        "print(f\"\\nGenerating Top-50 recommendations for User {target_user}...\")\n",
        "top_preds_df = als_model.transform(user_unrated_pairs).dropna()\n",
        "\n",
        "# Get top-50 highest predicted ratings\n",
        "top_50_preds = top_preds_df.orderBy(col(\"prediction\").desc()).limit(50)\n",
        "top_50_pd = top_50_preds.select(\"userId\", \"movieId\", \"prediction\").toPandas()\n",
        "top_50_pd['model'] = \"ALS (PySpark)\"\n",
        "top_50_pd = top_50_pd.rename(columns={'prediction': 'pred_rating'})\n",
        "\n",
        "# ==============================\n",
        "# Step 7: Merge with Movie Titles Only\n",
        "# ==============================\n",
        "\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_pd = top_50_pd.merge(movies, on='movieId', how='left')\n",
        "\n",
        "# ==============================\n",
        "# Step 8: Output Top-50\n",
        "# ==============================\n",
        "\n",
        "print(\"\\nTop 50 Recommendations for User 5549:\")\n",
        "print(top_50_pd[['movieId', 'title', 'pred_rating']].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLtk_Rsuom7R",
        "outputId": "5ae78569-65a9-4e28-ef1d-773016e71472"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   userId  movieId  true_rating  pred_rating          model\n",
            "0     148     2122            4     2.768286  ALS (PySpark)\n",
            "1     148     2142            4     3.385553  ALS (PySpark)\n",
            "2     148     2366            5     3.547921  ALS (PySpark)\n",
            "3     148     3175            5     3.873491  ALS (PySpark)\n",
            "4     148     1580            4     4.024727  ALS (PySpark)\n",
            "\n",
            "Final RMSE: 0.8357\n",
            "\n",
            "Generating Top-50 recommendations for User 5549...\n",
            "\n",
            "Top 50 Recommendations for User 5549:\n",
            "   movieId                            title  pred_rating\n",
            "0      572           Foreign Student (1994)     5.054198\n",
            "1     1471               Boys Life 2 (1997)     4.919178\n",
            "2     2760  Gambler, The (A Játékos) (1997)     4.464029\n",
            "3      953     It's a Wonderful Life (1946)     4.359691\n",
            "4     1519            Broken English (1996)     4.351626\n",
            "5     2503          Apple, The (Sib) (1998)     4.329711\n",
            "6     2129     Saltmen of Tibet, The (1997)     4.324127\n",
            "7      912                Casablanca (1942)     4.317692\n",
            "8      751                   Careful (1992)     4.312503\n",
            "9     3365            Searchers, The (1956)     4.302980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ls6RqljaolUG"
      }
    }
  ]
}