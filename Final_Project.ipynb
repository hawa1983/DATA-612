{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-612/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose**\n",
        "\n",
        "The purpose of this script is to enrich the MovieLens movie dataset (`movies.dat`) with detailed movie metadata from The Movie Database (TMDB) API. This metadata includes movie overviews, genres, poster and backdrop image URLs, cast and director information, keywords, user ratings, and trailer links. The enriched dataset will serve as the foundation for building content-based, collaborative, and hybrid recommender systems.\n",
        "\n",
        "### **Methodology**\n",
        "\n",
        "1. **Load MovieLens Movie Data**\n",
        "   The script loads the `movies.dat` file, which contains basic movie information including `movieId`, `title`, and `genres`.\n",
        "\n",
        "2. **Clean Titles and Extract Years**\n",
        "   It processes the movie titles to remove the year from the title string and separately extracts the release year to improve search accuracy when querying TMDB.\n",
        "\n",
        "3. **Query TMDB API**\n",
        "   For each movie, it sends a search request to TMDB using the cleaned title and release year. If a match is found, it retrieves the movieâ€™s TMDB ID.\n",
        "\n",
        "4. **Retrieve Detailed Metadata**\n",
        "   Using the TMDB ID, the script fetches:\n",
        "\n",
        "   * Overview (plot summary)\n",
        "   * Poster and backdrop image paths\n",
        "   * Genre IDs, which are then mapped to readable genre names\n",
        "   * Top 3 cast members\n",
        "   * Director(s)\n",
        "   * Associated keywords\n",
        "   * YouTube trailer link (if available)\n",
        "\n",
        "5. **Construct and Save Enriched Dataset**\n",
        "   All metadata is compiled into a structured format and merged with the original MovieLens data. The final dataset is saved as `movies_enriched_full.csv` for downstream use in recommendation models.\n"
      ],
      "metadata": {
        "id": "OptAJ12iBc8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qJ0maYGaxtJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ---------------------------------------\n",
        "# CONFIG\n",
        "# ---------------------------------------\n",
        "BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "IMAGE_BASE = \"https://image.tmdb.org/t/p/w500\"\n",
        "\n",
        "# Use your TMDB Bearer Token (v4)\n",
        "HEADERS = {\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyZGZlNjMwMGMzYjIzMjc2NzExNjQ0N2JhNzhiMjM5MyIsIm5iZiI6MTc1MTkyMjA3Ni4xMzUsInN1YiI6IjY4NmMzNTljMzc4NjllOGEyNDUxZTM0OSIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.S773ddH3FiIHtokPW4sYpJog0mXWS1o4OPov1KZneUw\"\n",
        "}\n",
        "\n",
        "# TMDB genre ID to name mapping\n",
        "GENRE_ID_TO_NAME = {\n",
        "    28: \"Action\", 12: \"Adventure\", 16: \"Animation\", 35: \"Comedy\", 80: \"Crime\",\n",
        "    99: \"Documentary\", 18: \"Drama\", 10751: \"Family\", 14: \"Fantasy\", 36: \"History\",\n",
        "    27: \"Horror\", 10402: \"Music\", 9648: \"Mystery\", 10749: \"Romance\", 878: \"Science Fiction\",\n",
        "    10770: \"TV Movie\", 53: \"Thriller\", 10752: \"War\", 37: \"Western\"\n",
        "}\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 1: Load MovieLens .dat Files\n",
        "# ---------------------------------------\n",
        "\n",
        "# Load movies.dat - format: MovieID::Title::Genres\n",
        "movies_df = pd.read_csv(\"movies.dat\", sep=\"::\", engine='python', header=None, names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 2: Clean Movie Titles and Extract Year\n",
        "# ---------------------------------------\n",
        "\n",
        "def extract_year(title):\n",
        "    if \"(\" in title:\n",
        "        try:\n",
        "            return int(title.strip()[-5:-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_title(title):\n",
        "    if \"(\" in title:\n",
        "        return title[:title.rfind(\"(\")].strip()\n",
        "    return title.strip()\n",
        "\n",
        "movies_df[\"year\"] = movies_df[\"title\"].apply(extract_year)\n",
        "movies_df[\"clean_title\"] = movies_df[\"title\"].apply(clean_title)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 3: TMDB Metadata Functions\n",
        "# ---------------------------------------\n",
        "\n",
        "# Search for movie in TMDB\n",
        "def search_tmdb(title, year):\n",
        "    url = f\"{BASE_URL}/search/movie\"\n",
        "    params = {\"query\": title, \"year\": year}\n",
        "    response = requests.get(url, headers=HEADERS, params=params)\n",
        "    r = response.json()\n",
        "    if r.get(\"results\"):\n",
        "        return r[\"results\"][0]\n",
        "    return None\n",
        "\n",
        "# Get full metadata from TMDB\n",
        "def get_full_tmdb_metadata(tmdb_id):\n",
        "    metadata = {}\n",
        "\n",
        "    # Credits (cast, crew)\n",
        "    credits = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/credits\", headers=HEADERS).json()\n",
        "    cast = [c[\"name\"] for c in credits.get(\"cast\", [])[:3]]\n",
        "    directors = [c[\"name\"] for c in credits.get(\"crew\", []) if c.get(\"job\") == \"Director\"]\n",
        "\n",
        "    # Keywords\n",
        "    keywords = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/keywords\", headers=HEADERS).json()\n",
        "    keyword_list = [k[\"name\"] for k in keywords.get(\"keywords\", [])]\n",
        "\n",
        "    # Videos (trailers)\n",
        "    videos = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/videos\", headers=HEADERS).json()\n",
        "    trailer_links = [\n",
        "        f\"https://www.youtube.com/watch?v={v['key']}\"\n",
        "        for v in videos.get(\"results\", [])\n",
        "        if v[\"site\"] == \"YouTube\" and v[\"type\"] == \"Trailer\"\n",
        "    ]\n",
        "\n",
        "    # Final metadata dictionary\n",
        "    metadata[\"top_3_cast\"] = \", \".join(cast)\n",
        "    metadata[\"directors\"] = \", \".join(directors)\n",
        "    metadata[\"keywords\"] = \", \".join(keyword_list)\n",
        "    metadata[\"trailer_link\"] = trailer_links[0] if trailer_links else None\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 4: Enrich Movie Data\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched = []\n",
        "\n",
        "for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
        "    movie_data = search_tmdb(row[\"clean_title\"], row[\"year\"])\n",
        "\n",
        "    if movie_data:\n",
        "        tmdb_id = movie_data[\"id\"]\n",
        "        extra = get_full_tmdb_metadata(tmdb_id)\n",
        "\n",
        "        genre_ids = movie_data.get(\"genre_ids\", [])\n",
        "        genre_names = [GENRE_ID_TO_NAME.get(gid, str(gid)) for gid in genre_ids]\n",
        "\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": tmdb_id,\n",
        "            \"overview\": movie_data.get(\"overview\", \"\"),\n",
        "            \"poster_path\": IMAGE_BASE + movie_data.get(\"poster_path\", \"\") if movie_data.get(\"poster_path\") else None,\n",
        "            \"backdrop_path\": IMAGE_BASE + movie_data.get(\"backdrop_path\", \"\") if movie_data.get(\"backdrop_path\") else None,\n",
        "            \"vote_average\": movie_data.get(\"vote_average\", None),\n",
        "            \"vote_count\": movie_data.get(\"vote_count\", None),\n",
        "            \"tmdb_genres\": \", \".join(genre_names),\n",
        "            **extra\n",
        "        })\n",
        "    else:\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": None,\n",
        "            \"overview\": None,\n",
        "            \"poster_path\": None,\n",
        "            \"backdrop_path\": None,\n",
        "            \"vote_average\": None,\n",
        "            \"vote_count\": None,\n",
        "            \"tmdb_genres\": None,\n",
        "            \"top_3_cast\": None,\n",
        "            \"directors\": None,\n",
        "            \"keywords\": None,\n",
        "            \"trailer_link\": None\n",
        "        })\n",
        "\n",
        "    time.sleep(0.25)  # Respect TMDB API rate limits\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 5: Save Final Dataset\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched)\n",
        "final_df = pd.concat([movies_df, enriched_df], axis=1)\n",
        "final_df.to_csv(\"movies_enriched_full.csv\", index=False)\n",
        "\n",
        "print(\"DONE: Saved as 'movies_enriched_full.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Personalized Content-Based Movie Recommendation System**\n",
        "\n",
        "This Python script implements a **Content-Based Filtering (CBF)** system enhanced with **personalized recommendations** using user-specific rating profiles. Built using the MovieLens 1M dataset and enriched metadata, the pipeline performs vectorization, similarity computation, and profile-based predictions.\n",
        "\n",
        "**What This Script Does**\n",
        "\n",
        "* **Module 1â€“2**: Load essential libraries and enriched movie data.\n",
        "* **Module 3**: Load user ratings and demographics.\n",
        "* **Module 4**: Engineer features combining genres, cast, crew, keywords, and movie overviews.\n",
        "* **Module 5**: Transform content into TF-IDF, Count, or Binary vectors, and compute pairwise similarities using Cosine or Jaccard metrics.\n",
        "* **Module 6**: Construct a weighted content profile per user based on past ratings.\n",
        "* **Module 7**: Recommend top-N movies similar to the user profile, excluding already seen titles.\n",
        "\n",
        "**Techniques Used**\n",
        "\n",
        "* **Text Vectorization**: TF-IDF, CountVectorizer, Binary Count\n",
        "* **Similarity Metrics**: Cosine Similarity, Jaccard Similarity\n",
        "* **Personalization**: Weighted vector averaging based on each userâ€™s rated items\n",
        "* **Parallelization**: Speeds up Jaccard similarity computation using joblib\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "* Personalized recommendations for new users with a few ratings (cold-start)\n",
        "* Improving diversity and relevance in suggested movies\n",
        "* Generating fallback content suggestions in hybrid recommender systems"
      ],
      "metadata": {
        "id": "c7KNZkOThu7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GeB2GWMyfPL",
        "outputId": "8f828d14-e799-42e2-a43c-c43f0d395ec8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: scikit-surprise==1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.1.4)\n",
            "Requirement already satisfied: scikit-learn==1.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: matplotlib==3.8.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.8.4)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
            "Requirement already satisfied: tqdm==4.66.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.66.4)\n",
            "Requirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise==1.1.4->-r requirements.txt (line 3)) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 2)) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Personalized Content-Based Movie Recommendation System Using Hybrid Textual Metadata and Multiple Similarity Models**\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "The goal of this project is to build a personalized movie recommendation system that leverages content-based filtering techniques using enriched movie metadata. By incorporating user rating data and multiple text-based similarity strategies, the system aims to generate relevant and diverse movie suggestions tailored to individual user preferencesâ€”especially in cold-start or sparsely rated scenarios.\n",
        "\n",
        "**Methodology**\n",
        "\n",
        "1. **Data Loading & Preparation**\n",
        "\n",
        "   * Movie metadata is loaded from an enriched dataset containing genres, keywords, cast, director, overview, and release year.\n",
        "   * User ratings and demographic data are loaded and used to personalize recommendations.\n",
        "\n",
        "2. **Feature Engineering**\n",
        "\n",
        "   * A composite text field (`cbf_features`) is created for each movie by concatenating cleaned metadata fields: genres, keywords, cast, director, overview, and year.\n",
        "\n",
        "3. **Vectorization**\n",
        "\n",
        "   * Three representations of movie content are generated:\n",
        "\n",
        "     * **TF-IDF Vectors**: Capture term importance within documents.\n",
        "     * **Count Vectors**: Raw term frequencies without weighting.\n",
        "     * **Binary Genre-Like Vectors**: For Jaccard similarity (1 if feature exists).\n",
        "\n",
        "4. **Similarity Computation**\n",
        "\n",
        "   * Cosine similarity is computed for TF-IDF and Count vectors.\n",
        "   * Jaccard similarity is computed for binary vectors using pairwise intersection-over-union.\n",
        "\n",
        "5. **User Profiling & Recommendation**\n",
        "\n",
        "   * For **TF-IDF** and **Count** models:\n",
        "\n",
        "     * A personalized **user profile vector** is created using a weighted average of vectors from rated movies.\n",
        "     * Recommendations are generated by finding unseen movies most similar to the userâ€™s profile.\n",
        "   * For the **Binary + Jaccard** model:\n",
        "\n",
        "     * The average Jaccard similarity is computed between each unseen movie and the userâ€™s seen movies.\n",
        "\n",
        "6. **Result Generation & Tagging**\n",
        "\n",
        "   * Top 50 movie recommendations are produced per user for each model.\n",
        "   * Each output is tagged with the model name: `\"TF-IDF + Cosine\"`, `\"Count + Cosine\"`, or `\"Binary + Jaccard\"`.\n",
        "\n",
        "7. **Output Consolidation**\n",
        "\n",
        "   * All recommendation outputs are combined into one labeled DataFrame for comparative analysis and visualization."
      ],
      "metadata": {
        "id": "vj82RTA_ldyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Hybrid CBF Pipeline with RMSE, Top-N, and CSV Export\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load Data\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# Feature Engineering\n",
        "def create_feature_string(df):\n",
        "    def clean(col):\n",
        "        return df[col].fillna('').astype(str).str.replace(',', ' ').str.replace(r'\\s+', ' ', regex=True)\n",
        "    df['cbf_features'] = (\n",
        "        clean('tmdb_genres') + ' ' +\n",
        "        clean('keywords') + ' ' +\n",
        "        clean('top_3_cast') + ' ' +\n",
        "        clean('directors') + ' ' +\n",
        "        df['overview'].fillna('').str.lower().str.replace(r'[^\\w\\s]', '', regex=True) + ' ' +\n",
        "        df['year'].astype(str)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "movies = create_feature_string(movies)\n",
        "\n",
        "# Train-Test Split Per User\n",
        "def train_test_split_user(ratings, test_size=0.2):\n",
        "    train_rows, test_rows = [], []\n",
        "    for user_id, group in ratings.groupby('userId'):\n",
        "        if len(group) >= 5:\n",
        "            train, test = train_test_split(group, test_size=test_size, random_state=42)\n",
        "            train_rows.append(train)\n",
        "            test_rows.append(test)\n",
        "        else:\n",
        "            train_rows.append(group)\n",
        "    return pd.concat(train_rows), pd.concat(test_rows)\n",
        "\n",
        "train_ratings, test_ratings = train_test_split_user(ratings)\n",
        "\n",
        "# Bias Terms\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "user_bias = train_ratings.groupby('userId')['rating'].mean() - global_mean\n",
        "item_bias = train_ratings.groupby('movieId')['rating'].mean() - global_mean\n",
        "\n",
        "# Vectorizers\n",
        "tfidf_matrix = TfidfVectorizer(stop_words='english').fit_transform(movies['cbf_features'])\n",
        "count_matrix = CountVectorizer(stop_words='english').fit_transform(movies['cbf_features'])\n",
        "binary_matrix = CountVectorizer(binary=True).fit_transform(movies['cbf_features'])\n",
        "\n",
        "# Helper Functions\n",
        "def build_user_profile(user_id, train_ratings, matrix, movies):\n",
        "    user_train = train_ratings[train_ratings['userId'] == user_id]\n",
        "    indices = movies[movies['movieId'].isin(user_train['movieId'])].index\n",
        "    if len(indices) == 0:\n",
        "        return None\n",
        "    weights = user_train.set_index('movieId').loc[movies.iloc[indices]['movieId']]['rating'].values\n",
        "    return np.average(matrix[indices].toarray(), axis=0, weights=weights).reshape(1, -1)\n",
        "\n",
        "def evaluate_rmse_for_user(user_id, train_ratings, test_ratings, matrix, movies, sim_fn):\n",
        "    profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "    if profile is None:\n",
        "        return None\n",
        "    user_test = test_ratings[test_ratings['userId'] == user_id]\n",
        "    test_movies = movies[movies['movieId'].isin(user_test['movieId'])]\n",
        "    test_indices = test_movies.index\n",
        "    if len(test_indices) == 0:\n",
        "        return None\n",
        "    sims = sim_fn(profile, matrix[test_indices]).flatten()\n",
        "    b_u = user_bias.get(user_id, 0)\n",
        "    b_i = item_bias.reindex(test_movies['movieId']).fillna(0).values\n",
        "    preds = np.clip(global_mean + b_u + b_i + sims * 1.5, 0.5, 5.0)\n",
        "    actual = user_test.set_index('movieId').loc[test_movies['movieId']]['rating'].values\n",
        "    return np.sqrt(mean_squared_error(actual, preds))\n",
        "\n",
        "def evaluate_rmse_all_users(train_ratings, test_ratings, matrix, movies, sim_fn):\n",
        "    user_ids = test_ratings['userId'].unique()\n",
        "    rmses = []\n",
        "    for user_id in tqdm(user_ids, desc=\"Evaluating users\"):\n",
        "        rmse = evaluate_rmse_for_user(user_id, train_ratings, test_ratings, matrix, movies, sim_fn)\n",
        "        if rmse is not None:\n",
        "            rmses.append(rmse)\n",
        "    return np.mean(rmses)\n",
        "\n",
        "def recommend_top_n(user_id, train_ratings, matrix, movies, sim_fn, top_n=10):\n",
        "    profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "    if profile is None:\n",
        "        return pd.DataFrame()\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId']\n",
        "    unseen = movies[~movies['movieId'].isin(seen)]\n",
        "    sims = sim_fn(profile, matrix[unseen.index]).flatten()\n",
        "    content_scores = sims * 1.5\n",
        "    b_u = user_bias.get(user_id, 0)\n",
        "    b_i = item_bias.reindex(unseen['movieId']).fillna(0).values\n",
        "    preds = np.clip(global_mean + b_u + b_i + content_scores, 0.5, 5.0)\n",
        "    top_idx = np.argsort(preds)[-top_n:][::-1]\n",
        "    return unseen.iloc[top_idx][['movieId', 'title']].assign(predicted_rating=preds[top_idx])\n",
        "\n",
        "# Save Predictions for Meta-Learner\n",
        "def save_predictions(user_ids, matrix, sim_fn, label):\n",
        "    dfs = []\n",
        "    for user_id in tqdm(user_ids, desc=f\"Scoring {label}\"):\n",
        "        profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "        if profile is None:\n",
        "            continue\n",
        "        user_test = test_ratings[test_ratings['userId'] == user_id]\n",
        "        test_movies = movies[movies['movieId'].isin(user_test['movieId'])]\n",
        "        test_indices = test_movies.index\n",
        "        if len(test_indices) == 0:\n",
        "            continue\n",
        "        sims = sim_fn(profile, matrix[test_indices]).flatten()\n",
        "        b_u = user_bias.get(user_id, 0)\n",
        "        b_i = item_bias.reindex(test_movies['movieId']).fillna(0).values\n",
        "        preds = np.clip(global_mean + b_u + b_i + sims * 1.5, 0.5, 5.0)\n",
        "        actual = user_test.set_index('movieId').loc[test_movies['movieId']]['rating'].values\n",
        "        df = pd.DataFrame({\n",
        "            'userId': user_id,\n",
        "            'movieId': test_movies['movieId'].values,\n",
        "            'true_rating': actual,\n",
        "            f'{label}_score': preds\n",
        "        })\n",
        "        dfs.append(df)\n",
        "    result = pd.concat(dfs)\n",
        "    result.to_csv(f'cbf_predictions_{label}.csv', index=False)\n",
        "\n",
        "# Run Evaluations and Save Predictions\n",
        "rmse_tfidf = evaluate_rmse_all_users(train_ratings, test_ratings, tfidf_matrix, movies, cosine_similarity)\n",
        "rmse_count = evaluate_rmse_all_users(train_ratings, test_ratings, count_matrix, movies, cosine_similarity)\n",
        "rmse_binary = evaluate_rmse_all_users(train_ratings, test_ratings, binary_matrix.toarray(), movies,\n",
        "                                      lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard'))\n",
        "\n",
        "print(f\"\\nTF-IDF + Cosine RMSE: {rmse_tfidf:.4f}\")\n",
        "print(f\"Count + Cosine RMSE: {rmse_count:.4f}\")\n",
        "print(f\"Binary + Jaccard RMSE: {rmse_binary:.4f}\")\n",
        "\n",
        "# Top-N Recommendations for User 5549\n",
        "print(\"\\nTop-N Recommendations for User 5549 â€” TF-IDF\")\n",
        "print(recommend_top_n(5549, train_ratings, tfidf_matrix, movies, cosine_similarity))\n",
        "\n",
        "print(\"\\nTop-N Recommendations for User 5549 â€” Count\")\n",
        "print(recommend_top_n(5549, train_ratings, count_matrix, movies, cosine_similarity))\n",
        "\n",
        "print(\"\\nTop-N Recommendations for User 5549 â€” Jaccard\")\n",
        "print(recommend_top_n(5549, train_ratings, binary_matrix, movies,\n",
        "                      lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard')))\n",
        "\n",
        "# Save Predictions\n",
        "save_predictions(test_ratings['userId'].unique(), tfidf_matrix, cosine_similarity, 'tfidf')\n",
        "save_predictions(test_ratings['userId'].unique(), count_matrix, cosine_similarity, 'count')\n",
        "save_predictions(test_ratings['userId'].unique(), binary_matrix.toarray(),\n",
        "                 lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard'), 'binary')\n"
      ],
      "metadata": {
        "id": "L2U48IdMw0rJ",
        "outputId": "32f187ba-6fba-4a23-eb29-81ada5234f0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating users:   1%|          | 68/6040 [00:02<03:40, 27.10it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Libraries and Imports\n",
        "# ============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.metrics import mean_squared_error  # Replacing root_mean_squared_error for compatibility\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppress DataConversionWarning (especially from Jaccard)\n",
        "import warnings\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
        "\n",
        "# ============================\n",
        "# Step 1: Load Data\n",
        "# ============================\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# ============================\n",
        "# Step 2: Create CBF Features\n",
        "# ============================\n",
        "def create_feature_string(df):\n",
        "    def clean(col):\n",
        "        return df[col].fillna('').astype(str).str.replace(',', ' ').str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "    genres = df['tmdb_genres'].fillna('').str.replace(',', ' ')\n",
        "    keywords = clean('keywords')\n",
        "    cast = clean('top_3_cast')\n",
        "    director = clean('directors')\n",
        "    overview = df['overview'].fillna('').str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "    year = df['year'].astype(str)\n",
        "\n",
        "    df['cbf_features'] = genres + ' ' + keywords + ' ' + cast + ' ' + director + ' ' + overview + ' ' + year\n",
        "    return df\n",
        "\n",
        "movies = create_feature_string(movies)\n",
        "\n",
        "# ============================\n",
        "# Step 3: Per-User Train/Test Split\n",
        "# ============================\n",
        "def train_test_split_user(ratings, test_size=0.2):\n",
        "    train_rows = []\n",
        "    test_rows = []\n",
        "    for user_id, group in ratings.groupby('userId'):\n",
        "        if len(group) >= 5:\n",
        "            train, test = train_test_split(group, test_size=test_size, random_state=42)\n",
        "            train_rows.append(train)\n",
        "            test_rows.append(test)\n",
        "        else:\n",
        "            train_rows.append(group)\n",
        "    return pd.concat(train_rows), pd.concat(test_rows)\n",
        "\n",
        "train_ratings, test_ratings = train_test_split_user(ratings)\n",
        "\n",
        "# ============================\n",
        "# Step 4: Compute Bias Terms\n",
        "# ============================\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "user_bias = train_ratings.groupby('userId')['rating'].mean() - global_mean\n",
        "item_bias = train_ratings.groupby('movieId')['rating'].mean() - global_mean\n",
        "\n",
        "# ============================\n",
        "# Step 5: Build User Profile\n",
        "# ============================\n",
        "def build_user_profile(user_id, train_ratings, matrix, movies):\n",
        "    user_train = train_ratings[train_ratings['userId'] == user_id]\n",
        "    indices = movies[movies['movieId'].isin(user_train['movieId'])].index\n",
        "\n",
        "    if len(indices) == 0:\n",
        "        return None\n",
        "\n",
        "    weights = user_train.set_index('movieId').loc[movies.iloc[indices]['movieId']]['rating'].values\n",
        "\n",
        "    # Check if matrix is sparse\n",
        "    row_vectors = matrix[indices].toarray() if hasattr(matrix, \"toarray\") else matrix[indices]\n",
        "\n",
        "    return np.average(row_vectors, axis=0, weights=weights).reshape(1, -1)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Step 6: RMSE Evaluation\n",
        "# ============================\n",
        "def evaluate_rmse_for_user(user_id, train_ratings, test_ratings, matrix, movies, sim_fn):\n",
        "    profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "    if profile is None:\n",
        "        return None\n",
        "    user_test = test_ratings[test_ratings['userId'] == user_id]\n",
        "    test_movies = movies[movies['movieId'].isin(user_test['movieId'])]\n",
        "    test_indices = test_movies.index\n",
        "    if len(test_indices) == 0:\n",
        "        return None\n",
        "\n",
        "    sims = sim_fn(profile, matrix[test_indices]).flatten()\n",
        "    content_scores = sims * 1.5\n",
        "    b_u = user_bias.get(user_id, 0)\n",
        "    b_i = item_bias.reindex(test_movies['movieId']).fillna(0).values\n",
        "    preds = np.clip(global_mean + b_u + b_i + content_scores, 0.5, 5.0)\n",
        "    actual = user_test.set_index('movieId').loc[test_movies['movieId']]['rating'].values\n",
        "    # return mean_squared_error(actual, preds, squared=False)\n",
        "    return np.sqrt(mean_squared_error(actual, preds))\n",
        "\n",
        "def evaluate_rmse_all_users(train_ratings, test_ratings, matrix, movies, sim_fn):\n",
        "    user_ids = test_ratings['userId'].unique()\n",
        "    rmse_list = []\n",
        "    for user_id in tqdm(user_ids, desc=\"Evaluating users\"):\n",
        "        rmse = evaluate_rmse_for_user(user_id, train_ratings, test_ratings, matrix, movies, sim_fn)\n",
        "        if rmse is not None:\n",
        "            rmse_list.append(rmse)\n",
        "    return np.mean(rmse_list) if rmse_list else None\n",
        "\n",
        "# ============================\n",
        "# Step 7: Top-N Recommendations\n",
        "# ============================\n",
        "def recommend_top_n(user_id, train_ratings, matrix, movies, sim_fn, top_n=10):\n",
        "    profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "    if profile is None:\n",
        "        return pd.DataFrame()\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId']\n",
        "    unseen = movies[~movies['movieId'].isin(seen)]\n",
        "    sims = sim_fn(profile, matrix[unseen.index]).flatten()\n",
        "    content_scores = sims * 1.5\n",
        "    b_u = user_bias.get(user_id, 0)\n",
        "    b_i = item_bias.reindex(unseen['movieId']).fillna(0).values\n",
        "    preds = np.clip(global_mean + b_u + b_i + content_scores, 0.5, 5.0)\n",
        "    top_idx = np.argsort(preds)[-top_n:][::-1]\n",
        "    return unseen.iloc[top_idx][['movieId', 'title']].assign(predicted_rating=preds[top_idx])\n",
        "\n",
        "# ============================\n",
        "# Step 8: Vectorization\n",
        "# ============================\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "binary_vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(movies['cbf_features'])\n",
        "count_matrix = count_vectorizer.fit_transform(movies['cbf_features'])\n",
        "binary_matrix = binary_vectorizer.fit_transform(movies['cbf_features'])\n",
        "\n",
        "# ============================\n",
        "# Step 9: RMSE Evaluation\n",
        "# ============================\n",
        "rmse_tfidf = evaluate_rmse_all_users(train_ratings, test_ratings, tfidf_matrix, movies, cosine_similarity)\n",
        "rmse_count = evaluate_rmse_all_users(train_ratings, test_ratings, count_matrix, movies, cosine_similarity)\n",
        "rmse_binary = evaluate_rmse_all_users(train_ratings, test_ratings, binary_matrix.toarray(),  # Convert to dense\n",
        "    movies,\n",
        "    lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard')\n",
        ")\n",
        "\n",
        "print(f\"\\nTF-IDF + Cosine RMSE: {rmse_tfidf:.4f}\")\n",
        "print(f\"Count + Cosine RMSE: {rmse_count:.4f}\")\n",
        "print(f\"Binary + Jaccard RMSE: {rmse_binary:.4f}\")\n",
        "\n",
        "# ============================\n",
        "# Step 10: Show Top-N for User 5549\n",
        "# ============================\n",
        "print(\"\\nTop-N Recommendations for User 5549 â€” TF-IDF\")\n",
        "print(recommend_top_n(5549, train_ratings, tfidf_matrix, movies, cosine_similarity))\n",
        "\n",
        "print(\"\\nTop-N Recommendations for User 5549 â€” Count\")\n",
        "print(recommend_top_n(5549, train_ratings, count_matrix, movies, cosine_similarity))\n",
        "\n",
        "print(\"\\nTop-N Recommendations for User 5549 â€” Jaccard\")\n",
        "print(recommend_top_n(5549, train_ratings, binary_matrix, movies,\n",
        "                      lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard')))\n"
      ],
      "metadata": {
        "id": "oP6fbVu-po3s",
        "outputId": "517f39e4-e9b5-4ce2-b64d-09c978e5d822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6040/6040 [03:47<00:00, 26.49it/s]\n",
            "Evaluating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6040/6040 [03:45<00:00, 26.76it/s]\n",
            "Evaluating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6040/6040 [04:10<00:00, 24.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF + Cosine RMSE: 0.9424\n",
            "Count + Cosine RMSE: 0.9693\n",
            "Binary + Jaccard RMSE: 0.9297\n",
            "\n",
            "Top-N Recommendations for User 5549 â€” TF-IDF\n",
            "      movieId                                              title  \\\n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1396     1420  Message to Love: The Isle of Wight Festival (1...   \n",
            "3313     3382                             Song of Freedom (1936)   \n",
            "3587     3656                                       Lured (1947)   \n",
            "777       787                 Gate of Heavenly Peace, The (1995)   \n",
            "568       572                             Foreign Student (1994)   \n",
            "977       989          Schlafes Bruder (Brother of Sleep) (1995)   \n",
            "3811     3881                           Bittersweet Motel (2000)   \n",
            "1339     1360  Identification of a Woman (Identificazione di ...   \n",
            "1762     1830                            Follow the Bitch (1998)   \n",
            "\n",
            "      predicted_rating  \n",
            "3164               5.0  \n",
            "1396               5.0  \n",
            "3313               5.0  \n",
            "3587               5.0  \n",
            "777                5.0  \n",
            "568                5.0  \n",
            "977                5.0  \n",
            "3811               5.0  \n",
            "1339               5.0  \n",
            "1762               5.0  \n",
            "\n",
            "Top-N Recommendations for User 5549 â€” Count\n",
            "      movieId                                              title  \\\n",
            "1762     1830                            Follow the Bitch (1998)   \n",
            "3811     3881                           Bittersweet Motel (2000)   \n",
            "977       989          Schlafes Bruder (Brother of Sleep) (1995)   \n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1339     1360  Identification of a Woman (Identificazione di ...   \n",
            "2862     2931        Time of the Gypsies (Dom za vesanje) (1989)   \n",
            "1396     1420  Message to Love: The Isle of Wight Festival (1...   \n",
            "3211     3280                                   Baby, The (1973)   \n",
            "568       572                             Foreign Student (1994)   \n",
            "3587     3656                                       Lured (1947)   \n",
            "\n",
            "      predicted_rating  \n",
            "1762               5.0  \n",
            "3811               5.0  \n",
            "977                5.0  \n",
            "3164               5.0  \n",
            "1339               5.0  \n",
            "2862               5.0  \n",
            "1396               5.0  \n",
            "3211               5.0  \n",
            "568                5.0  \n",
            "3587               5.0  \n",
            "\n",
            "Top-N Recommendations for User 5549 â€” Jaccard\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "scipy distance metrics do not support sparse matrices.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-655101378.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTop-N Recommendations for User 5549 â€” Jaccard\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m print(recommend_top_n(5549, train_ratings, binary_matrix, movies,\n\u001b[0m\u001b[1;32m    170\u001b[0m                       lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard')))\n",
            "\u001b[0;32m/tmp/ipython-input-3-655101378.py\u001b[0m in \u001b[0;36mrecommend_top_n\u001b[0;34m(user_id, train_ratings, matrix, movies, sim_fn, top_n)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'movieId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0munseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmovies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'movieId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0mcontent_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msims\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mb_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-655101378.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTop-N Recommendations for User 5549 â€” Jaccard\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m print(recommend_top_n(5549, train_ratings, binary_matrix, movies,\n\u001b[0;32m--> 170\u001b[0;31m                       lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard')))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2309\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scipy distance metrics do not support sparse matrices.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPAIRWISE_BOOLEAN_FUNCTIONS\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: scipy distance metrics do not support sparse matrices."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Memory-based collaborative filtering module (UBCF, IBCF)**\n",
        "\n",
        "***Purpose:***\n",
        "\n",
        "This module implements **memory-based collaborative filtering** using **user-user** or **item-item** similarity. It addresses **user bias** by normalizing ratings through mean-centering and optionally **rescaling predictions** to the original rating scale for interpretability.\n",
        "\n",
        "***Methodology:***\n",
        "\n",
        "1. **Rating Matrix Construction**:\n",
        "\n",
        "   * A user-item matrix is built from raw MovieLens-style ratings data.\n",
        "   * For `kind='user'`, ratings are mean-centered per user to reduce bias from lenient or strict raters.\n",
        "   * For `kind='item'`, raw ratings are used directly (no normalization), as the algorithm focuses on item similarities based on a single user's input.\n",
        "\n",
        "2. **Similarity Computation**:\n",
        "\n",
        "   * Cosine similarity is computed either:\n",
        "\n",
        "     * **Across users** for user-based CF (`kind='user'`)\n",
        "     * **Across items** for item-based CF (`kind='item'`)\n",
        "   * `sklearn.metrics.pairwise_distances` is used to derive similarity as `1 - cosine_distance`.\n",
        "\n",
        "3. **Prediction Generation**:\n",
        "\n",
        "   * For **user-based CF**:\n",
        "\n",
        "     * Ratings from similar users are weighted by similarity and averaged.\n",
        "     * The userâ€™s mean rating is **added back** to restore predictions to the original scale (e.g., 1â€“5).\n",
        "   * For **item-based CF**:\n",
        "\n",
        "     * A userâ€™s own ratings are used to compute scores for similar items.\n",
        "     * No mean is added back, since predictions are already on the correct scale.\n",
        "\n",
        "4. **Top-N Recommendations**:\n",
        "\n",
        "   * The system filters out movies the user has already rated.\n",
        "   * It ranks unseen movies by predicted score and returns the top-N recommendations.\n",
        "   * Each recommendation is labeled with the model type (`User-Based CF` or `Item-Based CF`) for downstream tracking."
      ],
      "metadata": {
        "id": "omg4Y6k5XmSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === STEP 1: Load Ratings ===\n",
        "# Purpose: Import user-movie ratings dataset and split it into training and test sets.\n",
        "# This allows us to train the model on one portion and evaluate on unseen data.\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# === STEP 2: Create Bias-Adjusted User-Item Matrix ===\n",
        "# Purpose: Create a user-item ratings matrix and adjust for biases by removing the global mean,\n",
        "# user bias (tendency to rate high/low), and item bias (popularity effects).\n",
        "def create_bias_adjusted_matrix(ratings_df):\n",
        "    matrix = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
        "    global_mean = ratings_df['rating'].mean()\n",
        "    user_bias = matrix.sub(global_mean, axis=0).mean(axis=1)\n",
        "    item_bias = matrix.sub(global_mean, axis=0).sub(user_bias, axis=0).mean(axis=0)\n",
        "    adjusted = matrix.sub(global_mean).sub(user_bias, axis=0).sub(item_bias, axis=1)\n",
        "    return adjusted.fillna(0), global_mean, user_bias, item_bias\n",
        "\n",
        "user_item_matrix, global_mean, user_bias, item_bias = create_bias_adjusted_matrix(train_ratings)\n",
        "\n",
        "# === STEP 3: Compute Similarity Matrices ===\n",
        "# Purpose: Measure similarity between users or items using cosine similarity on the bias-adjusted matrix.\n",
        "# These similarity scores will later help generate personalized recommendations.\n",
        "user_sim_matrix = cosine_similarity(user_item_matrix)\n",
        "item_sim_matrix = cosine_similarity(user_item_matrix.T)\n",
        "\n",
        "# === STEP 4: Recommender Function (Top-N or Full Scores) ===\n",
        "# Purpose: Generate movie recommendations using memory-based collaborative filtering.\n",
        "# Predict scores for unseen items using either user-based or item-based similarity and adjust with biases.\n",
        "def recommend_memory_based(user_id, matrix, global_mean, user_bias, item_bias, sim_matrix, kind='user', top_n=50, return_full=False):\n",
        "    if kind == 'user':\n",
        "        sim_scores = sim_matrix[matrix.index.get_loc(user_id)]\n",
        "        weighted = sim_scores @ matrix.values\n",
        "        norm = np.abs(sim_scores).sum()\n",
        "        preds = weighted / norm if norm != 0 else np.zeros_like(weighted)\n",
        "        preds += global_mean + user_bias.loc[user_id]\n",
        "    else:\n",
        "        user_vector = matrix.loc[user_id]\n",
        "        weighted = user_vector @ sim_matrix\n",
        "        norm = (user_vector != 0) @ np.abs(sim_matrix)\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            preds = np.true_divide(weighted, norm)\n",
        "            preds[norm == 0] = 0\n",
        "        preds += global_mean + user_bias.loc[user_id] + item_bias.values\n",
        "\n",
        "    preds = np.clip(preds, 1.0, 5.0)\n",
        "    pred_series = pd.Series(preds, index=matrix.columns)\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    pred_series = pred_series.drop(labels=seen, errors='ignore')\n",
        "\n",
        "    if return_full:\n",
        "        return pred_series\n",
        "    else:\n",
        "        top_preds = pred_series.sort_values(ascending=False).head(top_n)\n",
        "        return pd.DataFrame({\n",
        "            'userId': user_id,\n",
        "            'movieId': top_preds.index,\n",
        "            'score': top_preds.values\n",
        "        })\n",
        "\n",
        "# === STEP 5: Evaluation Function ===\n",
        "# Purpose: Evaluate model performance using RMSE by comparing predicted scores to actual ratings\n",
        "# in the test set for multiple users. Measures how accurate the recommender is overall.\n",
        "def evaluate_model(test_df, matrix, global_mean, user_bias, item_bias, sim_matrix, kind='user'):\n",
        "    all_preds = []\n",
        "    for uid in test_df['userId'].unique():\n",
        "        if uid not in matrix.index:\n",
        "            continue\n",
        "        recs = recommend_memory_based(uid, matrix, global_mean, user_bias, item_bias, sim_matrix, kind, top_n=1000, return_full=True)\n",
        "        actual = test_df[test_df['userId'] == uid]\n",
        "        merged = pd.merge(actual, recs.rename(\"score\"), on=\"movieId\")\n",
        "        all_preds.append(merged)\n",
        "\n",
        "    all_preds_df = pd.concat(all_preds, ignore_index=True)\n",
        "    rmse = np.sqrt(mean_squared_error(all_preds_df['rating'], all_preds_df['score'])) if not all_preds_df.empty else np.nan\n",
        "    return rmse\n",
        "\n",
        "# === STEP 6: Run Evaluation ===\n",
        "# Purpose: Calculate RMSE for user-based CF, item-based CF, and a dummy predictor\n",
        "# that always predicts the global mean rating.\n",
        "user_rmse = evaluate_model(test_ratings, user_item_matrix, global_mean, user_bias, item_bias, user_sim_matrix, 'user')\n",
        "item_rmse = evaluate_model(test_ratings, user_item_matrix, global_mean, user_bias, item_bias, item_sim_matrix, 'item')\n",
        "dummy_rmse = np.sqrt(mean_squared_error(test_ratings['rating'], [global_mean] * len(test_ratings)))\n",
        "\n",
        "print(f\"User-Based CF RMSE: {user_rmse:.4f}\")\n",
        "print(f\"Item-Based CF RMSE: {item_rmse:.4f}\")\n",
        "print(f\"Dummy Predictor RMSE: {dummy_rmse:.4f}\")\n",
        "\n",
        "# === STEP 7: Get Recommendations for a Specific User (Optional) ===\n",
        "# Purpose: Generate and display the top-N recommended movies for a target user using both models.\n",
        "# This is useful for presenting personalized suggestions.\n",
        "user_id = 5549\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "user_recs = recommend_memory_based(user_id, user_item_matrix, global_mean, user_bias, item_bias, user_sim_matrix, 'user', top_n=50)\n",
        "item_recs = recommend_memory_based(user_id, user_item_matrix, global_mean, user_bias, item_bias, item_sim_matrix, 'item', top_n=50)\n",
        "\n",
        "user_recs = user_recs.merge(movies, on='movieId', how='left')\n",
        "item_recs = item_recs.merge(movies, on='movieId', how='left')\n",
        "\n",
        "print(\"\\nTop 10 User-Based CF Recommendations:\")\n",
        "print(user_recs[['movieId', 'title', 'score']])\n",
        "\n",
        "print(\"\\nTop 10 Item-Based CF Recommendations:\")\n",
        "print(item_recs[['movieId', 'title', 'score']])\n",
        "\n",
        "# === STEP 8: Hybrid Score Fusion (Weighted Average) ===\n",
        "# Purpose: Combine UBCF and IBCF scores using a weighted average for improved recommendations.\n",
        "\n",
        "# Merge UBCF and IBCF scores\n",
        "merged = user_recs[['movieId', 'score']].rename(columns={'score': 'ubcf_score'}).merge(\n",
        "    item_recs[['movieId', 'score']].rename(columns={'score': 'ibcf_score'}),\n",
        "    on='movieId'\n",
        ")\n",
        "\n",
        "# Assign weights (adjust as needed)\n",
        "ubcf_weight = 0.5\n",
        "ibcf_weight = 0.5\n",
        "\n",
        "# Compute hybrid score\n",
        "merged['hybrid_score'] = ubcf_weight * merged['ubcf_score'] + ibcf_weight * merged['ibcf_score']\n",
        "hybrid_recs = merged.merge(movies, on='movieId', how='left').sort_values('hybrid_score', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Hybrid Recommendations:\")\n",
        "print(hybrid_recs[['movieId', 'title', 'hybrid_score']].head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "BmjSwcUPB0wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid Search Optimization of Hybrid Recommender Weights (UBCF + IBCF + CBF)**\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "To determine the optimal combination of weights for the hybrid recommender system that blends User-Based Collaborative Filtering (UBCF), Item-Based Collaborative Filtering (IBCF), and Content-Based Filtering (CBF), in order to minimize prediction error (RMSE) and improve recommendation accuracy.\n",
        "\n",
        "**Methodology:**\n",
        "\n",
        "1. **Define Evaluation Metric:**\n",
        "   Use Root Mean Squared Error (RMSE) to evaluate how well predicted ratings from the hybrid model match actual user ratings in the test set.\n",
        "\n",
        "2. **Generate Weight Combinations:**\n",
        "   Create a grid of possible weight combinations for UBCF, IBCF, and CBF using increments of 0.2, ensuring that all weights sum to 1.0.\n",
        "\n",
        "3. **Calculate Hybrid Predictions:**\n",
        "   For each user in the test set (or a subset for faster testing), generate hybrid recommendations using the current weight combination.\n",
        "\n",
        "4. **Compute RMSE:**\n",
        "   Compare predicted scores to actual ratings for that user and compute RMSE. Repeat for each user and take the mean RMSE across all users.\n",
        "\n",
        "5. **Store and Rank Results:**\n",
        "   Store all weight combinations along with their RMSEs, then sort the results to identify the combination that yields the lowest error.\n",
        "\n",
        "6. **Visualize Results:**\n",
        "   Generate a 3D scatter plot showing the relationship between UBCF, IBCF, and RMSE for visual insight into optimal regions of the weight space.\n"
      ],
      "metadata": {
        "id": "Q2gIxSDItWLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def grid_search_hybrid_weights(user_id, user_item_matrix, user_means,\n",
        "                                user_sim_matrix, item_sim_matrix,\n",
        "                                tfidf_matrix, ratings, movie_df,\n",
        "                                weight_step=0.2, k=10):\n",
        "    \"\"\"\n",
        "    Grid search over hybrid weight combinations (UBCF, IBCF, CBF) where weights sum to 1.\n",
        "    Returns the top-k hybrid combinations based on average hybrid score.\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Generate all valid combinations of weights summing to 1\n",
        "    steps = np.arange(0, 1 + weight_step, weight_step)\n",
        "    for w1, w2, w3 in product(steps, repeat=3):\n",
        "        if abs((w1 + w2 + w3) - 1.0) > 1e-5:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            recs = hybrid_ubcf_ibcf_cbf(\n",
        "                user_id=user_id,\n",
        "                user_item_matrix=user_item_matrix,\n",
        "                user_means=user_means,\n",
        "                user_sim_matrix=user_sim_matrix,\n",
        "                item_sim_matrix=item_sim_matrix,\n",
        "                tfidf_matrix=tfidf_matrix,\n",
        "                ratings=ratings,\n",
        "                movie_df=movie_df,\n",
        "                w_ubcf=w1,\n",
        "                w_ibcf=w2,\n",
        "                w_cbf=w3,\n",
        "                top_n=50\n",
        "            )\n",
        "\n",
        "            avg_score = recs['hybrid_score'].mean()\n",
        "            results.append({\n",
        "                'w_ubcf': w1,\n",
        "                'w_ibcf': w2,\n",
        "                'w_cbf': w3,\n",
        "                'avg_hybrid_score': avg_score,\n",
        "                'top_recs': recs\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Skipped combo ({w1}, {w2}, {w3}): {e}\")\n",
        "            continue\n",
        "\n",
        "    results = sorted(results, key=lambda x: x['avg_hybrid_score'], reverse=True)\n",
        "    return results[:k]\n",
        "\n",
        "grid_results = grid_search_hybrid_weights(\n",
        "    user_id=5549,\n",
        "    user_item_matrix=user_item_matrix,\n",
        "    user_means=user_means,\n",
        "    user_sim_matrix=user_sim_matrix,\n",
        "    item_sim_matrix=item_sim_matrix,\n",
        "    tfidf_matrix=tfidf_matrix,\n",
        "    ratings=ratings,\n",
        "    movie_df=movies,\n",
        "    weight_step=0.2,\n",
        "    k=10\n",
        ")\n",
        "\n",
        "for res in grid_results:\n",
        "    print(f\"Weights -> UBCF: {res['w_ubcf']}, IBCF: {res['w_ibcf']}, CBF: {res['w_cbf']}\")\n",
        "    print(res['top_recs'][['title', 'hybrid_score']].head(5))\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "WVhohLUWtSdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model-Based Filtering:**\n",
        "\n",
        "  * *SVD (Surprise)*: Learns latent features from the rating matrix.\n",
        "  * *ALS (PySpark)*: Scalable factorization method for large datasets.\n"
      ],
      "metadata": {
        "id": "R6d3SP0OolH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 9: Model-Based Collaborative Filtering (SVD using Surprise)**\n",
        "\n",
        "**Purpose:**\n",
        "Use matrix factorization (SVD) to learn latent user/item features from the rating matrix.\n",
        "\n",
        "**Application:**\n",
        "- Accurate, scalable recommendations for sparse datasets using user/item embeddings.\n",
        "- Suitable for small to medium datasets.\n",
        "- Optimized via `GridSearchCV` for hyperparameter tuning.\n",
        "- Good interpretability of latent factors per user and item.\n",
        "\n"
      ],
      "metadata": {
        "id": "4dCPxrNMnQkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 9: Model-Based Collaborative Filtering (SVD using Surprise)\n",
        "# ==============================\n",
        "\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split, GridSearchCV\n",
        "from surprise.accuracy import rmse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================\n",
        "# Prepare Surprise Dataset\n",
        "# ==============================\n",
        "\n",
        "def prepare_surprise_data(ratings):\n",
        "    reader = Reader(rating_scale=(0.5, 5.0))\n",
        "    return Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# ==============================\n",
        "# Tune SVD Model with Grid Search\n",
        "# ==============================\n",
        "\n",
        "def tune_svd_model(data):\n",
        "    param_grid = {\n",
        "        'n_factors': [50, 100],\n",
        "        'lr_all': [0.005, 0.01],\n",
        "        'reg_all': [0.02, 0.1]\n",
        "    }\n",
        "    print(\"Tuning SVD model with GridSearchCV...\")\n",
        "    gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, joblib_verbose=0)\n",
        "\n",
        "    with tqdm(total=1, desc=\"GridSearchCV\") as pbar:\n",
        "        gs.fit(data)\n",
        "        pbar.update(1)\n",
        "\n",
        "    print(f\"Best RMSE: {gs.best_score['rmse']} with params: {gs.best_params['rmse']}\")\n",
        "    return gs.best_estimator['rmse']\n",
        "\n",
        "# ==============================\n",
        "# Train and Evaluate SVD\n",
        "# ==============================\n",
        "\n",
        "def evaluate_svd(model, data, model_label='SVD (Surprise)'):\n",
        "    trainset, testset = train_test_split(data, test_size=0.2)\n",
        "    model.fit(trainset)\n",
        "\n",
        "    print(\"Making predictions...\")\n",
        "    predictions = [model.predict(item[0], item[1], r_ui=item[2]) for item in tqdm(testset, desc=\"Predicting\")]\n",
        "\n",
        "    score = rmse(predictions)\n",
        "\n",
        "    pred_df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
        "    pred_df = pred_df.rename(columns={'uid': 'userId', 'iid': 'movieId', 'rui': 'true_rating', 'est': 'pred_rating'})\n",
        "    pred_df['model'] = model_label\n",
        "    return pred_df[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']], score\n",
        "\n",
        "# ==============================\n",
        "# Main Execution\n",
        "# ==============================\n",
        "\n",
        "# Step 1: Load ratings\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# Step 2: Prepare Surprise data\n",
        "data = prepare_surprise_data(ratings)\n",
        "\n",
        "# Step 3: Tune model\n",
        "best_svd_model = tune_svd_model(data)\n",
        "\n",
        "# Step 4: Evaluate model\n",
        "pred_df, rmse_score = evaluate_svd(best_svd_model, data)\n",
        "\n",
        "# Step 5: Output\n",
        "print(pred_df.head())\n",
        "print(f\"Final RMSE: {rmse_score:.4f}\")\n",
        "\n",
        "# Step 6: Top-50 Predictions for User 5549\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = [mid for mid in all_movie_ids if mid not in rated_movie_ids]\n",
        "\n",
        "print(f\"\\nGenerating predictions for User {target_user}...\")\n",
        "top_preds = [(movie_id, best_svd_model.predict(target_user, movie_id).est)\n",
        "             for movie_id in tqdm(unrated_movie_ids, desc=\"Predicting for user\")]\n",
        "\n",
        "top_50_df = pd.DataFrame(top_preds, columns=['movieId', 'pred_rating'])\n",
        "top_50_df = top_50_df.sort_values(by='pred_rating', ascending=False).head(50)\n",
        "top_50_df['userId'] = target_user\n",
        "top_50_df['model'] = 'SVD (Surprise)'\n",
        "top_50_df = top_50_df[['userId', 'movieId', 'pred_rating', 'model']]\n",
        "\n",
        "# Step 7: Merge with movie titles only\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_df = top_50_df.merge(movies, on='movieId', how='left')\n",
        "\n",
        "# Step 8: Final Output\n",
        "print(\"\\nTop 50 Recommendations for User 5549:\")\n",
        "print(top_50_df[['movieId', 'title', 'pred_rating']].head(10))\n"
      ],
      "metadata": {
        "id": "P0JR9B0YokAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model-Based Collaborative Filtering (ALS using PySpark)**\n",
        "\n",
        "**Purpose:**\n",
        "Use Alternating Least Squares (ALS) to learn latent user/item features at scale.\n",
        "\n",
        "**Application:**\n",
        "- Distributed recommendation system for large-scale datasets.\n",
        "- Runs on Apache Spark for horizontal scalability.\n",
        "- Handles sparsity well using factorization.\n",
        "- Suited for real-time, production-level systems with massive data.\n"
      ],
      "metadata": {
        "id": "DeCmPC_DnZip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Module 10: Model-Based Collaborative Filtering (ALS using PySpark)\n",
        "# ==============================\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import Row\n",
        "import pandas as pd\n",
        "\n",
        "# --- Start Spark Session ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ALSModel\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Load Ratings ---\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "ratings_df = spark.createDataFrame(ratings[['userId', 'movieId', 'rating']])\n",
        "\n",
        "# --- Train ALS Model ---\n",
        "als = ALS(\n",
        "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "    rank=10, maxIter=10, regParam=0.1,\n",
        "    coldStartStrategy=\"drop\", nonnegative=True\n",
        ")\n",
        "als_model = als.fit(ratings_df)\n",
        "\n",
        "# --- Evaluate ALS Model ---\n",
        "predictions = als_model.transform(ratings_df)\n",
        "pred_pd = predictions.select('userId', 'movieId', 'rating', 'prediction').toPandas()\n",
        "pred_pd = pred_pd.rename(columns={'rating': 'true_rating', 'prediction': 'pred_rating'})\n",
        "pred_pd['model'] = 'ALS (PySpark)'\n",
        "\n",
        "# --- Evaluate ALS RMSE ---\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName='rmse',\n",
        "    labelCol='rating',\n",
        "    predictionCol='prediction'\n",
        ")\n",
        "rmse_score = evaluator.evaluate(predictions)\n",
        "\n",
        "# --- Output Evaluation ---\n",
        "print(pred_pd[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']].head())\n",
        "print(f\"\\nFinal RMSE: {rmse_score:.4f}\")\n",
        "\n",
        "# ==============================\n",
        "# Step 6: Top-50 Predictions for User 5549\n",
        "# ==============================\n",
        "\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = list(set(all_movie_ids) - set(rated_movie_ids))\n",
        "\n",
        "# Create Spark DataFrame of userId + unrated movieId pairs\n",
        "user_unrated_pairs = spark.createDataFrame([Row(userId=target_user, movieId=int(mid)) for mid in unrated_movie_ids])\n",
        "\n",
        "# Predict ratings using ALS model\n",
        "print(f\"\\nGenerating Top-50 recommendations for User {target_user}...\")\n",
        "top_preds_df = als_model.transform(user_unrated_pairs).dropna()\n",
        "\n",
        "# Get top-50 highest predicted ratings\n",
        "top_50_preds = top_preds_df.orderBy(col(\"prediction\").desc()).limit(50)\n",
        "top_50_pd = top_50_preds.select(\"userId\", \"movieId\", \"prediction\").toPandas()\n",
        "top_50_pd['model'] = \"ALS (PySpark)\"\n",
        "top_50_pd = top_50_pd.rename(columns={'prediction': 'pred_rating'})\n",
        "\n",
        "# ==============================\n",
        "# Step 7: Merge with Movie Titles Only\n",
        "# ==============================\n",
        "\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_pd = top_50_pd.merge(movies, on='movieId', how='left')\n",
        "\n",
        "# ==============================\n",
        "# Step 8: Output Top-50\n",
        "# ==============================\n",
        "\n",
        "print(\"\\nTop 50 Recommendations for User 5549:\")\n",
        "print(top_50_pd[['movieId', 'title', 'pred_rating']].head(10))\n"
      ],
      "metadata": {
        "id": "LLtk_Rsuom7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ls6RqljaolUG"
      }
    }
  ]
}