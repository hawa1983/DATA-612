{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-612/blob/main/Final_Project_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OptAJ12iBc8G"
      },
      "source": [
        "### **Purpose**\n",
        "\n",
        "The purpose of this script is to enrich the MovieLens movie dataset (`movies.dat`) with detailed movie metadata from The Movie Database (TMDB) API. This metadata includes movie overviews, genres, poster and backdrop image URLs, cast and director information, keywords, user ratings, and trailer links. The enriched dataset will serve as the foundation for building content-based, collaborative, and hybrid recommender systems.\n",
        "\n",
        "### **Methodology**\n",
        "\n",
        "1. **Load MovieLens Movie Data**\n",
        "   The script loads the `movies.dat` file, which contains basic movie information including `movieId`, `title`, and `genres`.\n",
        "\n",
        "2. **Clean Titles and Extract Years**\n",
        "   It processes the movie titles to remove the year from the title string and separately extracts the release year to improve search accuracy when querying TMDB.\n",
        "\n",
        "3. **Query TMDB API**\n",
        "   For each movie, it sends a search request to TMDB using the cleaned title and release year. If a match is found, it retrieves the movie’s TMDB ID.\n",
        "\n",
        "4. **Retrieve Detailed Metadata**\n",
        "   Using the TMDB ID, the script fetches:\n",
        "\n",
        "   * Overview (plot summary)\n",
        "   * Poster and backdrop image paths\n",
        "   * Genre IDs, which are then mapped to readable genre names\n",
        "   * Top 3 cast members\n",
        "   * Director(s)\n",
        "   * Associated keywords\n",
        "   * YouTube trailer link (if available)\n",
        "\n",
        "5. **Construct and Save Enriched Dataset**\n",
        "   All metadata is compiled into a structured format and merged with the original MovieLens data. The final dataset is saved as `movies_enriched_full.csv` for downstream use in recommendation models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qJ0maYGaxtJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ---------------------------------------\n",
        "# CONFIG\n",
        "# ---------------------------------------\n",
        "BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "IMAGE_BASE = \"https://image.tmdb.org/t/p/w500\"\n",
        "\n",
        "# Use your TMDB Bearer Token (v4)\n",
        "HEADERS = {\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyZGZlNjMwMGMzYjIzMjc2NzExNjQ0N2JhNzhiMjM5MyIsIm5iZiI6MTc1MTkyMjA3Ni4xMzUsInN1YiI6IjY4NmMzNTljMzc4NjllOGEyNDUxZTM0OSIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.S773ddH3FiIHtokPW4sYpJog0mXWS1o4OPov1KZneUw\"\n",
        "}\n",
        "\n",
        "# TMDB genre ID to name mapping\n",
        "GENRE_ID_TO_NAME = {\n",
        "    28: \"Action\", 12: \"Adventure\", 16: \"Animation\", 35: \"Comedy\", 80: \"Crime\",\n",
        "    99: \"Documentary\", 18: \"Drama\", 10751: \"Family\", 14: \"Fantasy\", 36: \"History\",\n",
        "    27: \"Horror\", 10402: \"Music\", 9648: \"Mystery\", 10749: \"Romance\", 878: \"Science Fiction\",\n",
        "    10770: \"TV Movie\", 53: \"Thriller\", 10752: \"War\", 37: \"Western\"\n",
        "}\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 1: Load MovieLens .dat Files\n",
        "# ---------------------------------------\n",
        "\n",
        "# Load movies.dat - format: MovieID::Title::Genres\n",
        "movies_df = pd.read_csv(\"movies.dat\", sep=\"::\", engine='python', header=None, names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 2: Clean Movie Titles and Extract Year\n",
        "# ---------------------------------------\n",
        "\n",
        "def extract_year(title):\n",
        "    if \"(\" in title:\n",
        "        try:\n",
        "            return int(title.strip()[-5:-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_title(title):\n",
        "    if \"(\" in title:\n",
        "        return title[:title.rfind(\"(\")].strip()\n",
        "    return title.strip()\n",
        "\n",
        "movies_df[\"year\"] = movies_df[\"title\"].apply(extract_year)\n",
        "movies_df[\"clean_title\"] = movies_df[\"title\"].apply(clean_title)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 3: TMDB Metadata Functions\n",
        "# ---------------------------------------\n",
        "\n",
        "# Search for movie in TMDB\n",
        "def search_tmdb(title, year):\n",
        "    url = f\"{BASE_URL}/search/movie\"\n",
        "    params = {\"query\": title, \"year\": year}\n",
        "    response = requests.get(url, headers=HEADERS, params=params)\n",
        "    r = response.json()\n",
        "    if r.get(\"results\"):\n",
        "        return r[\"results\"][0]\n",
        "    return None\n",
        "\n",
        "# Get full metadata from TMDB\n",
        "def get_full_tmdb_metadata(tmdb_id):\n",
        "    metadata = {}\n",
        "\n",
        "    # Credits (cast, crew)\n",
        "    credits = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/credits\", headers=HEADERS).json()\n",
        "    cast = [c[\"name\"] for c in credits.get(\"cast\", [])[:3]]\n",
        "    directors = [c[\"name\"] for c in credits.get(\"crew\", []) if c.get(\"job\") == \"Director\"]\n",
        "\n",
        "    # Keywords\n",
        "    keywords = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/keywords\", headers=HEADERS).json()\n",
        "    keyword_list = [k[\"name\"] for k in keywords.get(\"keywords\", [])]\n",
        "\n",
        "    # Videos (trailers)\n",
        "    videos = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/videos\", headers=HEADERS).json()\n",
        "    trailer_links = [\n",
        "        f\"https://www.youtube.com/watch?v={v['key']}\"\n",
        "        for v in videos.get(\"results\", [])\n",
        "        if v[\"site\"] == \"YouTube\" and v[\"type\"] == \"Trailer\"\n",
        "    ]\n",
        "\n",
        "    # Final metadata dictionary\n",
        "    metadata[\"top_3_cast\"] = \", \".join(cast)\n",
        "    metadata[\"directors\"] = \", \".join(directors)\n",
        "    metadata[\"keywords\"] = \", \".join(keyword_list)\n",
        "    metadata[\"trailer_link\"] = trailer_links[0] if trailer_links else None\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 4: Enrich Movie Data\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched = []\n",
        "\n",
        "for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
        "    movie_data = search_tmdb(row[\"clean_title\"], row[\"year\"])\n",
        "\n",
        "    if movie_data:\n",
        "        tmdb_id = movie_data[\"id\"]\n",
        "        extra = get_full_tmdb_metadata(tmdb_id)\n",
        "\n",
        "        genre_ids = movie_data.get(\"genre_ids\", [])\n",
        "        genre_names = [GENRE_ID_TO_NAME.get(gid, str(gid)) for gid in genre_ids]\n",
        "\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": tmdb_id,\n",
        "            \"overview\": movie_data.get(\"overview\", \"\"),\n",
        "            \"poster_path\": IMAGE_BASE + movie_data.get(\"poster_path\", \"\") if movie_data.get(\"poster_path\") else None,\n",
        "            \"backdrop_path\": IMAGE_BASE + movie_data.get(\"backdrop_path\", \"\") if movie_data.get(\"backdrop_path\") else None,\n",
        "            \"vote_average\": movie_data.get(\"vote_average\", None),\n",
        "            \"vote_count\": movie_data.get(\"vote_count\", None),\n",
        "            \"tmdb_genres\": \", \".join(genre_names),\n",
        "            **extra\n",
        "        })\n",
        "    else:\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": None,\n",
        "            \"overview\": None,\n",
        "            \"poster_path\": None,\n",
        "            \"backdrop_path\": None,\n",
        "            \"vote_average\": None,\n",
        "            \"vote_count\": None,\n",
        "            \"tmdb_genres\": None,\n",
        "            \"top_3_cast\": None,\n",
        "            \"directors\": None,\n",
        "            \"keywords\": None,\n",
        "            \"trailer_link\": None\n",
        "        })\n",
        "\n",
        "    time.sleep(0.25)  # Respect TMDB API rate limits\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 5: Save Final Dataset\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched)\n",
        "final_df = pd.concat([movies_df, enriched_df], axis=1)\n",
        "final_df.to_csv(\"movies_enriched_full.csv\", index=False)\n",
        "\n",
        "print(\"DONE: Saved as 'movies_enriched_full.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7KNZkOThu7F"
      },
      "source": [
        "## **Personalized Content-Based Movie Recommendation System**\n",
        "\n",
        "This Python script implements a **Content-Based Filtering (CBF)** system enhanced with **personalized recommendations** using user-specific rating profiles. Built using the MovieLens 1M dataset and enriched metadata, the pipeline performs vectorization, similarity computation, and profile-based predictions.\n",
        "\n",
        "**What This Script Does**\n",
        "\n",
        "* **Module 1–2**: Load essential libraries and enriched movie data.\n",
        "* **Module 3**: Load user ratings and demographics.\n",
        "* **Module 4**: Engineer features combining genres, cast, crew, keywords, and movie overviews.\n",
        "* **Module 5**: Transform content into TF-IDF, Count, or Binary vectors, and compute pairwise similarities using Cosine or Jaccard metrics.\n",
        "* **Module 6**: Construct a weighted content profile per user based on past ratings.\n",
        "* **Module 7**: Recommend top-N movies similar to the user profile, excluding already seen titles.\n",
        "\n",
        "**Techniques Used**\n",
        "\n",
        "* **Text Vectorization**: TF-IDF, CountVectorizer, Binary Count\n",
        "* **Similarity Metrics**: Cosine Similarity, Jaccard Similarity\n",
        "* **Personalization**: Weighted vector averaging based on each user’s rated items\n",
        "* **Parallelization**: Speeds up Jaccard similarity computation using joblib\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "* Personalized recommendations for new users with a few ratings (cold-start)\n",
        "* Improving diversity and relevance in suggested movies\n",
        "* Generating fallback content suggestions in hybrid recommender systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9GeB2GWMyfPL",
        "outputId": "0dd43904-a19e-45df-bec4-e69bf7799e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m466.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Collecting scikit-surprise==1.1.4 (from -r requirements.txt (line 3))\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==1.4.2 (from -r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting matplotlib==3.8.4 (from -r requirements.txt (line 5))\n",
            "  Downloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
            "Collecting tqdm==4.66.4 (from -r requirements.txt (line 7))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise==1.1.4->-r requirements.txt (line 3)) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 2)) (1.17.0)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2469542 sha256=b7824633188ffbc8cb3cd5fed396940484b1d606ec3a0a183e54f0db45649c18\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: tqdm, numpy, scikit-surprise, scikit-learn, matplotlib\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "dataproc-spark-connect 0.8.2 requires tqdm>=4.67, but you have tqdm 4.66.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.8.4 numpy-1.26.4 scikit-learn-1.4.2 scikit-surprise-1.1.4 tqdm-4.66.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "17d063935acc43bf8b541998d25912c8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj82RTA_ldyR"
      },
      "source": [
        "### **Personalized Content-Based Movie Recommendation System Using Hybrid Textual Metadata and Multiple Similarity Models**\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "The goal of this project is to build a personalized movie recommendation system that leverages content-based filtering techniques using enriched movie metadata. By incorporating user rating data and multiple text-based similarity strategies, the system aims to generate relevant and diverse movie suggestions tailored to individual user preferences—especially in cold-start or sparsely rated scenarios.\n",
        "\n",
        "**Methodology**\n",
        "\n",
        "1. **Data Loading & Preparation**\n",
        "\n",
        "   * Movie metadata is loaded from an enriched dataset containing genres, keywords, cast, director, overview, and release year.\n",
        "   * User ratings and demographic data are loaded and used to personalize recommendations.\n",
        "\n",
        "2. **Feature Engineering**\n",
        "\n",
        "   * A composite text field (`cbf_features`) is created for each movie by concatenating cleaned metadata fields: genres, keywords, cast, director, overview, and year.\n",
        "\n",
        "3. **Vectorization**\n",
        "\n",
        "   * Three representations of movie content are generated:\n",
        "\n",
        "     * **TF-IDF Vectors**: Capture term importance within documents.\n",
        "     * **Count Vectors**: Raw term frequencies without weighting.\n",
        "     * **Binary Genre-Like Vectors**: For Jaccard similarity (1 if feature exists).\n",
        "\n",
        "4. **Similarity Computation**\n",
        "\n",
        "   * Cosine similarity is computed for TF-IDF and Count vectors.\n",
        "   * Jaccard similarity is computed for binary vectors using pairwise intersection-over-union.\n",
        "\n",
        "5. **User Profiling & Recommendation**\n",
        "\n",
        "   * For **TF-IDF** and **Count** models:\n",
        "\n",
        "     * A personalized **user profile vector** is created using a weighted average of vectors from rated movies.\n",
        "     * Recommendations are generated by finding unseen movies most similar to the user’s profile.\n",
        "   * For the **Binary + Jaccard** model:\n",
        "\n",
        "     * The average Jaccard similarity is computed between each unseen movie and the user’s seen movies.\n",
        "\n",
        "6. **Result Generation & Tagging**\n",
        "\n",
        "   * Top 50 movie recommendations are produced per user for each model.\n",
        "   * Each output is tagged with the model name: `\"TF-IDF + Cosine\"`, `\"Count + Cosine\"`, or `\"Binary + Jaccard\"`.\n",
        "\n",
        "7. **Output Consolidation**\n",
        "\n",
        "   * All recommendation outputs are combined into one labeled DataFrame for comparative analysis and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2U48IdMw0rJ",
        "outputId": "b84ebf08-fb35-41d8-9490-2dbed78db0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating users: 100%|██████████| 6040/6040 [02:57<00:00, 34.08it/s]\n",
            "Evaluating users: 100%|██████████| 6040/6040 [02:50<00:00, 35.35it/s]\n",
            "Evaluating users: 100%|██████████| 6040/6040 [03:14<00:00, 31.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF + Cosine RMSE: 0.9424\n",
            "Count + Cosine RMSE: 0.9693\n",
            "Binary + Jaccard RMSE: 0.9297\n",
            "\n",
            "Top-N Recommendations for User 5549 — TF-IDF\n",
            "      movieId                                              title  \\\n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1396     1420  Message to Love: The Isle of Wight Festival (1...   \n",
            "3313     3382                             Song of Freedom (1936)   \n",
            "3587     3656                                       Lured (1947)   \n",
            "777       787                 Gate of Heavenly Peace, The (1995)   \n",
            "\n",
            "      predicted_rating  \n",
            "3164               5.0  \n",
            "1396               5.0  \n",
            "3313               5.0  \n",
            "3587               5.0  \n",
            "777                5.0  \n",
            "\n",
            "Top-N Recommendations for User 5549 — Count\n",
            "      movieId                                              title  \\\n",
            "1762     1830                            Follow the Bitch (1998)   \n",
            "3811     3881                           Bittersweet Motel (2000)   \n",
            "977       989          Schlafes Bruder (Brother of Sleep) (1995)   \n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1339     1360  Identification of a Woman (Identificazione di ...   \n",
            "\n",
            "      predicted_rating  \n",
            "1762               5.0  \n",
            "3811               5.0  \n",
            "977                5.0  \n",
            "3164               5.0  \n",
            "1339               5.0  \n",
            "\n",
            "Top-N Recommendations for User 5549 — Jaccard\n",
            "\n",
            "Top-N Recommendations for User 5549 — Jaccard\n",
            "      movieId                                              title  \\\n",
            "3313     3382                             Song of Freedom (1936)   \n",
            "1762     1830                            Follow the Bitch (1998)   \n",
            "777       787                 Gate of Heavenly Peace, The (1995)   \n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1396     1420  Message to Love: The Isle of Wight Festival (1...   \n",
            "\n",
            "      predicted_rating  \n",
            "3313               5.0  \n",
            "1762               5.0  \n",
            "777                5.0  \n",
            "3164               5.0  \n",
            "1396               5.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring tfidf: 100%|██████████| 6040/6040 [02:51<00:00, 35.18it/s]\n",
            "Scoring count: 100%|██████████| 6040/6040 [02:47<00:00, 36.06it/s]\n",
            "Scoring binary: 100%|██████████| 6040/6040 [03:14<00:00, 31.01it/s]\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Hybrid CBF Pipeline with RMSE, Top-N, and CSV Export\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load Data\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# Feature Engineering\n",
        "def create_feature_string(df):\n",
        "    def clean(col):\n",
        "        return df[col].fillna('').astype(str).str.replace(',', ' ').str.replace(r'\\s+', ' ', regex=True)\n",
        "    df['cbf_features'] = (\n",
        "        clean('tmdb_genres') + ' ' +\n",
        "        clean('keywords') + ' ' +\n",
        "        clean('top_3_cast') + ' ' +\n",
        "        clean('directors') + ' ' +\n",
        "        df['overview'].fillna('').str.lower().str.replace(r'[^\\w\\s]', '', regex=True) + ' ' +\n",
        "        df['year'].astype(str)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "movies = create_feature_string(movies)\n",
        "\n",
        "# Train-Test Split Per User\n",
        "def train_test_split_user(ratings, test_size=0.2):\n",
        "    train_rows, test_rows = [], []\n",
        "    for user_id, group in ratings.groupby('userId'):\n",
        "        if len(group) >= 5:\n",
        "            train, test = train_test_split(group, test_size=test_size, random_state=42)\n",
        "            train_rows.append(train)\n",
        "            test_rows.append(test)\n",
        "        else:\n",
        "            train_rows.append(group)\n",
        "    return pd.concat(train_rows), pd.concat(test_rows)\n",
        "\n",
        "train_ratings, test_ratings = train_test_split_user(ratings)\n",
        "\n",
        "# Bias Terms\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "user_bias = train_ratings.groupby('userId')['rating'].mean() - global_mean\n",
        "item_bias = train_ratings.groupby('movieId')['rating'].mean() - global_mean\n",
        "\n",
        "# Vectorizers\n",
        "tfidf_matrix = TfidfVectorizer(stop_words='english').fit_transform(movies['cbf_features'])\n",
        "count_matrix = CountVectorizer(stop_words='english').fit_transform(movies['cbf_features'])\n",
        "binary_matrix = CountVectorizer(binary=True).fit_transform(movies['cbf_features'])\n",
        "\n",
        "# Helper Functions\n",
        "def build_user_profile(user_id, train_ratings, matrix, movies):\n",
        "    user_train = train_ratings[train_ratings['userId'] == user_id]\n",
        "    indices = movies[movies['movieId'].isin(user_train['movieId'])].index\n",
        "\n",
        "    if len(indices) == 0:\n",
        "        return None\n",
        "\n",
        "    weights = user_train.set_index('movieId').loc[movies.iloc[indices]['movieId']]['rating'].values\n",
        "\n",
        "    # Check if matrix is sparse\n",
        "    row_vectors = matrix[indices].toarray() if hasattr(matrix, \"toarray\") else matrix[indices]\n",
        "\n",
        "    return np.average(row_vectors, axis=0, weights=weights).reshape(1, -1)\n",
        "\n",
        "def evaluate_rmse_for_user(user_id, train_ratings, test_ratings, matrix, movies, sim_fn):\n",
        "    profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "    if profile is None:\n",
        "        return None\n",
        "    user_test = test_ratings[test_ratings['userId'] == user_id]\n",
        "    test_movies = movies[movies['movieId'].isin(user_test['movieId'])]\n",
        "    test_indices = test_movies.index\n",
        "    if len(test_indices) == 0:\n",
        "        return None\n",
        "    sims = sim_fn(profile, matrix[test_indices]).flatten()\n",
        "    b_u = user_bias.get(user_id, 0)\n",
        "    b_i = item_bias.reindex(test_movies['movieId']).fillna(0).values\n",
        "    preds = np.clip(global_mean + b_u + b_i + sims * 1.5, 0.5, 5.0)\n",
        "    actual = user_test.set_index('movieId').loc[test_movies['movieId']]['rating'].values\n",
        "    return np.sqrt(mean_squared_error(actual, preds))\n",
        "\n",
        "def evaluate_rmse_all_users(train_ratings, test_ratings, matrix, movies, sim_fn):\n",
        "    user_ids = test_ratings['userId'].unique()\n",
        "    rmses = []\n",
        "    for user_id in tqdm(user_ids, desc=\"Evaluating users\"):\n",
        "        rmse = evaluate_rmse_for_user(user_id, train_ratings, test_ratings, matrix, movies, sim_fn)\n",
        "        if rmse is not None:\n",
        "            rmses.append(rmse)\n",
        "    return np.mean(rmses)\n",
        "\n",
        "def recommend_top_n(user_id, train_ratings, matrix, movies, sim_fn, top_n=50):\n",
        "    profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "    if profile is None:\n",
        "        return pd.DataFrame()\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId']\n",
        "    unseen = movies[~movies['movieId'].isin(seen)]\n",
        "    sims = sim_fn(profile, matrix[unseen.index]).flatten()\n",
        "    content_scores = sims * 1.5\n",
        "    b_u = user_bias.get(user_id, 0)\n",
        "    b_i = item_bias.reindex(unseen['movieId']).fillna(0).values\n",
        "    preds = np.clip(global_mean + b_u + b_i + content_scores, 0.5, 5.0)\n",
        "    top_idx = np.argsort(preds)[-top_n:][::-1]\n",
        "    return unseen.iloc[top_idx][['movieId', 'title']].assign(predicted_rating=preds[top_idx])\n",
        "\n",
        "# Save Predictions for Meta-Learner\n",
        "def save_predictions(user_ids, matrix, sim_fn, label):\n",
        "    dfs = []\n",
        "    for user_id in tqdm(user_ids, desc=f\"Scoring {label}\"):\n",
        "        profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "        if profile is None:\n",
        "            continue\n",
        "        user_test = test_ratings[test_ratings['userId'] == user_id]\n",
        "        test_movies = movies[movies['movieId'].isin(user_test['movieId'])]\n",
        "        test_indices = test_movies.index\n",
        "        if len(test_indices) == 0:\n",
        "            continue\n",
        "        sims = sim_fn(profile, matrix[test_indices]).flatten()\n",
        "        b_u = user_bias.get(user_id, 0)\n",
        "        b_i = item_bias.reindex(test_movies['movieId']).fillna(0).values\n",
        "        preds = np.clip(global_mean + b_u + b_i + sims * 1.5, 0.5, 5.0)\n",
        "        actual = user_test.set_index('movieId').loc[test_movies['movieId']]['rating'].values\n",
        "        df = pd.DataFrame({\n",
        "            'userId': user_id,\n",
        "            'movieId': test_movies['movieId'].values,\n",
        "            'true_rating': actual,\n",
        "            f'{label}_score': preds\n",
        "        })\n",
        "        dfs.append(df)\n",
        "    result = pd.concat(dfs)\n",
        "    result.to_csv(f'cbf_predictions_{label}.csv', index=False)\n",
        "\n",
        "# Run Evaluations and Save Predictions\n",
        "rmse_tfidf = evaluate_rmse_all_users(train_ratings, test_ratings, tfidf_matrix, movies, cosine_similarity)\n",
        "rmse_count = evaluate_rmse_all_users(train_ratings, test_ratings, count_matrix, movies, cosine_similarity)\n",
        "rmse_binary = evaluate_rmse_all_users(train_ratings, test_ratings, binary_matrix.toarray(), movies,\n",
        "                                      lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard'))\n",
        "\n",
        "print(f\"\\nTF-IDF + Cosine RMSE: {rmse_tfidf:.4f}\")\n",
        "print(f\"Count + Cosine RMSE: {rmse_count:.4f}\")\n",
        "print(f\"Binary + Jaccard RMSE: {rmse_binary:.4f}\")\n",
        "\n",
        "# Top-N Recommendations for User 5549\n",
        "print(\"\\nTop-N Recommendations for User 5549 — TF-IDF\")\n",
        "print(recommend_top_n(5549, train_ratings, tfidf_matrix, movies, cosine_similarity).head())\n",
        "\n",
        "print(\"\\nTop-N Recommendations for User 5549 — Count\")\n",
        "print(recommend_top_n(5549, train_ratings, count_matrix, movies, cosine_similarity).head())\n",
        "\n",
        "print(\"\\nTop-N Recommendations for User 5549 — Jaccard\")\n",
        "print(\"\\nTop-N Recommendations for User 5549 — Jaccard\")\n",
        "print(recommend_top_n(\n",
        "    5549,\n",
        "    train_ratings,\n",
        "    binary_matrix.toarray(),   # Convert to dense\n",
        "    movies,\n",
        "    lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard')\n",
        ").head())\n",
        "\n",
        "\n",
        "# Save Predictions\n",
        "save_predictions(test_ratings['userId'].unique(), tfidf_matrix, cosine_similarity, 'tfidf')\n",
        "save_predictions(test_ratings['userId'].unique(), count_matrix, cosine_similarity, 'count')\n",
        "save_predictions(test_ratings['userId'].unique(), binary_matrix.toarray(),\n",
        "                 lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard'), 'binary')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omg4Y6k5XmSn"
      },
      "source": [
        "### **Memory-based collaborative filtering module (UBCF, IBCF)**\n",
        "\n",
        "***Purpose:***\n",
        "\n",
        "This module implements **memory-based collaborative filtering** using **user-user** or **item-item** similarity. It addresses **user bias** by normalizing ratings through mean-centering and optionally **rescaling predictions** to the original rating scale for interpretability.\n",
        "\n",
        "***Methodology:***\n",
        "\n",
        "1. **Rating Matrix Construction**:\n",
        "\n",
        "   * A user-item matrix is built from raw MovieLens-style ratings data.\n",
        "   * For `kind='user'`, ratings are mean-centered per user to reduce bias from lenient or strict raters.\n",
        "   * For `kind='item'`, raw ratings are used directly (no normalization), as the algorithm focuses on item similarities based on a single user's input.\n",
        "\n",
        "2. **Similarity Computation**:\n",
        "\n",
        "   * Cosine similarity is computed either:\n",
        "\n",
        "     * **Across users** for user-based CF (`kind='user'`)\n",
        "     * **Across items** for item-based CF (`kind='item'`)\n",
        "   * `sklearn.metrics.pairwise_distances` is used to derive similarity as `1 - cosine_distance`.\n",
        "\n",
        "3. **Prediction Generation**:\n",
        "\n",
        "   * For **user-based CF**:\n",
        "\n",
        "     * Ratings from similar users are weighted by similarity and averaged.\n",
        "     * The user’s mean rating is **added back** to restore predictions to the original scale (e.g., 1–5).\n",
        "   * For **item-based CF**:\n",
        "\n",
        "     * A user’s own ratings are used to compute scores for similar items.\n",
        "     * No mean is added back, since predictions are already on the correct scale.\n",
        "\n",
        "4. **Top-N Recommendations**:\n",
        "\n",
        "   * The system filters out movies the user has already rated.\n",
        "   * It ranks unseen movies by predicted score and returns the top-N recommendations.\n",
        "   * Each recommendation is labeled with the model type (`User-Based CF` or `Item-Based CF`) for downstream tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDxfs5nRH3hv",
        "outputId": "020529c7-ae88-41c9-d445-211d21cf22af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-Based CF RMSE: 1.0336\n",
            "Item-Based CF RMSE: 0.8796\n",
            "Dummy Predictor RMSE: 1.1197\n",
            "\n",
            "Top 10 User-Based CF Recommendations:\n",
            "   movieId                          title     score\n",
            "0     2701          Wild Wild West (1999)  3.608871\n",
            "1     1917              Armageddon (1998)  3.608796\n",
            "2     1721                 Titanic (1997)  3.607812\n",
            "3     3753            Patriot, The (2000)  3.605514\n",
            "4     2881         Double Jeopardy (1999)  3.604429\n",
            "5     2722           Deep Blue Sea (1999)  3.602775\n",
            "6      736                 Twister (1996)  3.602065\n",
            "7     3113             End of Days (1999)  3.601182\n",
            "8      780  Independence Day (ID4) (1996)  3.600492\n",
            "9     1101                 Top Gun (1986)  3.600417\n",
            "\n",
            "Top 10 Item-Based CF Recommendations:\n",
            "   movieId                                              title     score\n",
            "0      557                                  Mamma Roma (1962)  5.000000\n",
            "1      657                                 Yankee Zulu (1994)  5.000000\n",
            "2     3601                        Castaway Cowboy, The (1974)  5.000000\n",
            "3      989          Schlafes Bruder (Brother of Sleep) (1995)  5.000000\n",
            "4     3517                                  Bells, The (1926)  5.000000\n",
            "5      729  Institute Benjamenta, or This Dream People Cal...  5.000000\n",
            "6     2591  Jeanne and the Perfect Guy (Jeanne et le garço...  5.000000\n",
            "7     1787  Paralyzing Fear: The Story of Polio in America...  4.865259\n",
            "8     3323                              Chain of Fools (2000)  4.825758\n",
            "9      787                 Gate of Heavenly Peace, The (1995)  4.823364\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === Step 1: Load Data ===\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# === Step 2: Create Bias-Adjusted Matrix ===\n",
        "def create_bias_adjusted_matrix(ratings_df):\n",
        "    matrix = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
        "    global_mean = ratings_df['rating'].mean()\n",
        "    user_bias = matrix.sub(global_mean, axis=0).mean(axis=1)\n",
        "    item_bias = matrix.sub(global_mean, axis=0).sub(user_bias, axis=0).mean(axis=0)\n",
        "    adjusted = matrix.sub(global_mean).sub(user_bias, axis=0).sub(item_bias, axis=1)\n",
        "    return adjusted.fillna(0), global_mean, user_bias, item_bias\n",
        "\n",
        "user_item_matrix, global_mean, user_bias, item_bias = create_bias_adjusted_matrix(train_ratings)\n",
        "\n",
        "# === Step 3: Similarity Matrices ===\n",
        "user_sim_matrix = cosine_similarity(user_item_matrix)\n",
        "item_sim_matrix = cosine_similarity(user_item_matrix.T)\n",
        "\n",
        "# === Step 4: Recommender Function ===\n",
        "def recommend_memory_based(user_id, matrix, global_mean, user_bias, item_bias, sim_matrix, kind='user', top_n=50, return_full=False):\n",
        "    if kind == 'user':\n",
        "        sim_scores = sim_matrix[matrix.index.get_loc(user_id)]\n",
        "        weighted = sim_scores @ matrix.values\n",
        "        norm = np.abs(sim_scores).sum()\n",
        "        preds = weighted / norm if norm != 0 else np.zeros_like(weighted)\n",
        "        preds += global_mean + user_bias.loc[user_id]\n",
        "    else:\n",
        "        user_vector = matrix.loc[user_id]\n",
        "        weighted = user_vector @ sim_matrix\n",
        "        norm = (user_vector != 0) @ np.abs(sim_matrix)\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            preds = np.true_divide(weighted, norm)\n",
        "            preds[norm == 0] = 0\n",
        "        preds += global_mean + user_bias.loc[user_id] + item_bias.values\n",
        "\n",
        "    preds = np.clip(preds, 1.0, 5.0)\n",
        "    pred_series = pd.Series(preds, index=matrix.columns)\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    pred_series = pred_series.drop(labels=seen, errors='ignore')\n",
        "\n",
        "    if return_full:\n",
        "        return pred_series\n",
        "    else:\n",
        "        top_preds = pred_series.sort_values(ascending=False).head(top_n)\n",
        "        return pd.DataFrame({\n",
        "            'userId': user_id,\n",
        "            'movieId': top_preds.index,\n",
        "            'score': top_preds.values\n",
        "        })\n",
        "\n",
        "# === Step 5: Evaluation Function ===\n",
        "def evaluate_model_and_save(test_df, matrix, global_mean, user_bias, item_bias, sim_matrix, kind='user', output_file=None):\n",
        "    all_preds = []\n",
        "    for uid in test_df['userId'].unique():\n",
        "        if uid not in matrix.index:\n",
        "            continue\n",
        "        recs = recommend_memory_based(uid, matrix, global_mean, user_bias, item_bias, sim_matrix, kind, top_n=1000, return_full=True)\n",
        "        actual = test_df[test_df['userId'] == uid]\n",
        "        merged = pd.merge(actual, recs.rename(\"score\"), on=\"movieId\")\n",
        "        all_preds.append(merged)\n",
        "\n",
        "    all_preds_df = pd.concat(all_preds, ignore_index=True)\n",
        "    if output_file:\n",
        "        all_preds_df.to_csv(output_file, index=False)\n",
        "    rmse = np.sqrt(mean_squared_error(all_preds_df['rating'], all_preds_df['score'])) if not all_preds_df.empty else np.nan\n",
        "    return rmse\n",
        "\n",
        "# === Step 6: Run Evaluation and Save Predictions ===\n",
        "user_rmse = evaluate_model_and_save(test_ratings, user_item_matrix, global_mean, user_bias, item_bias, user_sim_matrix, 'user', \"ubcf_predictions.csv\")\n",
        "item_rmse = evaluate_model_and_save(test_ratings, user_item_matrix, global_mean, user_bias, item_bias, item_sim_matrix, 'item', \"ibcf_predictions.csv\")\n",
        "dummy_rmse = np.sqrt(mean_squared_error(test_ratings['rating'], [global_mean] * len(test_ratings)))\n",
        "\n",
        "print(f\"User-Based CF RMSE: {user_rmse:.4f}\")\n",
        "print(f\"Item-Based CF RMSE: {item_rmse:.4f}\")\n",
        "print(f\"Dummy Predictor RMSE: {dummy_rmse:.4f}\")\n",
        "\n",
        "# === Step 7: Top-N for One User ===\n",
        "user_id = 5549\n",
        "user_recs = recommend_memory_based(user_id, user_item_matrix, global_mean, user_bias, item_bias, user_sim_matrix, 'user', top_n=50)\n",
        "item_recs = recommend_memory_based(user_id, user_item_matrix, global_mean, user_bias, item_bias, item_sim_matrix, 'item', top_n=50)\n",
        "\n",
        "user_recs = user_recs.merge(movies, on='movieId', how='left')\n",
        "item_recs = item_recs.merge(movies, on='movieId', how='left')\n",
        "\n",
        "print(\"\\nTop 10 User-Based CF Recommendations:\")\n",
        "print(user_recs[['movieId', 'title', 'score']].head(10))\n",
        "\n",
        "print(\"\\nTop 10 Item-Based CF Recommendations:\")\n",
        "print(item_recs[['movieId', 'title', 'score']].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6d3SP0OolH0"
      },
      "source": [
        "## **Model-Based Filtering:**\n",
        "\n",
        "  * *SVD (Surprise)*: Learns latent features from the rating matrix.\n",
        "  * *ALS (PySpark)*: Scalable factorization method for large datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dCPxrNMnQkh"
      },
      "source": [
        "### **Module 9: Model-Based Collaborative Filtering (SVD using Surprise)**\n",
        "\n",
        "**Purpose:**\n",
        "Use matrix factorization (SVD) to learn latent user/item features from the rating matrix.\n",
        "\n",
        "**Application:**\n",
        "- Accurate, scalable recommendations for sparse datasets using user/item embeddings.\n",
        "- Suitable for small to medium datasets.\n",
        "- Optimized via `GridSearchCV` for hyperparameter tuning.\n",
        "- Good interpretability of latent factors per user and item.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQVXyqXBMFes",
        "outputId": "c73467d0-5aa0-41df-9215-3ab6fe1e36b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE: 0.8818, Params: {'n_factors': 50, 'lr_all': 0.005, 'reg_all': 0.02}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200042/200042 [00:01<00:00, 147547.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.8707\n",
            "Saved: svd_surprise_predictions.csv\n",
            "\n",
            "Generating Top-N for User 5549...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3673/3673 [00:00<00:00, 147173.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 Recommendations:\n",
            "   movieId                                              title  pred_rating\n",
            "0     2905                                     Sanjuro (1962)     4.517540\n",
            "1     2019  Seven Samurai (The Magnificent Seven) (Shichin...     4.473381\n",
            "2     1207                       To Kill a Mockingbird (1962)     4.445691\n",
            "3     3091                                   Kagemusha (1980)     4.418558\n",
            "4     1203                                12 Angry Men (1957)     4.399932\n",
            "5     2501                                 October Sky (1999)     4.379961\n",
            "6      527                            Schindler's List (1993)     4.377627\n",
            "7     1178                              Paths of Glory (1957)     4.372463\n",
            "8     1148                         Wrong Trousers, The (1993)     4.372194\n",
            "9     3307                                 City Lights (1931)     4.369825\n",
            "Saved: top50_svd_surprise_user_5549.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split, GridSearchCV\n",
        "from surprise.accuracy import rmse as surprise_rmse\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Step 1: Load ratings\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# === Step 2: Prepare Surprise data\n",
        "reader = Reader(rating_scale=(0.5, 5.0))\n",
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# === Step 3: Tune SVD Model\n",
        "param_grid = {\n",
        "    'n_factors': [50, 100],\n",
        "    'lr_all': [0.005, 0.01],\n",
        "    'reg_all': [0.02, 0.1]\n",
        "}\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
        "gs.fit(data)\n",
        "best_svd_model = gs.best_estimator['rmse']\n",
        "print(f\"Best RMSE: {gs.best_score['rmse']:.4f}, Params: {gs.best_params['rmse']}\")\n",
        "\n",
        "# === Step 4: Train/Test Split and Evaluate\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "best_svd_model.fit(trainset)\n",
        "\n",
        "predictions = [best_svd_model.predict(uid, iid, r_ui=rui) for uid, iid, rui in tqdm(testset)]\n",
        "svd_rmse = surprise_rmse(predictions)\n",
        "\n",
        "# === Save full predictions with tag\n",
        "pred_df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
        "pred_df = pred_df.rename(columns={'uid': 'userId', 'iid': 'movieId', 'rui': 'true_rating', 'est': 'pred_rating'})\n",
        "pred_df['model'] = 'svd_surprise'\n",
        "pred_df.to_csv(\"svd_surprise_predictions.csv\", index=False)\n",
        "print(\"Saved: svd_surprise_predictions.csv\")\n",
        "\n",
        "# === Step 5: Top-N for User 5549\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = [mid for mid in all_movie_ids if mid not in rated_movie_ids]\n",
        "\n",
        "print(f\"\\nGenerating Top-N for User {target_user}...\")\n",
        "top_preds = [(mid, best_svd_model.predict(target_user, mid).est) for mid in tqdm(unrated_movie_ids)]\n",
        "top_50_df = pd.DataFrame(top_preds, columns=['movieId', 'pred_rating'])\n",
        "top_50_df = top_50_df.sort_values(by='pred_rating', ascending=False).head(50)\n",
        "top_50_df['userId'] = target_user\n",
        "top_50_df['model'] = 'svd_surprise'\n",
        "top_50_df = top_50_df[['userId', 'movieId', 'pred_rating', 'model']]\n",
        "\n",
        "# Optional: Merge with movie titles\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_df = top_50_df.merge(movies, on='movieId', how='left')\n",
        "\n",
        "print(\"\\nTop 10 Recommendations:\")\n",
        "print(top_50_df[['movieId', 'title', 'pred_rating']].head(10))\n",
        "\n",
        "# === Save Top-N recommendations\n",
        "top_50_df.to_csv(\"top50_svd_surprise_user_5549.csv\", index=False)\n",
        "print(\"Saved: top50_svd_surprise_user_5549.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeCmPC_DnZip"
      },
      "source": [
        "### **Model-Based Collaborative Filtering (ALS using PySpark)**\n",
        "\n",
        "**Purpose:**\n",
        "Use Alternating Least Squares (ALS) to learn latent user/item features at scale.\n",
        "\n",
        "**Application:**\n",
        "- Distributed recommendation system for large-scale datasets.\n",
        "- Runs on Apache Spark for horizontal scalability.\n",
        "- Handles sparsity well using factorization.\n",
        "- Suited for real-time, production-level systems with massive data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42y8vTW1SmGY",
        "outputId": "59827082-c878-4f8a-f78a-ee4df28d7e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   userId  movieId  true_rating  pred_rating          model\n",
            "0     148       11            5     4.056976  ALS (PySpark)\n",
            "1     148       17            4     3.796230  ALS (PySpark)\n",
            "2     148      107            4     3.295793  ALS (PySpark)\n",
            "3     148      165            3     3.911792  ALS (PySpark)\n",
            "4     148      185            3     3.471385  ALS (PySpark)\n",
            "\n",
            "Final RMSE on Test Set: 0.8746\n",
            "\n",
            "Generating Top-50 recommendations for User 5549...\n",
            "\n",
            "Top 10 ALS Recommendations for User 5549:\n",
            "   movieId                                        title  pred_rating\n",
            "0      572                       Foreign Student (1994)     4.545667\n",
            "1     1164  Two or Three Things I Know About Her (1966)     4.526562\n",
            "2     1471                           Boys Life 2 (1997)     4.524452\n",
            "3      557                            Mamma Roma (1962)     4.466455\n",
            "4     2503                      Apple, The (Sib) (1998)     4.429305\n",
            "5     3172                      Ulysses (Ulisse) (1954)     4.379709\n",
            "6      912                            Casablanca (1942)     4.376063\n",
            "7     3365                        Searchers, The (1956)     4.342710\n",
            "8      989    Schlafes Bruder (Brother of Sleep) (1995)     4.322570\n",
            "9      920                    Gone with the Wind (1939)     4.321684\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# ALS with Train/Test Split and Evaluation\n",
        "# ==============================\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import Row\n",
        "import pandas as pd\n",
        "\n",
        "# --- Start Spark Session ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ALSModel\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Load Ratings ---\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "ratings_df = spark.createDataFrame(ratings[['userId', 'movieId', 'rating']])\n",
        "\n",
        "# --- Train/Test Split ---\n",
        "(training_df, test_df) = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# --- Train ALS Model ---\n",
        "als = ALS(\n",
        "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "    rank=10, maxIter=10, regParam=0.1,\n",
        "    coldStartStrategy=\"drop\", nonnegative=True\n",
        ")\n",
        "als_model = als.fit(training_df)\n",
        "\n",
        "# --- Predict on Test Set ---\n",
        "test_predictions = als_model.transform(test_df)\n",
        "\n",
        "# --- Evaluate RMSE ---\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName='rmse',\n",
        "    labelCol='rating',\n",
        "    predictionCol='prediction'\n",
        ")\n",
        "rmse_score = evaluator.evaluate(test_predictions)\n",
        "\n",
        "# --- Save Prediction Data ---\n",
        "pred_pd = test_predictions.select('userId', 'movieId', 'rating', 'prediction').toPandas()\n",
        "pred_pd = pred_pd.rename(columns={'rating': 'true_rating', 'prediction': 'pred_rating'})\n",
        "pred_pd['model'] = 'ALS (PySpark)'\n",
        "pred_pd.to_csv(\"als_predictions_test.csv\", index=False)\n",
        "\n",
        "print(pred_pd[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']].head())\n",
        "print(f\"\\nFinal RMSE on Test Set: {rmse_score:.4f}\")\n",
        "\n",
        "# ==============================\n",
        "# Top-50 Predictions for User 5549\n",
        "# ==============================\n",
        "\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = list(set(all_movie_ids) - set(rated_movie_ids))\n",
        "\n",
        "# Create Spark DataFrame of userId + unrated movieId pairs\n",
        "user_unrated_pairs = spark.createDataFrame([Row(userId=target_user, movieId=int(mid)) for mid in unrated_movie_ids])\n",
        "\n",
        "# Predict ratings for unrated movies\n",
        "print(f\"\\nGenerating Top-50 recommendations for User {target_user}...\")\n",
        "top_preds_df = als_model.transform(user_unrated_pairs).dropna()\n",
        "\n",
        "# Get top-50 highest predicted ratings\n",
        "top_50_preds = top_preds_df.orderBy(col(\"prediction\").desc()).limit(50)\n",
        "top_50_pd = top_50_preds.select(\"userId\", \"movieId\", \"prediction\").toPandas()\n",
        "top_50_pd['model'] = \"ALS (PySpark)\"\n",
        "top_50_pd = top_50_pd.rename(columns={'prediction': 'pred_rating'})\n",
        "\n",
        "# Merge with Movie Titles\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_pd = top_50_pd.merge(movies, on='movieId', how='left')\n",
        "\n",
        "# Save Top-N\n",
        "top_50_pd.to_csv(\"als_top_50_user_5549.csv\", index=False)\n",
        "\n",
        "# Print Top-N\n",
        "print(\"\\nTop 10 ALS Recommendations for User 5549:\")\n",
        "print(top_50_pd[['movieId', 'title', 'pred_rating']].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls6RqljaolUG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOPkUwmWieTt"
      },
      "source": [
        "### Meta-Learner Preprocessing\n",
        "\n",
        "1. **Sample Subset of Predictions**\n",
        "   Instead of merging all 2.2 million+ rows, we’ll sample a manageable subset per model (e.g., 10K rows per model) for meta-learner training.\n",
        "\n",
        "2. **Join Movie Metadata**\n",
        "   Merge only this sampled data with the selected metadata columns.\n",
        "\n",
        "3. **Feature Engineering**\n",
        "\n",
        "   * **Target**: `true_rating`\n",
        "   * **Features**:\n",
        "\n",
        "     * `pred_rating` from each model (wide format pivoted per model)\n",
        "     * Movie metadata:\n",
        "\n",
        "       * `vote_average` (numeric → scale)\n",
        "       * `vote_count` (numeric → scale)\n",
        "       * `genres`, `top_3_cast`, `directors`, `keywords` → use **TF-IDF vectorization** (not dummy encoding).\n",
        "\n",
        "4. **Model**: Use **XGBoost** as the meta-learner (handles missing values, scales well, supports tree-based split on sparse data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByEFskVij2re"
      },
      "source": [
        "#### Step 1: Load and Merge Each Prediction File with Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMDYya4Qie_u",
        "outputId": "c53553b0-982e-4f07-cb11-0412a1c820f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/ubcf_merged.csv with shape (200016, 20)\n",
            "Saved: /content/svd_merged.csv with shape (200042, 20)\n",
            "Saved: /content/ibcf_merged.csv with shape (200016, 20)\n",
            "Saved: /content/cbf_tfidf_merged.csv with shape (202451, 20)\n",
            "Saved: /content/cbf_count_merged.csv with shape (202451, 20)\n",
            "Saved: /content/cbf_binary_merged.csv with shape (202451, 20)\n",
            "Saved: /content/als_merged.csv with shape (1000209, 20)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load movie metadata\n",
        "movie_df = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "\n",
        "# List of prediction files with desired output name\n",
        "prediction_files = {\n",
        "    \"ubcf\": \"ubcf_predictions.csv\",\n",
        "    \"svd\": \"svd_surprise_predictions.csv\",\n",
        "    \"ibcf\": \"ibcf_predictions.csv\",\n",
        "    \"cbf_tfidf\": \"cbf_predictions_tfidf.csv\",\n",
        "    \"cbf_count\": \"cbf_predictions_count.csv\",\n",
        "    \"cbf_binary\": \"cbf_predictions_binary.csv\",\n",
        "    \"als\": \"als_pyspark_predictions.csv\"\n",
        "}\n",
        "\n",
        "# Merge and save each separately\n",
        "for model_name, file_path in prediction_files.items():\n",
        "    df = pd.read_csv(file_path)\n",
        "    merged = pd.merge(df, movie_df, on=\"movieId\", how=\"left\")\n",
        "    output_path = f\"/content/{model_name}_merged.csv\"\n",
        "    merged.to_csv(output_path, index=False)\n",
        "    print(f\"Saved: {output_path} with shape {merged.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I919p6k2j6SR"
      },
      "source": [
        "#### Step 2 Combine All Merged Files for Meta-Learner Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 0: Setup ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.sparse import hstack\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# --- Step 1: Load and merge prediction files ---\n",
        "merged_dfs = []\n",
        "for model_name in prediction_files.keys():\n",
        "    df = pd.read_csv(f\"{model_name}_merged.csv\")\n",
        "    df[\"model\"] = model_name\n",
        "    merged_dfs.append(df)\n",
        "\n",
        "df = pd.concat(merged_dfs, ignore_index=True)\n",
        "\n",
        "# --- Step 2: Clean dataset ---\n",
        "df = df.dropna(subset=[\"true_rating\"]).reset_index(drop=True)\n",
        "\n",
        "# Save userId and movieId for reattachment later\n",
        "id_df = df[[\"userId\", \"movieId\"]].reset_index(drop=True)\n",
        "\n",
        "# --- Step 3: Combine metadata into CBF text features ---\n",
        "text_cols = [\"tmdb_genres\", \"top_3_cast\", \"directors\", \"keywords\"]\n",
        "for col in text_cols:\n",
        "    df[col] = df[col].fillna(\"\").astype(str)\n",
        "\n",
        "df[\"cbf_features\"] = df[text_cols].agg(\" \".join, axis=1)\n",
        "\n",
        "# --- Step 4: TF-IDF Vectorization ---\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_text = tfidf.fit_transform(df[\"cbf_features\"])  # sparse matrix\n",
        "\n",
        "# --- Step 5: Numeric features and scaling ---\n",
        "numeric_cols = [\"vote_average\", \"vote_count\", \"pred_rating\"]\n",
        "X_numeric = df[numeric_cols].fillna(0).values\n",
        "scaler = StandardScaler()\n",
        "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "# --- Step 6: Combine TF-IDF and numeric features ---\n",
        "X_combined = hstack([X_text, X_numeric_scaled])  # final input matrix (sparse)\n",
        "y = df[\"true_rating\"].values  # target variable\n",
        "\n",
        "# --- Step 7: Train/test split (also split ID columns) ---\n",
        "X_train, X_test, y_train, y_test, id_train, id_test = train_test_split(\n",
        "    X_combined, y, id_df, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- Step 8: Train XGBoost model (GPU) ---\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "params = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"tree_method\": \"hist\",             # use hist\n",
        "    \"device\": \"cuda\",                  # enables GPU\n",
        "    \"max_depth\": 6,\n",
        "    \"eta\": 0.1,\n",
        "    \"eval_metric\": \"rmse\"\n",
        "}\n",
        "\n",
        "\n",
        "model = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# --- Step 9: Predict and reattach IDs ---\n",
        "y_pred = model.predict(dtest)\n",
        "\n",
        "pred_df = id_test.copy()\n",
        "pred_df[\"pred_rating\"] = y_pred\n",
        "pred_df[\"true_rating\"] = y_test\n",
        "\n",
        "# --- Step 10: Evaluate ---\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(pred_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKwvbNoAK2gj",
        "outputId": "756a1911-41d1-4c32-f162-9c270cd01a14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ RMSE: 0.8846\n",
            "✅ R² Score: 0.3747\n",
            "         userId  movieId  pred_rating  true_rating\n",
            "1318706    3032     3421     4.459865            5\n",
            "662395     1836      367     4.028944            3\n",
            "336973      261      505     2.592825            2\n",
            "564891     3216     1527     3.222407            3\n",
            "1797252    4478      367     2.913699            2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X-9Smatbj_Wm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8a326fc6-52fa-4ccb-bda1-f2f8acf2c041"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'prediction_files' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-4133547955.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load merged versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmerged_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_name}_merged.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m  \u001b[0;31m# reinforce model column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'prediction_files' is not defined"
          ]
        }
      ],
      "source": [
        "# Load merged versions\n",
        "merged_dfs = []\n",
        "for model_name in prediction_files.keys():\n",
        "    df = pd.read_csv(f\"{model_name}_merged.csv\")\n",
        "    df['model'] = model_name  # reinforce model column\n",
        "    merged_dfs.append(df)\n",
        "\n",
        "# Concatenate all into one dataset\n",
        "all_merged_df = pd.concat(merged_dfs, ignore_index=True)\n",
        "all_merged_df.to_csv(\"all_models_merged.csv\", index=False)\n",
        "print(\"Final merged dataset saved as 'all_models_merged.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McLSv7qznOEY"
      },
      "source": [
        "#### STEP 3: Feature Engineering for Meta-Learner"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 0: Setup ---\n",
        "# !pip install -U --pre --extra-index-url=https://pypi.nvidia.com cudf-cu12 cuml-cu12 --no-cache-dir\n",
        "\n",
        "import cudf\n",
        "import cupy as cp\n",
        "from cuml.model_selection import train_test_split\n",
        "from cuml.preprocessing import StandardScaler\n",
        "\n",
        "# --- Step 1: Load and merge all model prediction files ---\n",
        "merged_dfs = []\n",
        "for model_name in prediction_files.keys():\n",
        "    df = cudf.read_csv(f\"{model_name}_merged.csv\")\n",
        "    df['model'] = model_name\n",
        "    merged_dfs.append(df)\n",
        "\n",
        "all_merged_df = cudf.concat(merged_dfs, ignore_index=True)\n",
        "\n",
        "# --- Step 2: Clean and retain IDs ---\n",
        "df = all_merged_df.dropna(subset=[\"true_rating\"]).reset_index(drop=True)\n",
        "id_df = df[[\"userId\", \"movieId\"]]\n",
        "\n",
        "# --- Step 3: GPU-native multi-hot encoder with top-K filtering ---\n",
        "def encode_multilabel_column(df, col, prefix, top_k=100):\n",
        "    temp = df[[col]].fillna(\"\").copy()\n",
        "    temp[\"row_id\"] = cp.arange(len(temp))\n",
        "    temp[col] = temp[col].str.strip().str.replace(\" \", \"\")\n",
        "\n",
        "    splits = temp[col].str.split(\",\", expand=True)\n",
        "    splits[\"row_id\"] = temp[\"row_id\"]\n",
        "    melted = splits.melt(id_vars=[\"row_id\"], value_name=\"value\").dropna()\n",
        "    melted = melted[melted[\"value\"].str.len() > 0]\n",
        "\n",
        "    # Limit to top-K most frequent values\n",
        "    top_values = melted[\"value\"].value_counts().head(top_k).index\n",
        "    melted = melted[melted[\"value\"].isin(top_values)]\n",
        "\n",
        "    melted = melted.drop_duplicates(subset=[\"row_id\", \"value\"])\n",
        "    melted[\"indicator\"] = 1\n",
        "    dummies = melted.pivot(index=\"row_id\", columns=\"value\", values=\"indicator\").fillna(0)\n",
        "    dummies.columns = [f\"{prefix}_{col}\" for col in dummies.columns]\n",
        "\n",
        "    return dummies.sort_index()\n",
        "\n",
        "# --- Step 4: Encode only selected multi-label columns (keywords excluded) ---\n",
        "multi_cols = [\"tmdb_genres\", \"top_3_cast\", \"directors\"]\n",
        "mlb_features = []\n",
        "\n",
        "for col in multi_cols:\n",
        "    encoded_df = encode_multilabel_column(df, col, col, top_k=100)  # limit to top 100 values\n",
        "    mlb_features.append(encoded_df)\n",
        "\n",
        "# --- Step 5: Prepare numerical + model-based features ---\n",
        "numerical_cols = [\"vote_average\", \"vote_count\"]\n",
        "model_cols = [\"pred_rating\"]\n",
        "\n",
        "X_numeric = df[numerical_cols + model_cols].fillna(0)\n",
        "y = df[\"true_rating\"]\n",
        "\n",
        "# --- Step 6: Scale numeric features using cuML ---\n",
        "scaler = StandardScaler()\n",
        "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "# --- Step 7: Combine all features on GPU ---\n",
        "X_combined = cudf.concat([X_numeric_scaled] + mlb_features, axis=1)\n",
        "\n",
        "# --- Step 8: GPU Train/Test Split ---\n",
        "X_train, X_test, y_train, y_test, train_ids, test_ids = train_test_split(\n",
        "    X_combined, y, id_df, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ✅ Ready for training on GPU (cuML, XGBoost, etc.)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "fmyu187n-crF",
        "outputId": "bf60ca65-0fe1-4e9d-a7c2-717e16b29740"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "std::bad_alloc: out_of_memory: CUDA error (failed to allocate 928060800 bytes) at: /pyenv/versions/3.13.4/lib/python3.13/site-packages/librmm/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-3812234014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmulti_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mencoded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_multilabel_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# limit to top 100 values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mmlb_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-7-3812234014.py\u001b[0m in \u001b[0;36mencode_multilabel_column\u001b[0;34m(df, col, prefix, top_k)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmelted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmelted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"row_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmelted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"indicator\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mdummies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmelted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"row_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mdummies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"{prefix}_{col}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdummies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/dataframe.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(self, columns, index, values)\u001b[0m\n\u001b[1;32m   7767\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcopy_docstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7768\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7769\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7771\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_performance_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/reshape.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(data, columns, index, values)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Duplicate index-column pairs found. Cannot reshape.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     result = _pivot(\n\u001b[0m\u001b[1;32m   1170\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_by_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols_to_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/reshape.py\u001b[0m in \u001b[0;36m_pivot\u001b[0;34m(col_accessor, index, columns)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0mscatter_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcolumns_idx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindex_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0mtarget_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0mtarget_col\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscatter_map\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             result.update(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/column/column.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_obj_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid scatter map type {key.dtype}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scatter_by_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_normalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/column/column.py\u001b[0m in \u001b[0;36m_scatter_by_column\u001b[0;34m(self, key, value, bounds_check)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m             return ColumnBase.from_pylibcudf(  # type: ignore[return-value]\n\u001b[0;32m-> 1273\u001b[0;31m                 copying.scatter(\n\u001b[0m\u001b[1;32m   1274\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                 )[0]\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/_internals/copying.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(sources, scatter_map, target_columns, bounds_check)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumnBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     plc_tbl = plc.copying.scatter(\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mplc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pylibcudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumnBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcopying.pyx\u001b[0m in \u001b[0;36mpylibcudf.copying.__pyx_fuse_0scatter\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcopying.pyx\u001b[0m in \u001b[0;36mpylibcudf.copying.scatter\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error (failed to allocate 928060800 bytes) at: /pyenv/versions/3.13.4/lib/python3.13/site-packages/librmm/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kRYkqDprnPwg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "596c17f4-bd0c-4dfa-8193-83e40620b21e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-1244732635.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mX_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# include userId and movieId\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Optional: split out IDs from X for later merging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2683\u001b[0;31m     return list(\n\u001b[0m\u001b[1;32m   2684\u001b[0m         chain.from_iterable(\n\u001b[1;32m   2685\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2683\u001b[0m     return list(\n\u001b[1;32m   2684\u001b[0m         chain.from_iterable(\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m         )\n\u001b[1;32m   2687\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# TODO: we should probably use _is_pandas_df(X) instead but this would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# require updating some tests such as test_train_test_split_mock_pandas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_polars_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_polars_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_pandas_indexing\u001b[0;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# using take() instead of iloc[] ensures the return value is a \"proper\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# copy that will not raise SettingWithCopyWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# check whether we should index with loc or iloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4131\u001b[0m             )\n\u001b[1;32m   4132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4133\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   4134\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4135\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    685\u001b[0m             )\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             new_blocks = [\n\u001b[0m\u001b[1;32m    688\u001b[0m                 blk.take_nd(\n\u001b[1;32m    689\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             new_blocks = [\n\u001b[0;32m--> 688\u001b[0;31m                 blk.take_nd(\n\u001b[0m\u001b[1;32m    689\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;31m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m         new_values = algos.take_nd(\n\u001b[0m\u001b[1;32m   1308\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    112\u001b[0m             )\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/sparse/array.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, allow_fill, fill_value)\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_without_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/sparse/array.py\u001b[0m in \u001b[0;36m_take_with_fill\u001b[0;34m(self, indices, fill_value)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;31m# 1.) we took for an index of -1 (new)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;31m# 2.) we took a value that was self.fill_value (old)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0msp_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0mnew_fill_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0mold_fill_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msp_indexer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mnew_fill_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
        "\n",
        "# === Step 1: Load merged dataset and retain IDs ===\n",
        "# df = pd.read_csv(\"all_models_merged.csv\")\n",
        "df = all_merged_df\n",
        "\n",
        "df = df.dropna(subset=[\"true_rating\"])\n",
        "df = df.reset_index(drop=True)  # reset index for clean merging later\n",
        "id_df = df[[\"userId\", \"movieId\"]]  # retain IDs\n",
        "\n",
        "# === Step 2: Drop rows with missing true_rating ===\n",
        "df = df.dropna(subset=[\"true_rating\"])\n",
        "\n",
        "# === Step 3: Clean and encode multi-categorical features ===\n",
        "multi_cols = [\"tmdb_genres\", \"top_3_cast\", \"directors\", \"keywords\"]\n",
        "for col in multi_cols:\n",
        "    df[col] = df[col].fillna(\"\").apply(lambda x: list(set(x.split(\", \"))) if isinstance(x, str) else [])\n",
        "\n",
        "# Apply MultiLabelBinarizer for each multi-categorical column\n",
        "mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "mlb_features = []\n",
        "\n",
        "for col in multi_cols:\n",
        "    encoded = mlb.fit_transform(df[col])\n",
        "    encoded_df = pd.DataFrame.sparse.from_spmatrix(encoded, columns=[f\"{col}_{cls}\" for cls in mlb.classes_])\n",
        "    mlb_features.append(encoded_df)\n",
        "\n",
        "# === Step 4: Select numerical and model-based features ===\n",
        "numerical_cols = [\"vote_average\", \"vote_count\"]\n",
        "model_cols = [\"pred_rating\"]\n",
        "target_col = \"true_rating\"\n",
        "\n",
        "X_numeric = df[numerical_cols + model_cols].fillna(0)\n",
        "y = df[target_col]\n",
        "\n",
        "# === Step 5: Combine all features ===\n",
        "X = pd.concat([X_numeric] + mlb_features, axis=1)\n",
        "\n",
        "# === Step 6: Scale numeric features ===\n",
        "scaler = StandardScaler()\n",
        "X[numerical_cols + model_cols] = scaler.fit_transform(X[numerical_cols + model_cols])\n",
        "\n",
        "# === Step 7: Train/Test Split with IDs ===\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_full = pd.concat([id_df, X], axis=1)  # include userId and movieId\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optional: split out IDs from X for later merging\n",
        "train_ids = X_train[[\"userId\", \"movieId\"]]\n",
        "test_ids = X_test[[\"userId\", \"movieId\"]]\n",
        "\n",
        "# Drop IDs before modeling\n",
        "X_train = X_train.drop(columns=[\"userId\", \"movieId\"])\n",
        "X_test = X_test.drop(columns=[\"userId\", \"movieId\"])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Train XGBoost Meta-Learner and Predict"
      ],
      "metadata": {
        "id": "WxWFF9vJ69kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 4: Train Meta-Learner (XGBoost) ===\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Train the model ---\n",
        "meta_model = XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)\n",
        "meta_model.fit(X_train, y_train)\n",
        "\n",
        "# --- Predict on test set ---\n",
        "y_pred = meta_model.predict(X_test)\n",
        "\n",
        "# --- Compute RMSE ---\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"\\nMeta-Learner RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "CnUq_raB7BPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Save Predictions for Evaluation"
      ],
      "metadata": {
        "id": "BQcbusBe7JZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Reconstruct full test prediction set with movieId/userId ---\n",
        "meta_preds = test_ids.copy()\n",
        "meta_preds[\"true_rating\"] = y_test.values\n",
        "meta_preds[\"pred_rating\"] = y_pred\n",
        "meta_preds[\"model\"] = \"Meta-Learner (XGBoost)\"\n",
        "\n",
        "# --- Save to CSV for Top-N, Diversity, Serendipity, etc. ---\n",
        "meta_preds.to_csv(\"meta_learner_predictions.csv\", index=False)\n",
        "print(\"\\nSaved: meta_learner_predictions.csv\")\n"
      ],
      "metadata": {
        "id": "m4OmGkKt7Lk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}